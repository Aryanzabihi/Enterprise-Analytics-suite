import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import datetime
from datetime import datetime, time
import io
import base64
import random

# IT metric calculation functions will be defined in this file

# Enhanced Chart creation functions with optimized analytics
def create_chart(chart_type, data, **kwargs):
    """Create enhanced charts with optimized analytics, tooltips, and legends"""
    
    if chart_type == "bar":
        fig = px.bar(data, **kwargs)
        fig.update_layout(
            xaxis_tickangle=-45,
            hovermode='x unified',
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            )
        )
        # Enhanced tooltips for bar charts
        fig.update_traces(
            hovertemplate="<b>%{x}</b><br>" +
                         "Value: %{y}<br>" +
                         "<extra></extra>"
        )
        
    elif chart_type == "pie":
        fig = px.pie(data, **kwargs)
        fig.update_layout(
            showlegend=True,
            legend=dict(
                orientation="v",
                yanchor="middle",
                y=0.5,
                xanchor="left",
                x=1.02
            )
        )
        # Enhanced tooltips for pie charts
        fig.update_traces(
            hovertemplate="<b>%{label}</b><br>" +
                         "Value: %{value}<br>" +
                         "Percentage: %{percent:.1%}<br>" +
                         "<extra></extra>"
        )
        
    elif chart_type == "line":
        fig = px.line(data, **kwargs, markers=True)
        fig.update_layout(
            hovermode='x unified',
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            )
        )
        # Enhanced tooltips for line charts
        fig.update_traces(
            hovertemplate="<b>%{x}</b><br>" +
                         "Value: %{y}<br>" +
                         "<extra></extra>"
        )
        
    elif chart_type == "scatter":
        fig = px.scatter(data, **kwargs)
        fig.update_layout(
            hovermode='closest',
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            )
        )
        # Enhanced tooltips for scatter plots
        fig.update_traces(
            hovertemplate="<b>%{x}</b><br>" +
                         "Y-Value: %{y}<br>" +
                         "<extra></extra>"
        )
        
    elif chart_type == "heatmap":
        fig = px.imshow(data, **kwargs)
        fig.update_layout(
            hovermode='closest',
            showlegend=True
        )
        # Enhanced tooltips for heatmaps
        fig.update_traces(
            hovertemplate="<b>Row: %{y}</b><br>" +
                         "Column: %{x}<br>" +
                         "Value: %{z}<br>" +
                         "<extra></extra>"
        )
        
    else:
        fig = px.bar(data, **kwargs)
        fig.update_layout(
            xaxis_tickangle=-45,
            hovermode='x unified',
            showlegend=True
        )
    
    # Common enhancements for all charts
    fig.update_layout(
        font=dict(family="Arial, sans-serif", size=12),
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        margin=dict(l=50, r=50, t=80, b=50),
        height=500
    )
    
    return fig

def create_advanced_chart(chart_type, data, **kwargs):
    """Create advanced charts with sophisticated analytics and enhanced tooltips"""
    
    if chart_type == "multi_line":
        fig = go.Figure()
        for trace_name, trace_data in data.items():
            fig.add_trace(go.Scatter(
                x=trace_data['x'],
                y=trace_data['y'],
                mode='lines+markers',
                name=trace_name,
                line=dict(width=3),
                marker=dict(size=8),
                hovertemplate="<b>%{fullData.name}</b><br>" +
                             "Time: %{x}<br>" +
                             "Value: %{y}<br>" +
                             "<extra></extra>"
            ))
        
        fig.update_layout(
            title=kwargs.get('title', 'Multi-Line Chart'),
            xaxis_title=kwargs.get('xaxis_title', 'X-Axis'),
            yaxis_title=kwargs.get('yaxis_title', 'Y-Axis'),
            hovermode='x unified',
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            ),
            font=dict(family="Arial, sans-serif", size=12),
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            margin=dict(l=50, r=50, t=80, b=50),
            height=500
        )
        
    elif chart_type == "stacked_bar":
        fig = go.Figure()
        categories = data['categories']
        values = data['values']
        names = data['names']
        
        for i, name in enumerate(names):
            fig.add_trace(go.Bar(
                name=name,
                x=categories,
                y=values[i],
                hovertemplate="<b>%{fullData.name}</b><br>" +
                             "Category: %{x}<br>" +
                             "Value: %{y}<br>" +
                             "<extra></extra>"
            ))
        
        fig.update_layout(
            title=kwargs.get('title', 'Stacked Bar Chart'),
            xaxis_title=kwargs.get('xaxis_title', 'Categories'),
            yaxis_title=kwargs.get('yaxis_title', 'Values'),
            barmode='stack',
            hovermode='x unified',
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            ),
            font=dict(family="Arial, sans-serif", size=12),
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            margin=dict(l=50, r=50, t=80, b=50),
            height=500
        )
    
    return fig

def load_custom_css():
    st.markdown("""
    <style>
    /* Modern SaaS Dashboard Styling */
    
    /* Main background gradient */
    .main .block-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 0;
        max-width: 100%;
    }
    
    /* Sidebar styling - compact */
    .css-1d391kg {
        background: linear-gradient(180deg, #1a202c 0%, #2d3748 100%);
        padding: 20px 12px;
        width: 250px;
        min-width: 250px;
        box-shadow: 2px 0 10px rgba(0,0,0,0.1);
    }
    
    /* Optimize sidebar width */
    .css-1lcbmhc {
        width: 250px;
        min-width: 250px;
    }
    
    /* Main content area - expanded width */
    .main .block-container {
        padding-left: 1rem;
        padding-right: 1rem;
        max-width: 100%;
        width: 100%;
    }
    
    /* Expand main content width */
    .main {
        max-width: 100%;
        padding-left: 0;
        padding-right: 0;
    }
    
    /* Remove default Streamlit width constraints */
    .block-container {
        max-width: 100%;
        padding-left: 1rem;
        padding-right: 1rem;
    }
    
    /* Expand chart containers */
    .chart-container {
        width: 100%;
        max-width: none;
    }
    
    /* Make plots wider */
    .js-plotly-plot {
        width: 100% !important;
    }
    
    /* Expand dataframe width */
    .dataframe {
        width: 100% !important;
        max-width: none;
    }
    
    /* Force full width for all content */
    .element-container {
        width: 100% !important;
        max-width: none !important;
    }
    
    /* Ensure plots use full width */
    .plotly-graph-div {
        width: 100% !important;
        max-width: none !important;
        height: auto !important;
    }
    
    /* Optimize chart height for wide layout */
    .js-plotly-plot {
        height: 500px !important;
    }
    
    /* Optimize column layouts for wider space */
    .row-widget.stHorizontal {
        width: 100%;
    }
    
    /* Remove any remaining width constraints */
    .stApp > div:first-child {
        max-width: 100%;
    }
    
    /* Ensure all Streamlit elements use full width */
    .stApp {
        max-width: 100%;
    }
    
    /* Optimize for wide screens */
    @media (min-width: 1200px) {
        .main .block-container {
            padding-left: 2rem;
            padding-right: 2rem;
        }
    }
    
    /* Make sure all containers expand */
    .stContainer {
        width: 100% !important;
        max-width: none !important;
    }
    
    /* Sidebar button styling */
    .stButton > button {
        background: rgba(255, 255, 255, 0.08);
        border: 1px solid rgba(255, 255, 255, 0.15);
        border-radius: 10px;
        color: #ffffff;
        font-weight: 500;
        margin: 6px 0;
        padding: 12px 16px;
        transition: all 0.3s ease;
        text-shadow: 0 1px 2px rgba(0,0,0,0.2);
        font-size: 0.95rem;
    }
    
    .stButton > button:hover {
        background: rgba(255, 255, 255, 0.15);
        border-color: rgba(255, 255, 255, 0.3);
        transform: translateX(3px);
        box-shadow: 0 2px 8px rgba(0,0,0,0.2);
    }
    
    /* Active button styling */
    .stButton > button[data-active="true"] {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-color: rgba(255, 255, 255, 0.4);
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
        font-weight: 600;
    }
    
    /* Card styling */
    .metric-card {
        background: white;
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
        border: 1px solid rgba(255,255,255,0.2);
        backdrop-filter: blur(10px);
    }
    
    .metric-card-blue {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        box-shadow: 0 8px 32px rgba(102, 126, 234, 0.3);
    }
    
    .metric-card-purple {
        background: linear-gradient(135deg, #a855f7 0%, #7c3aed 100%);
        color: white;
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        box-shadow: 0 8px 32px rgba(168, 85, 247, 0.3);
    }
    
    .metric-card-orange {
        background: linear-gradient(135deg, #f97316 0%, #ea580c 100%);
        color: white;
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        box-shadow: 0 8px 32px rgba(249, 115, 22, 0.3);
    }
    
    .metric-card-teal {
        background: linear-gradient(135deg, #14b8a6 0%, #0d9488 100%);
        color: white;
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        box-shadow: 0 8px 32px rgba(20, 184, 166, 0.3);
    }
    
    .metric-card-green {
        background: linear-gradient(135deg, #22c55e 0%, #16a34a 100%);
        color: white;
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        box-shadow: 0 8px 32px rgba(34, 197, 94, 0.3);
    }
    
    .metric-card-red {
        background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
        color: white;
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        box-shadow: 0 8px 32px rgba(239, 68, 68, 0.3);
    }
    
    /* Chart container styling */
    .chart-container {
        background: white;
        border-radius: 15px;
        padding: 20px;
        margin: 15px 0;
        box-shadow: 0 4px 20px rgba(0,0,0,0.08);
        border: 1px solid rgba(0,0,0,0.05);
    }
    
    /* Header styling */
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 30px;
        border-radius: 0 0 20px 20px;
        margin-bottom: 30px;
        text-align: center;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1);
    }
    
    /* Welcome section */
    .welcome-section {
        background: white;
        border-radius: 15px;
        padding: 25px;
        margin: 20px 0;
        box-shadow: 0 4px 20px rgba(0,0,0,0.08);
        border: 1px solid rgba(0,0,0,0.05);
    }
    
    /* Progress circle styling */
    .progress-circle {
        width: 80px;
        height: 80px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        font-weight: bold;
        font-size: 18px;
        margin: 10px auto;
    }
    
    /* Button styling */
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 10px;
        padding: 10px 20px;
        font-weight: 600;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
    }
    
    /* Tab styling */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background: transparent;
    }
    
    .stTabs [data-baseweb="tab"] {
        background: white;
        border-radius: 10px 10px 0 0;
        padding: 10px 20px;
        font-weight: 600;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
    }
    
    /* Dataframe styling */
    .dataframe {
        border-radius: 10px;
        overflow: hidden;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    
    /* Insights container */
    .insights-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 15px;
        padding: 20px;
        margin: 15px 0;
        color: white;
        box-shadow: 0 8px 32px rgba(102, 126, 234, 0.3);
    }
    
    /* Hide Streamlit default elements */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    
    /* Custom scrollbar */
    ::-webkit-scrollbar {
        width: 8px;
    }
    
    ::-webkit-scrollbar-track {
        background: #f1f1f1;
        border-radius: 10px;
    }
    
    ::-webkit-scrollbar-thumb {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 10px;
    }
    
    ::-webkit-scrollbar-thumb:hover {
        background: linear-gradient(135deg, #5a67d8 0%, #6b46c1 100%);
    }
    
    /* Responsive design */
    @media (max-width: 768px) {
        .metric-card {
            margin: 5px 0;
            padding: 15px;
        }
        
        .main-header {
            padding: 20px;
            font-size: 24px;
        }
    }
    </style>
    """, unsafe_allow_html=True)

def display_dataframe_with_index_1(df, **kwargs):
    """Display dataframe with index starting from 1"""
    if not df.empty:
        df_display = df.reset_index(drop=True)
        df_display.index = df_display.index + 1
        # Set a reasonable height to avoid inner scrollbars
        height = min(400, len(df_display) * 35 + 50)
        return st.dataframe(df_display, height=height, use_container_width=True, **kwargs)
    else:
        return st.dataframe(df, **kwargs)

def create_template_for_download():
    """Create an Excel template with all required IT data schema and make it downloadable"""
    
    # Create empty DataFrames with the correct IT schema
    servers_template = pd.DataFrame(columns=[
        'server_id', 'server_name', 'server_type', 'location', 'ip_address', 
        'os_version', 'cpu_cores', 'ram_gb', 'storage_tb', 'status', 'last_maintenance'
    ])
    
    network_devices_template = pd.DataFrame(columns=[
        'device_id', 'device_name', 'device_type', 'location', 'ip_address', 
        'model', 'firmware_version', 'status', 'last_backup'
    ])
    
    applications_template = pd.DataFrame(columns=[
        'app_id', 'app_name', 'app_type', 'version', 'server_id', 'department', 
        'critical_level', 'status', 'last_update'
    ])
    
    incidents_template = pd.DataFrame(columns=[
        'incident_id', 'title', 'description', 'priority', 'category', 'reported_by', 
        'reported_date', 'assigned_to', 'status', 'resolution_date', 'resolution_time_minutes'
    ])
    
    tickets_template = pd.DataFrame(columns=[
        'ticket_id', 'title', 'description', 'priority', 'category', 'submitted_by', 
        'submitted_date', 'assigned_to', 'status', 'resolution_date', 'satisfaction_score'
    ])
    
    assets_template = pd.DataFrame(columns=[
        'asset_id', 'asset_name', 'asset_type', 'model', 'serial_number', 'purchase_date', 
        'warranty_expiry', 'location', 'assigned_to', 'status', 'purchase_cost'
    ])
    
    security_events_template = pd.DataFrame(columns=[
        'event_id', 'event_type', 'severity', 'source_ip', 'target_ip', 'timestamp', 
        'description', 'status', 'investigation_required'
    ])
    
    backups_template = pd.DataFrame(columns=[
        'backup_id', 'system_name', 'backup_type', 'start_time', 'end_time', 
        'size_gb', 'status', 'retention_days', 'location'
    ])
    
    projects_template = pd.DataFrame(columns=[
        'project_id', 'project_name', 'description', 'start_date', 'end_date', 
        'budget', 'actual_cost', 'status', 'manager', 'team_size'
    ])
    
    users_template = pd.DataFrame(columns=[
        'user_id', 'username', 'full_name', 'email', 'department', 'role', 
        'access_level', 'last_login', 'status', 'created_date'
    ])
    
    # Create Excel file in memory
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        # Write each template to a separate sheet
        servers_template.to_excel(writer, sheet_name='Servers', index=False)
        network_devices_template.to_excel(writer, sheet_name='Network_Devices', index=False)
        applications_template.to_excel(writer, sheet_name='Applications', index=False)
        incidents_template.to_excel(writer, sheet_name='Incidents', index=False)
        tickets_template.to_excel(writer, sheet_name='Tickets', index=False)
        assets_template.to_excel(writer, sheet_name='Assets', index=False)
        security_events_template.to_excel(writer, sheet_name='Security_Events', index=False)
        backups_template.to_excel(writer, sheet_name='Backups', index=False)
        projects_template.to_excel(writer, sheet_name='Projects', index=False)
        users_template.to_excel(writer, sheet_name='Users', index=False)
        
        # Get the workbook for formatting
        workbook = writer.book
        
        # Add instructions sheet
        instructions_data = {
            'Sheet Name': ['Servers', 'Network_Devices', 'Applications', 'Incidents', 'Tickets', 'Assets', 'Security_Events', 'Backups', 'Projects', 'Users'],
            'Required Fields': [
                'server_id, server_name, server_type, location, ip_address, os_version, cpu_cores, ram_gb, storage_tb, status, last_maintenance',
                'device_id, device_name, device_type, location, ip_address, model, firmware_version, status, last_backup',
                'app_id, app_name, app_type, version, server_id, department, critical_level, status, last_update',
                'incident_id, title, description, priority, category, reported_by, reported_date, assigned_to, status, resolution_date, resolution_time_minutes',
                'ticket_id, title, description, priority, category, submitted_by, submitted_date, assigned_to, status, resolution_date, satisfaction_score',
                'asset_id, asset_name, asset_type, model, serial_number, purchase_date, warranty_expiry, location, assigned_to, status, purchase_cost',
                'event_id, event_type, severity, source_ip, target_ip, timestamp, description, status, investigation_required',
                'backup_id, system_name, backup_type, start_time, end_time, size_gb, status, retention_days, location',
                'project_id, project_name, description, start_date, end_date, budget, actual_cost, status, manager, team_size',
                'user_id, username, full_name, email, department, role, access_level, last_login, status, created_date'
            ],
            'Data Types': [
                'Text, Text, Text, Text, Text, Text, Number, Number, Number, Text, Date',
                'Text, Text, Text, Text, Text, Text, Text, Text, Date',
                'Text, Text, Text, Text, Text, Text, Text, Text, Date',
                'Text, Text, Text, Text, Text, Text, Date, Text, Text, Date, Number',
                'Text, Text, Text, Text, Text, Text, Date, Text, Text, Date, Number',
                'Text, Text, Text, Text, Text, Date, Date, Text, Text, Text, Number',
                'Text, Text, Text, Text, Text, Date, Text, Text, Boolean',
                'Text, Text, Text, Date, Date, Number, Text, Number, Text',
                'Text, Text, Text, Date, Date, Number, Number, Text, Text, Number',
                'Text, Text, Text, Text, Text, Text, Text, Date, Text, Date'
            ]
        }
        
        instructions_df = pd.DataFrame(instructions_data)
        instructions_df.to_excel(writer, sheet_name='Instructions', index=False)
        
        # Format the instructions sheet
        worksheet = writer.sheets['Instructions']
        for i, col in enumerate(instructions_df.columns):
            worksheet.set_column(i, i, len(col) + 5)
    
    output.seek(0)
    return output

def export_data_to_excel():
    """Export current data to Excel file"""
    if 'servers_data' not in st.session_state or st.session_state.servers_data.empty:
        st.error("No data available to export. Please upload data first.")
        return None
    
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        # Write each dataset to a separate sheet
        st.session_state.servers_data.to_excel(writer, sheet_name='Servers', index=False)
        st.session_state.network_devices_data.to_excel(writer, sheet_name='Network_Devices', index=False)
        st.session_state.applications_data.to_excel(writer, sheet_name='Applications', index=False)
        st.session_state.incidents_data.to_excel(writer, sheet_name='Incidents', index=False)
        st.session_state.tickets_data.to_excel(writer, sheet_name='Tickets', index=False)
        st.session_state.assets_data.to_excel(writer, sheet_name='Assets', index=False)
        st.session_state.security_events_data.to_excel(writer, sheet_name='Security_Events', index=False)
        st.session_state.backups_data.to_excel(writer, sheet_name='Backups', index=False)
        st.session_state.projects_data.to_excel(writer, sheet_name='Projects', index=False)
        st.session_state.users_data.to_excel(writer, sheet_name='Users', index=False)
    
    output.seek(0)
    return output

def create_basic_sample_data():
    """Create basic sample data for IT analytics testing"""
    # Sample servers data
    servers_data = pd.DataFrame({
        'server_id': [f'SVR{i:03d}' for i in range(1, 16)],
        'server_name': [f'Server-{i}' for i in range(1, 16)],
        'server_type': ['Production', 'Development', 'Testing', 'Production', 'Development'] * 3,
        'location': ['Data Center A', 'Data Center B', 'Office A', 'Data Center A', 'Office B'] * 3,
        'ip_address': [f'192.168.1.{i}' for i in range(10, 25)],
        'os_version': ['Windows Server 2019', 'Ubuntu 20.04', 'CentOS 8', 'Windows Server 2022', 'Ubuntu 22.04'] * 3,
        'cpu_cores': [8, 16, 4, 32, 8, 16, 4, 32, 8, 16, 4, 32, 8, 16, 4],
        'ram_gb': [32, 64, 16, 128, 32, 64, 16, 128, 32, 64, 16, 128, 32, 64, 16],
        'storage_tb': [2, 4, 1, 8, 2, 4, 1, 8, 2, 4, 1, 8, 2, 4, 1],
        'status': ['Online', 'Online', 'Offline', 'Online', 'Online', 'Online', 'Offline', 'Online', 'Online', 'Online', 'Offline', 'Online', 'Online', 'Online', 'Offline'],
        'last_maintenance': pd.date_range(start='2024-01-01', periods=15, freq='D'),
        'uptime_percentage': np.random.uniform(95.0, 99.9, 15),
        'cpu_utilization': np.random.uniform(10.0, 90.0, 15),
        'memory_utilization': np.random.uniform(20.0, 85.0, 15),
        'next_maintenance': pd.date_range(start='2024-06-01', periods=15, freq='D')
    })
    
    # Sample network devices data
    network_devices_data = pd.DataFrame({
        'device_id': [f'NET{i:03d}' for i in range(1, 21)],
        'device_name': [f'Switch-{i}' for i in range(1, 21)],
        'device_type': ['Switch', 'Router', 'Firewall', 'Switch', 'Router'] * 4,
        'location': ['Data Center A', 'Data Center B', 'Office A', 'Office B', 'Remote Site'] * 4,
        'ip_address': [f'10.0.{i}.{j}' for i in range(1, 5) for j in range(1, 6)],
        'model': ['Cisco Catalyst', 'Juniper Router', 'Fortinet Firewall', 'HP Switch', 'Mikrotik Router'] * 4,
        'firmware_version': ['15.2', '20.4', '6.4', '16.2', '6.48'] * 4,
        'status': ['Active', 'Active', 'Active', 'Active', 'Active'] * 4,
        'last_backup': pd.date_range(start='2024-01-01', periods=20, freq='D'),
        'latency_ms': np.random.uniform(1.0, 50.0, 20),
        'packet_loss_percentage': np.random.uniform(0.0, 2.0, 20),
        'uptime_percentage': np.random.uniform(98.0, 99.9, 20)
    })
    
    # Sample applications data
    applications_data = pd.DataFrame({
        'app_id': [f'APP{i:03d}' for i in range(1, 26)],
        'app_name': [f'Application-{i}' for i in range(1, 26)],
        'app_type': ['Web App', 'Database', 'API', 'Web App', 'Database'] * 5,
        'version': ['2.1.0', '1.5.2', '3.0.1', '2.1.0', '1.5.2'] * 5,
        'server_id': [f'SVR{i:03d}' for i in range(1, 26)],
        'department': ['IT', 'Finance', 'HR', 'Sales', 'Marketing'] * 5,
        'critical_level': ['High', 'Medium', 'Low', 'High', 'Medium'] * 5,
        'status': ['Active', 'Active', 'Active', 'Active', 'Active'] * 5,
        'last_update': pd.date_range(start='2024-01-01', periods=25, freq='D')
    })
    
    # Sample incidents data
    incidents_data = pd.DataFrame({
        'incident_id': [f'INC{i:03d}' for i in range(1, 51)],
        'title': [f'Incident {i}' for i in range(1, 51)],
        'description': [f'Description for incident {i}' for i in range(1, 51)],
        'priority': ['High', 'Medium', 'Low', 'High', 'Medium'] * 10,
        'category': ['Hardware', 'Software', 'Network', 'Security', 'User Error'] * 10,
        'reported_by': [f'User{i}' for i in range(1, 51)],
        'reported_date': pd.date_range(start='2024-01-01', periods=50, freq='D'),
        'assigned_to': [f'Tech{i}' for i in range(1, 51)],
        'status': ['Resolved', 'In Progress', 'Open', 'Resolved', 'In Progress'] * 10,
        'resolution_date': pd.date_range(start='2024-01-01', periods=50, freq='D'),
        'resolution_time_minutes': np.random.randint(30, 480, 50)
    })
    
    # Sample tickets data
    tickets_data = pd.DataFrame({
        'ticket_id': [f'TKT{i:03d}' for i in range(1, 101)],
        'title': [f'Support Ticket {i}' for i in range(1, 101)],
        'description': [f'Description for ticket {i}' for i in range(1, 101)],
        'priority': ['High', 'Medium', 'Low', 'High', 'Medium'] * 20,
        'category': ['Hardware', 'Software', 'Network', 'Access', 'Training'] * 20,
        'submitted_by': [f'User{i}' for i in range(1, 101)],
        'submitted_date': pd.date_range(start='2024-01-01', periods=100, freq='D'),
        'assigned_to': [f'Tech{i}' for i in range(1, 101)],
        'status': ['Resolved', 'In Progress', 'Open', 'Resolved', 'In Progress'] * 20,
        'resolution_date': pd.date_range(start='2024-01-01', periods=100, freq='D'),
        'satisfaction_score': np.random.randint(1, 6, 100)
    })
    
    # Sample assets data
    assets_data = pd.DataFrame({
        'asset_id': [f'AST{i:03d}' for i in range(1, 31)],
        'asset_name': [f'Asset-{i}' for i in range(1, 31)],
        'asset_type': ['Laptop', 'Desktop', 'Monitor', 'Laptop', 'Desktop'] * 6,
        'model': [f'Model-{i}' for i in range(1, 31)],
        'serial_number': [f'SN{i:06d}' for i in range(1, 31)],
        'purchase_date': pd.date_range(start='2023-01-01', periods=30, freq='D'),
        'warranty_expiry': pd.date_range(start='2025-01-01', periods=30, freq='D'),
        'location': ['Office A', 'Office B', 'Office A', 'Office B', 'Office A'] * 6,
        'assigned_to': [f'User{i}' for i in range(1, 31)],
        'status': ['Active', 'Active', 'Active', 'Active', 'Active'] * 6,
        'purchase_cost': np.random.randint(500, 5000, 30)
    })
    
    # Sample security events data
    security_events_data = pd.DataFrame({
        'event_id': [f'SEC{i:03d}' for i in range(1, 41)],
        'event_type': ['Login Attempt', 'File Access', 'Network Scan', 'Login Attempt', 'File Access'] * 8,
        'severity': ['Low', 'Medium', 'High', 'Low', 'Medium'] * 8,
        'source_ip': [f'192.168.{i}.{j}' for i in range(1, 5) for j in range(1, 11)],
        'target_ip': [f'10.0.{i}.{j}' for i in range(1, 5) for j in range(1, 11)],
        'timestamp': pd.date_range(start='2024-01-01', periods=40, freq='H'),
        'description': [f'Security event {i} description' for i in range(1, 41)],
        'status': ['Investigated', 'Open', 'Closed', 'Investigated', 'Open'] * 8,
        'investigation_required': [True, False, True, False, True] * 8
    })
    
    # Sample backups data
    backups_data = pd.DataFrame({
        'backup_id': [f'BAK{i:03d}' for i in range(1, 21)],
        'system_name': [f'System-{i}' for i in range(1, 21)],
        'backup_type': ['Full', 'Incremental', 'Full', 'Incremental', 'Full'] * 4,
        'start_time': pd.date_range(start='2024-01-01', periods=20, freq='D'),
        'end_time': pd.date_range(start='2024-01-01', periods=20, freq='D') + pd.Timedelta(hours=2),
        'size_gb': np.random.randint(10, 500, 20),
        'status': ['Success', 'Success', 'Success', 'Success', 'Success'] * 4,
        'retention_days': [30, 7, 30, 7, 30] * 4,
        'location': ['Local', 'Cloud', 'Local', 'Cloud', 'Local'] * 4
    })
    
    # Sample projects data
    projects_data = pd.DataFrame({
        'project_id': [f'PRJ{i:03d}' for i in range(1, 16)],
        'project_name': [f'Project-{i}' for i in range(1, 16)],
        'description': [f'Description for project {i}' for i in range(1, 16)],
        'start_date': pd.date_range(start='2024-01-01', periods=15, freq='D'),
        'end_date': pd.date_range(start='2024-06-01', periods=15, freq='D'),
        'budget': np.random.randint(10000, 100000, 15),
        'actual_cost': np.random.randint(8000, 120000, 15),
        'status': ['In Progress', 'Completed', 'Planning', 'In Progress', 'Completed'] * 3,
        'manager': [f'Manager{i}' for i in range(1, 16)],
        'team_size': np.random.randint(3, 15, 15)
    })
    
    # Sample users data
    users_data = pd.DataFrame({
        'user_id': [f'USR{i:03d}' for i in range(1, 51)],
        'username': [f'user{i}' for i in range(1, 51)],
        'full_name': [f'User {i}' for i in range(1, 51)],
        'email': [f'user{i}@company.com' for i in range(1, 51)],
        'department': ['IT', 'Finance', 'HR', 'Sales', 'Marketing'] * 10,
        'role': ['Developer', 'Analyst', 'Manager', 'Developer', 'Analyst'] * 10,
        'access_level': ['Admin', 'User', 'Manager', 'Admin', 'User'] * 10,
        'last_login': pd.date_range(start='2024-01-01', periods=50, freq='D'),
        'status': ['Active', 'Active', 'Active', 'Active', 'Active'] * 10,
        'created_date': pd.date_range(start='2023-01-01', periods=50, freq='D')
    })
    
    return {
        'Servers': servers_data,
        'Network_Devices': network_devices_data,
        'Applications': applications_data,
        'Incidents': incidents_data,
        'Tickets': tickets_data,
        'Assets': assets_data,
        'Security_Events': security_events_data,
        'Backups': backups_data,
        'Projects': projects_data,
        'Users': users_data
    }

def create_security_sample_data():
    """Create security-focused sample data for IT analytics testing"""
    # Enhanced security events with more realistic data
    security_events_data = pd.DataFrame({
        'event_id': [f'SEC{i:03d}' for i in range(1, 51)],
        'event_type': ['Failed Login', 'Suspicious Activity', 'Data Access', 'Network Intrusion', 'Malware Detection'] * 10,
        'severity': ['High', 'Medium', 'Low', 'High', 'Medium'] * 10,
        'source_ip': [f'192.168.{i}.{j}' for i in range(1, 6) for j in range(1, 11)],
        'target_ip': [f'10.0.{i}.{j}' for i in range(1, 6) for j in range(1, 11)],
        'timestamp': pd.date_range(start='2024-01-01', periods=50, freq='H'),
        'description': [f'Enhanced security event {i} with detailed threat analysis' for i in range(1, 51)],
        'status': ['Investigated', 'Open', 'Closed', 'Escalated', 'Resolved'] * 10,
        'investigation_required': [True, True, False, True, True] * 10
    })
    
    # Enhanced incidents with security focus
    incidents_data = pd.DataFrame({
        'incident_id': [f'INC{i:03d}' for i in range(1, 61)],
        'title': [f'Security Incident {i}' for i in range(1, 61)],
        'description': [f'Detailed security incident description {i} with threat indicators' for i in range(1, 61)],
        'priority': ['Critical', 'High', 'Medium', 'Critical', 'High'] * 12,
        'category': ['Security Breach', 'Data Leak', 'Malware', 'Unauthorized Access', 'Network Attack'] * 12,
        'reported_by': [f'Security{i}' for i in range(1, 61)],
        'reported_date': pd.date_range(start='2024-01-01', periods=60, freq='D'),
        'assigned_to': [f'SecTech{i}' for i in range(1, 61)],
        'status': ['Resolved', 'In Progress', 'Open', 'Escalated', 'Resolved'] * 12,
        'resolution_date': pd.date_range(start='2024-01-01', periods=60, freq='D'),
        'resolution_time_minutes': np.random.randint(60, 1440, 60)
    })
    
    # Enhanced backups with security considerations
    backups_data = pd.DataFrame({
        'backup_id': [f'BAK{i:03d}' for i in range(1, 31)],
        'system_name': [f'Secure-System-{i}' for i in range(1, 31)],
        'backup_type': ['Encrypted Full', 'Incremental', 'Encrypted Full', 'Incremental', 'Encrypted Full'] * 6,
        'start_time': pd.date_range(start='2024-01-01', periods=30, freq='D'),
        'end_time': pd.date_range(start='2024-01-01', periods=30, freq='D') + pd.Timedelta(hours=3),
        'size_gb': np.random.randint(50, 1000, 30),
        'status': ['Success', 'Success', 'Success', 'Success', 'Success'] * 6,
        'retention_days': [90, 30, 90, 30, 90] * 6,
        'location': ['Secure Local', 'Encrypted Cloud', 'Secure Local', 'Encrypted Cloud', 'Secure Local'] * 6
    })
    
    # Enhanced users with security roles
    users_data = pd.DataFrame({
        'user_id': [f'USR{i:03d}' for i in range(1, 61)],
        'username': [f'secuser{i}' for i in range(1, 61)],
        'full_name': [f'Security User {i}' for i in range(1, 61)],
        'email': [f'secuser{i}@company.com' for i in range(1, 61)],
        'department': ['Security', 'IT', 'Security', 'IT', 'Security'] * 12,
        'role': ['Security Analyst', 'Security Engineer', 'Security Manager', 'Security Analyst', 'Security Engineer'] * 12,
        'access_level': ['Admin', 'User', 'Manager', 'Admin', 'User'] * 12,
        'last_login': pd.date_range(start='2024-01-01', periods=60, freq='D'),
        'status': ['Active', 'Active', 'Active', 'Active', 'Active'] * 12,
        'created_date': pd.date_range(start='2023-01-01', periods=60, freq='D')
    })
    
    return {
        'Security_Events': security_events_data,
        'Incidents': incidents_data,
        'Backups': backups_data,
        'Users': users_data
    }

def create_performance_sample_data():
    """Create performance-focused sample data for IT analytics testing"""
    # Enhanced servers with performance metrics
    servers_data = pd.DataFrame({
        'server_id': [f'SVR{i:03d}' for i in range(1, 21)],
        'server_name': [f'Perf-Server-{i}' for i in range(1, 21)],
        'server_type': ['High-Performance', 'Production', 'Development', 'High-Performance', 'Production'] * 4,
        'location': ['Data Center A', 'Data Center B', 'Office A', 'Data Center A', 'Office B'] * 4,
        'ip_address': [f'192.168.2.{i}' for i in range(10, 30)],
        'os_version': ['Windows Server 2022', 'Ubuntu 22.04', 'CentOS 9', 'Windows Server 2022', 'Ubuntu 22.04'] * 4,
        'cpu_cores': [32, 64, 16, 128, 32, 64, 16, 128, 32, 64, 16, 128, 32, 64, 16, 128, 32, 64, 16, 128],
        'ram_gb': [128, 256, 64, 512, 128, 256, 64, 512, 128, 256, 64, 512, 128, 256, 64, 512, 128, 256, 64, 512],
        'storage_tb': [8, 16, 4, 32, 8, 16, 4, 32, 8, 16, 4, 32, 8, 16, 4, 32, 8, 16, 4, 32],
        'status': ['Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online'],
        'last_maintenance': pd.date_range(start='2024-01-01', periods=20, freq='D')
    })
    
    # Enhanced applications with performance metrics
    applications_data = pd.DataFrame({
        'app_id': [f'APP{i:03d}' for i in range(1, 31)],
        'app_name': [f'Perf-App-{i}' for i in range(1, 31)],
        'app_type': ['High-Performance Web', 'Database', 'API', 'High-Performance Web', 'Database'] * 6,
        'version': ['3.1.0', '2.5.2', '4.0.1', '3.1.0', '2.5.2'] * 6,
        'server_id': [f'SVR{i:03d}' for i in range(1, 31)],
        'department': ['IT', 'Finance', 'HR', 'Sales', 'Marketing'] * 6,
        'critical_level': ['Critical', 'High', 'Medium', 'Critical', 'High'] * 6,
        'status': ['Active', 'Active', 'Active', 'Active', 'Active'] * 6,
        'last_update': pd.date_range(start='2024-01-01', periods=30, freq='D')
    })
    
    # Enhanced tickets with performance metrics
    tickets_data = pd.DataFrame({
        'ticket_id': [f'TKT{i:03d}' for i in range(1, 121)],
        'title': [f'Performance Ticket {i}' for i in range(1, 121)],
        'description': [f'Performance optimization request {i} with detailed requirements' for i in range(1, 121)],
        'priority': ['Critical', 'High', 'Medium', 'Critical', 'High'] * 24,
        'category': ['Performance', 'Optimization', 'Scalability', 'Performance', 'Optimization'] * 24,
        'submitted_by': [f'PerfUser{i}' for i in range(1, 121)],
        'submitted_date': pd.date_range(start='2024-01-01', periods=120, freq='D'),
        'assigned_to': [f'PerfTech{i}' for i in range(1, 121)],
        'status': ['Resolved', 'In Progress', 'Open', 'Resolved', 'In Progress'] * 24,
        'resolution_date': pd.date_range(start='2024-01-01', periods=120, freq='D'),
        'satisfaction_score': np.random.randint(4, 6, 120)
    })
    
    # Enhanced projects with performance focus
    projects_data = pd.DataFrame({
        'project_id': [f'PRJ{i:03d}' for i in range(1, 21)],
        'project_name': [f'Performance Project {i}' for i in range(1, 21)],
        'description': [f'Performance optimization project {i} with scalability goals' for i in range(1, 21)],
        'start_date': pd.date_range(start='2024-01-01', periods=20, freq='D'),
        'end_date': pd.date_range(start='2024-06-01', periods=20, freq='D'),
        'budget': np.random.randint(50000, 200000, 20),
        'actual_cost': np.random.randint(40000, 250000, 20),
        'status': ['In Progress', 'Completed', 'Planning', 'In Progress', 'Completed'] * 4,
        'manager': [f'PerfManager{i}' for i in range(1, 21)],
        'team_size': np.random.randint(5, 20, 20)
    })
    
    return {
        'Servers': servers_data,
        'Applications': applications_data,
        'Tickets': tickets_data,
        'Projects': projects_data
    }

def create_comprehensive_sample_data():
    """Create comprehensive sample data for IT analytics testing with enhanced datasets"""
    
    def cycle(values, desired_length):
        """Repeat values to exactly desired_length elements."""
        base = list(values)
        return [base[i % len(base)] for i in range(desired_length)]

    # Enhanced servers with more realistic metrics
    servers_data = pd.DataFrame({
        'server_id': [f'SVR{i:03d}' for i in range(1, 51)],
        'server_name': [f'Server-{i}' for i in range(1, 51)],
        'server_type': ['Production', 'Development', 'Testing', 'Staging', 'Backup'] * 10,
        'location': ['Data Center A', 'Data Center B', 'Office A', 'Office B', 'Cloud AWS'] * 10,
        'ip_address': [f'192.168.{i//10}.{i%10+1}' for i in range(1, 51)],
        'os_version': ['Windows Server 2022', 'Ubuntu 22.04', 'CentOS 9', 'Windows Server 2019', 'Ubuntu 20.04'] * 10,
        'cpu_cores': np.random.randint(4, 128, 50),
        'ram_gb': np.random.randint(8, 512, 50),
        'storage_tb': np.random.randint(1, 32, 50),
        'status': ['Online', 'Online', 'Online', 'Maintenance', 'Offline'] * 10,
        'uptime_percentage': np.random.uniform(95.0, 99.9, 50),
        'cpu_utilization': np.random.uniform(10.0, 90.0, 50),
        'memory_utilization': np.random.uniform(20.0, 85.0, 50),
        'disk_utilization': np.random.uniform(15.0, 80.0, 50),
        'last_maintenance': pd.date_range(start='2024-01-01', periods=50, freq='D'),
        'next_maintenance': pd.date_range(start='2024-06-01', periods=50, freq='D'),
        'power_consumption_watts': np.random.randint(200, 800, 50),
        'temperature_celsius': np.random.uniform(18.0, 35.0, 50),
        'network_bandwidth_mbps': np.random.randint(100, 10000, 50)
    })
    
    # Enhanced network devices with performance metrics
    network_devices_data = pd.DataFrame({
        'device_id': [f'NET{i:03d}' for i in range(1, 41)],
        'device_name': [f'Device-{i}' for i in range(1, 41)],
        'device_type': ['Switch', 'Router', 'Firewall', 'Load Balancer', 'Wireless AP'] * 8,
        'location': ['Data Center A', 'Data Center B', 'Office A', 'Office B', 'Remote Site'] * 8,
        'ip_address': [f'10.0.{i//10}.{i%10+1}' for i in range(1, 41)],
        'model': ['Cisco Catalyst', 'Juniper Router', 'Fortinet Firewall', 'F5 BIG-IP', 'Aruba AP'] * 8,
        'firmware_version': [f'{random.randint(15, 20)}.{random.randint(0, 9)}.{random.randint(0, 9)}' for _ in range(40)],
        'status': ['Active', 'Active', 'Active', 'Backup', 'Maintenance'] * 8,
        'last_backup': pd.date_range(start='2024-01-01', periods=40, freq='D'),
        'latency_ms': np.random.uniform(1.0, 50.0, 40),
        'packet_loss_percentage': np.random.uniform(0.0, 2.0, 40),
        'throughput_mbps': np.random.randint(100, 10000, 40),
        'error_rate_percentage': np.random.uniform(0.0, 1.0, 40),
        'uptime_percentage': np.random.uniform(98.0, 99.9, 40),
        'temperature_celsius': np.random.uniform(20.0, 40.0, 40),
        'fan_speed_rpm': np.random.randint(2000, 8000, 40)
    })
    
    # Enhanced applications with detailed metrics
    applications_data = pd.DataFrame({
        'app_id': [f'APP{i:03d}' for i in range(1, 61)],
        'app_name': [f'Application-{i}' for i in range(1, 61)],
        'app_type': ['Web App', 'Database', 'API', 'Mobile App', 'Desktop App'] * 12,
        'version': [f'{random.randint(1, 5)}.{random.randint(0, 9)}.{random.randint(0, 9)}' for _ in range(60)],
        'server_id': [f'SVR{random.randint(1, 50):03d}' for _ in range(60)],
        'department': ['IT', 'Finance', 'HR', 'Sales', 'Marketing', 'Operations'] * 10,
        'critical_level': ['Critical', 'High', 'Medium', 'Low'] * 15,
        'status': ['Active', 'Active', 'Active', 'Development', 'Testing'] * 12,
        'last_update': pd.date_range(start='2024-01-01', periods=60, freq='D'),
        'response_time_ms': np.random.uniform(50.0, 2000.0, 60),
        'availability_percentage': np.random.uniform(95.0, 99.9, 60),
        'user_count': np.random.randint(10, 1000, 60),
        'data_size_gb': np.random.uniform(0.1, 100.0, 60),
        'last_backup': pd.date_range(start='2024-01-01', periods=60, freq='D'),
        'security_score': np.random.randint(70, 100, 60),
        'performance_score': np.random.randint(60, 100, 60),
        'compliance_status': ['Compliant', 'Compliant', 'Compliant', 'Under Review', 'Non-Compliant'] * 12
    })
    
    # Enhanced incidents with detailed tracking
    incidents_data = pd.DataFrame({
        'incident_id': [f'INC{i:03d}' for i in range(1, 101)],
        'title': [f'Incident {i}' for i in range(1, 101)],
        'description': [f'Detailed description for incident {i} with technical details' for i in range(1, 101)],
        'priority': cycle(['Critical', 'High', 'Medium', 'Low'], 100),
        'category': cycle(['Hardware', 'Software', 'Network', 'Security', 'Performance', 'User Error'], 100),
        'reported_by': [f'User{i}' for i in range(1, 101)],
        'reported_date': pd.date_range(start='2024-01-01', periods=100, freq='D'),
        'assigned_to': [f'Tech{i}' for i in range(1, 101)],
        'status': cycle(['Resolved', 'In Progress', 'Open', 'Escalated', 'Closed'], 100),
        'resolution_date': pd.date_range(start='2024-01-01', periods=100, freq='D'),
        'resolution_time_minutes': np.random.randint(30, 1440, 100),
        'impact_level': cycle(['High', 'Medium', 'Low'], 100),
        'affected_users': np.random.randint(1, 500, 100),
        'business_impact': cycle(['Revenue Loss', 'Productivity Loss', 'Data Loss', 'Reputation Damage', 'Compliance Risk'], 100),
        'root_cause': cycle(['Hardware Failure', 'Software Bug', 'Network Issue', 'Human Error', 'External Attack'], 100),
        'prevention_measures': cycle(['Hardware Upgrade', 'Software Patch', 'Network Redundancy', 'Training', 'Security Enhancement'], 100)
    })
    
    # Enhanced tickets with SLA tracking
    tickets_data = pd.DataFrame({
        'ticket_id': [f'TKT{i:03d}' for i in range(1, 201)],
        'title': [f'Support Ticket {i}' for i in range(1, 201)],
        'description': [f'Detailed description for ticket {i} with user requirements' for i in range(1, 201)],
        'priority': cycle(['Critical', 'High', 'Medium', 'Low'], 200),
        'category': cycle(['Hardware', 'Software', 'Network', 'Access', 'Training', 'Account Management'], 200),
        'submitted_by': [f'User{i}' for i in range(1, 201)],
        'submitted_date': pd.date_range(start='2024-01-01', periods=200, freq='D'),
        'assigned_to': [f'Tech{i}' for i in range(1, 201)],
        'status': cycle(['Resolved', 'In Progress', 'Open', 'Escalated', 'Closed'], 200),
        'resolution_date': pd.date_range(start='2024-01-01', periods=200, freq='D'),
        'satisfaction_score': np.random.randint(1, 6, 200),
        'sla_target_hours': np.random.randint(2, 48, 200),
        'actual_resolution_hours': np.random.randint(1, 72, 200),
        'sla_met': np.random.choice([True, False], 200, p=[0.85, 0.15]),
        'escalation_count': np.random.randint(0, 3, 200),
        'customer_priority': cycle(['VIP', 'Regular', 'VIP', 'Regular', 'VIP'], 200),
        'resolution_quality': np.random.randint(1, 6, 200),
        'knowledge_article_created': np.random.choice([True, False], 200, p=[0.3, 0.7])
    })
    
    # Enhanced assets with lifecycle tracking
    assets_data = pd.DataFrame({
        'asset_id': [f'AST{i:03d}' for i in range(1, 101)],
        'asset_name': [f'Asset-{i}' for i in range(1, 101)],
        'asset_type': cycle(['Laptop', 'Desktop', 'Monitor', 'Printer', 'Tablet', 'Phone', 'Server', 'Network Device'], 100),
        'model': [f'Model-{i}' for i in range(1, 101)],
        'serial_number': [f'SN{i:06d}' for i in range(1, 101)],
        'purchase_date': pd.date_range(start='2023-01-01', periods=100, freq='D'),
        'warranty_expiry': pd.date_range(start='2025-01-01', periods=100, freq='D'),
        'location': ['Office A', 'Office B', 'Home Office', 'Data Center', 'Remote Site'] * 20,
        'assigned_to': [f'User{i}' for i in range(1, 101)],
        'status': cycle(['Active', 'Active', 'Active', 'Maintenance', 'Retired', 'Lost'], 100),
        'purchase_cost': np.random.randint(500, 10000, 100),
        'current_value': np.random.randint(200, 8000, 100),
        'depreciation_rate': np.random.uniform(0.1, 0.3, 100),
        'last_maintenance': pd.date_range(start='2024-01-01', periods=100, freq='D'),
        'next_maintenance': pd.date_range(start='2024-06-01', periods=100, freq='D'),
        'lifecycle_stage': ['Deployment', 'Active', 'Maintenance', 'End of Life', 'Replacement'] * 20,
        'vendor': cycle(['Dell', 'HP', 'Lenovo', 'Cisco', 'Apple', 'Samsung'], 100),
        'support_contract': cycle(['Active', 'Active', 'Expired', 'Active', 'Active', 'Expired'], 100)
    })
    
    # Enhanced security events with threat intelligence
    security_events_data = pd.DataFrame({
        'event_id': [f'SEC{i:03d}' for i in range(1, 101)],
        'event_type': cycle(['Failed Login', 'Suspicious Activity', 'Data Access', 'Network Intrusion', 'Malware Detection', 'Privilege Escalation'], 100),
        'severity': ['Critical', 'High', 'Medium', 'Low'] * 25,
        'source_ip': [f'192.168.{random.randint(1, 255)}.{random.randint(1, 255)}' for _ in range(100)],
        'target_ip': [f'10.0.{random.randint(1, 255)}.{random.randint(1, 255)}' for _ in range(100)],
        'timestamp': pd.date_range(start='2024-01-01', periods=100, freq='H'),
        'description': [f'Detailed security event {i} with threat indicators and analysis' for i in range(1, 101)],
        'status': cycle(['Investigated', 'Open', 'Closed', 'Escalated', 'Resolved'], 100),
        'investigation_required': np.random.choice([True, False], 100, p=[0.7, 0.3]),
        'threat_level': ['Low', 'Medium', 'High', 'Critical'] * 25,
        'affected_systems': np.random.randint(1, 20, 100),
        'data_compromised': np.random.choice([True, False], 100, p=[0.2, 0.8]),
        'response_time_minutes': np.random.randint(5, 120, 100),
        'mitigation_applied': ['Firewall Rule', 'Access Revoked', 'System Isolated', 'Patch Applied', 'User Notified'] * 20,
        'false_positive': np.random.choice([True, False], 100, p=[0.1, 0.9]),
        'compliance_impact': cycle(['GDPR', 'HIPAA', 'SOX', 'PCI-DSS', 'None'], 100)
    })
    
    # Enhanced backups with detailed metrics
    backups_data = pd.DataFrame({
        'backup_id': [f'BAK{i:03d}' for i in range(1, 51)],
        'system_name': [f'System-{i}' for i in range(1, 51)],
        'backup_type': ['Full', 'Incremental', 'Differential', 'Snapshot', 'Archive'] * 10,
        'start_time': pd.date_range(start='2024-01-01', periods=50, freq='D'),
        'end_time': pd.date_range(start='2024-01-01', periods=50, freq='D') + pd.Timedelta(hours=2),
        'size_gb': np.random.randint(10, 1000, 50),
        'status': cycle(['Success', 'Success', 'Success', 'Partial Success', 'Failed'], 50),
        'retention_days': cycle([30, 7, 30, 7, 90, 365], 50),
        'location': cycle(['Local', 'Cloud', 'Offsite', 'Hybrid'], 50),
        'compression_ratio': np.random.uniform(1.5, 4.0, 50),
        'encryption_enabled': np.random.choice([True, False], 50, p=[0.9, 0.1]),
        'verification_status': cycle(['Verified', 'Verified', 'Verified', 'Failed', 'Pending'], 50),
        'recovery_time_minutes': np.random.randint(5, 120, 50),
        'backup_window_hours': np.random.randint(1, 8, 50),
        'data_integrity_score': np.random.randint(90, 100, 50),
        'cost_per_gb': np.random.uniform(0.01, 0.10, 50)
    })
    
    # Enhanced projects with detailed tracking
    projects_data = pd.DataFrame({
        'project_id': [f'PRJ{i:03d}' for i in range(1, 31)],
        'project_name': [f'Project-{i}' for i in range(1, 31)],
        'description': [f'Detailed description for project {i} with objectives and deliverables' for i in range(1, 31)],
        'start_date': pd.date_range(start='2024-01-01', periods=30, freq='D'),
        'end_date': pd.date_range(start='2024-06-01', periods=30, freq='D'),
        'budget': np.random.randint(10000, 500000, 30),
        'actual_cost': np.random.randint(8000, 600000, 30),
        'status': ['In Progress', 'Completed', 'Planning', 'On Hold', 'Cancelled'] * 6,
        'manager': [f'Manager{i}' for i in range(1, 31)],
        'team_size': np.random.randint(3, 25, 30),
        'progress_percentage': np.random.randint(0, 100, 30),
        'risk_level': ['Low', 'Medium', 'High'] * 10,
        'priority': ['High', 'Medium', 'Low'] * 10,
        'stakeholders': np.random.randint(3, 15, 30),
        'milestones_completed': np.random.randint(0, 10, 30),
        'total_milestones': np.random.randint(5, 15, 30),
        'quality_score': np.random.randint(60, 100, 30),
        'customer_satisfaction': np.random.randint(1, 6, 30),
        'lessons_learned_documented': np.random.choice([True, False], 30, p=[0.8, 0.2])
    })
    
    # Enhanced users with detailed profiles
    users_data = pd.DataFrame({
        'user_id': [f'USR{i:03d}' for i in range(1, 101)],
        'username': [f'user{i}' for i in range(1, 101)],
        'full_name': [f'User {i}' for i in range(1, 101)],
        'email': [f'user{i}@company.com' for i in range(1, 101)],
        'department': cycle(['IT', 'Finance', 'HR', 'Sales', 'Marketing', 'Operations', 'Engineering', 'Support'], 100),
        'role': cycle(['Developer', 'Analyst', 'Manager', 'Director', 'Specialist', 'Coordinator'], 100),
        'access_level': cycle(['Admin', 'User', 'Manager', 'Power User', 'Read Only'], 100),
        'last_login': pd.date_range(start='2024-01-01', periods=100, freq='D'),
        'status': cycle(['Active', 'Active', 'Active', 'Inactive', 'Suspended'], 100),
        'created_date': pd.date_range(start='2023-01-01', periods=100, freq='D'),
        'last_password_change': pd.date_range(start='2024-01-01', periods=100, freq='D'),
        'failed_login_attempts': np.random.randint(0, 5, 100),
        'account_locked': np.random.choice([True, False], 100, p=[0.05, 0.95]),
        'mfa_enabled': np.random.choice([True, False], 100, p=[0.8, 0.2]),
        'session_timeout_minutes': np.random.randint(30, 480, 100),
        'data_access_level': cycle(['Public', 'Internal', 'Confidential', 'Restricted'], 100),
        'training_completed': np.random.choice([True, False], 100, p=[0.9, 0.1]),
        'compliance_status': cycle(['Compliant', 'Compliant', 'Compliant', 'Under Review', 'Non-Compliant'], 100)
    })
    
    # Enhanced cost data
    cost_data = pd.DataFrame({
        'cost_id': [f'COST{i:03d}' for i in range(1, 51)],
        'category': cycle(['Hardware', 'Software', 'Cloud Services', 'Maintenance', 'Personnel', 'Training', 'Licensing', 'Infrastructure'], 50),
        'subcategory': cycle(['Servers', 'Network Equipment', 'Security Tools', 'Backup Services', 'Support Contracts'], 50),
        'amount': np.random.randint(1000, 100000, 50),
        'date': pd.date_range(start='2024-01-01', periods=50, freq='D'),
        'department': cycle(['IT', 'IT', 'IT', 'IT', 'IT'], 50),
        'vendor': cycle(['Dell', 'Microsoft', 'AWS', 'Cisco', 'Oracle'], 50),
        'payment_terms': cycle(['Net 30', 'Net 30', 'Net 30', 'Net 30', 'Net 30'], 50),
        'recurring': np.random.choice([True, False], 50, p=[0.6, 0.4]),
        'budgeted': np.random.choice([True, False], 50, p=[0.8, 0.2]),
        'approval_status': cycle(['Approved', 'Approved', 'Approved', 'Pending', 'Rejected'], 50)
    })
    
    # Enhanced performance metrics
    performance_data = pd.DataFrame({
        'metric_id': [f'PERF{i:03d}' for i in range(1, 51)],
        'metric_name': ['CPU Utilization', 'Memory Usage', 'Disk I/O', 'Network Throughput', 'Response Time'] * 10,
        'system_id': [f'SYS{random.randint(1, 50):03d}' for _ in range(50)],
        'value': np.random.uniform(10.0, 95.0, 50),
        'unit': ['%', '%', 'MB/s', 'Mbps', 'ms'] * 10,
        'timestamp': pd.date_range(start='2024-01-01', periods=50, freq='H'),
        'threshold': np.random.uniform(70.0, 90.0, 50),
        'status': ['Normal', 'Normal', 'Warning', 'Critical', 'Normal'] * 10,
        'trend': ['Stable', 'Increasing', 'Decreasing', 'Stable', 'Stable'] * 10,
        'alert_sent': np.random.choice([True, False], 50, p=[0.2, 0.8])
    })
    
    return {
        'Servers': servers_data,
        'Network_Devices': network_devices_data,
        'Applications': applications_data,
        'Incidents': incidents_data,
        'Tickets': tickets_data,
        'Assets': assets_data,
        'Security_Events': security_events_data,
        'Backups': backups_data,
        'Projects': projects_data,
        'Users': users_data,
        'Cost_Data': cost_data,
        'Performance_Metrics': performance_data
    }

def create_disaster_recovery_sample_data():
    """Create disaster recovery and business continuity focused sample data"""
    
    # Disaster recovery scenarios
    dr_scenarios_data = pd.DataFrame({
        'scenario_id': [f'DR{i:03d}' for i in range(1, 21)],
        'scenario_name': [f'DR Scenario {i}' for i in range(1, 21)],
        'scenario_type': ['Natural Disaster', 'Cyber Attack', 'Hardware Failure', 'Human Error', 'Power Outage'] * 4,
        'severity': ['Critical', 'High', 'Medium', 'Low'] * 5,
        'affected_systems': np.random.randint(5, 50, 20),
        'estimated_downtime_hours': np.random.randint(1, 72, 20),
        'rto_target_hours': np.random.randint(1, 24, 20),
        'rpo_target_hours': np.random.randint(0, 4, 20),
        'last_tested': pd.date_range(start='2023-01-01', periods=20, freq='D'),
        'next_test_date': pd.date_range(start='2024-06-01', periods=20, freq='D'),
        'test_status': ['Passed', 'Passed', 'Passed', 'Failed', 'Not Tested'] * 4,
        'recovery_procedures_documented': np.random.choice([True, False], 20, p=[0.9, 0.1]),
        'team_trained': np.random.choice([True, False], 20, p=[0.8, 0.2])
    })
    
    # Business impact analysis
    business_impact_data = pd.DataFrame({
        'impact_id': [f'IMP{i:03d}' for i in range(1, 31)],
        'system_id': [f'SYS{random.randint(1, 50):03d}' for _ in range(30)],
        'system_name': [f'System-{i}' for i in range(1, 31)],
        'business_criticality': (['Critical', 'High', 'Medium', 'Low'] * 8)[:30],
        'revenue_impact_per_hour': np.random.randint(1000, 100000, 30),
        'productivity_impact_per_hour': np.random.randint(100, 10000, 30),
        'customer_impact': (['High', 'Medium', 'Low'] * 10)[:30],
        'compliance_impact': (['GDPR', 'HIPAA', 'SOX', 'PCI-DSS', 'None'] * 6)[:30],
        'recovery_priority': (['P0', 'P1', 'P2', 'P3'] * 8)[:30],
        'dependencies': np.random.randint(1, 10, 30),
        'last_assessment': pd.date_range(start='2024-01-01', periods=30, freq='D')
    })
    
    # Recovery procedures
    recovery_procedures_data = pd.DataFrame({
        'procedure_id': [f'PROC{i:03d}' for i in range(1, 26)],
        'procedure_name': [f'Recovery Procedure {i}' for i in range(1, 26)],
        'system_id': [f'SYS{random.randint(1, 50):03d}' for _ in range(25)],
        'procedure_type': (['Automated', 'Manual', 'Semi-Automated'] * 9)[:25],
        'estimated_duration_minutes': np.random.randint(5, 120, 25),
        'required_skills': ['System Admin', 'Network Admin', 'Database Admin', 'Security Admin', 'General IT'] * 5,
        'prerequisites': np.random.randint(0, 5, 25),
        'success_rate': np.random.uniform(85.0, 99.9, 25),
        'last_updated': pd.date_range(start='2024-01-01', periods=25, freq='D'),
        'version': [f'{random.randint(1, 5)}.{random.randint(0, 9)}' for _ in range(25)],
        'approved_by': [f'Manager{random.randint(1, 10)}' for _ in range(25)]
    })
    
    # Recovery team
    recovery_team_data = pd.DataFrame({
        'member_id': [f'TEAM{i:03d}' for i in range(1, 21)],
        'member_name': [f'Team Member {i}' for i in range(1, 21)],
        'role': ['Incident Commander', 'Technical Lead', 'Communications Lead', 'Business Liaison', 'Technical Specialist'] * 4,
        'department': ['IT', 'IT', 'IT', 'Business', 'IT'] * 4,
        'contact_number': [f'+1-555-{random.randint(100, 999)}-{random.randint(1000, 9999)}' for _ in range(20)],
        'availability': ['24/7', 'Business Hours', '24/7', 'Business Hours', 'On-Call'] * 4,
        'skills': ['Incident Management', 'Technical Recovery', 'Communication', 'Business Analysis', 'System Administration'] * 4,
        'training_completed': np.random.choice([True, False], 20, p=[0.9, 0.1]),
        'last_training': pd.date_range(start='2023-01-01', periods=20, freq='D'),
        'certification': ['ITIL', 'CISSP', 'PMP', 'ITIL', 'CISSP'] * 4
    })
    
    # Recovery testing
    recovery_testing_data = pd.DataFrame({
        'test_id': [f'TEST{i:03d}' for i in range(1, 16)],
        'test_name': [f'Recovery Test {i}' for i in range(1, 16)],
        'test_type': ['Tabletop Exercise', 'Walkthrough', 'Simulation', 'Full Recovery', 'Partial Recovery'] * 3,
        'test_date': pd.date_range(start='2024-01-01', periods=15, freq='D'),
        'duration_hours': np.random.randint(2, 8, 15),
        'participants': np.random.randint(5, 20, 15),
        'scenarios_tested': np.random.randint(1, 5, 15),
        'success_rate': np.random.uniform(70.0, 100.0, 15),
        'issues_found': np.random.randint(0, 10, 15),
        'lessons_learned': np.random.randint(1, 8, 15),
        'next_test_recommendation': pd.date_range(start='2024-06-01', periods=15, freq='D'),
        'test_coordinator': [f'Coordinator{i}' for i in range(1, 16)]
    })
    
    return {
        'DR_Scenarios': dr_scenarios_data,
        'Business_Impact': business_impact_data,
        'Recovery_Procedures': recovery_procedures_data,
        'Recovery_Team': recovery_team_data,
        'Recovery_Testing': recovery_testing_data
    }

def main():
    # Configure page for wide layout
    st.set_page_config(
        page_title="IT Analytics Dashboard",
        page_icon="",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Load custom CSS
    load_custom_css()
    
    # Modern header
    st.markdown("""
    <div class="main-header">
        <h1 style="margin: 0; font-size: 2.5rem; font-weight: 700;"> IT Analytics Dashboard</h1>
    </div>
    """, unsafe_allow_html=True)
    
    # Initialize session state
    if 'servers_data' not in st.session_state:
        st.session_state.servers_data = pd.DataFrame()
    if 'network_devices_data' not in st.session_state:
        st.session_state.network_devices_data = pd.DataFrame()
    if 'applications_data' not in st.session_state:
        st.session_state.applications_data = pd.DataFrame()
    if 'incidents_data' not in st.session_state:
        st.session_state.incidents_data = pd.DataFrame()
    if 'tickets_data' not in st.session_state:
        st.session_state.tickets_data = pd.DataFrame()
    if 'assets_data' not in st.session_state:
        st.session_state.assets_data = pd.DataFrame()
    if 'security_events_data' not in st.session_state:
        st.session_state.security_events_data = pd.DataFrame()
    if 'backups_data' not in st.session_state:
        st.session_state.backups_data = pd.DataFrame()
    if 'projects_data' not in st.session_state:
        st.session_state.projects_data = pd.DataFrame()
    if 'users_data' not in st.session_state:
        st.session_state.users_data = pd.DataFrame()
    
    # Sidebar navigation for main sections
    with st.sidebar:
        st.markdown("""
        <div style="padding: 20px 0; border-bottom: 1px solid rgba(255,255,255,0.2); margin-bottom: 20px;">
            <h3 style="color: #4CAF50; margin-bottom: 15px; text-align: center; font-size: 1.2rem; font-weight: 600; text-shadow: 0 1px 2px rgba(0,0,0,0.3);">
                 Navigation
            </h3>
            <p style="color: #2196F3; text-align: center; margin: 0; font-size: 0.85rem; font-weight: 500;">
                Select a section to explore
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # Main navigation buttons
        if st.button(" Home", key="nav_home", use_container_width=True):
            st.session_state.current_page = " Home"
        
        if st.button(" Data Input", key="nav_data_input", use_container_width=True):
            st.session_state.current_page = " Data Input"
        
        if st.button(" Infrastructure & Systems", key="nav_infrastructure", use_container_width=True):
            st.session_state.current_page = " Infrastructure & Systems"
        
        if st.button(" Security & Risk", key="nav_security", use_container_width=True):
            st.session_state.current_page = " Security & Risk"
        
        if st.button(" IT Support", key="nav_support", use_container_width=True):
            st.session_state.current_page = " IT Support"
        
        if st.button(" Asset Management", key="nav_assets", use_container_width=True):
            st.session_state.current_page = " Asset Management"
        
        if st.button(" Data Management", key="nav_data_mgmt", use_container_width=True):
            st.session_state.current_page = " Data Management"
        
        if st.button(" Project Management", key="nav_projects", use_container_width=True):
            st.session_state.current_page = " Project Management"
        
        if st.button(" User Experience", key="nav_ux", use_container_width=True):
            st.session_state.current_page = " User Experience"
        
        if st.button(" Cost Optimization", key="nav_cost", use_container_width=True):
            st.session_state.current_page = " Cost Optimization"
        
        if st.button(" Strategy & Innovation", key="nav_strategy", use_container_width=True):
            st.session_state.current_page = " Strategy & Innovation"
        
        if st.button(" Training & Development", key="nav_training", use_container_width=True):
            st.session_state.current_page = " Training & Development"
        
        if st.button(" Disaster Recovery", key="nav_disaster", use_container_width=True):
            st.session_state.current_page = " Disaster Recovery"
        
        if st.button(" Integration", key="nav_integration", use_container_width=True):
            st.session_state.current_page = " Integration"
        
        if st.button(" Predictive Analytics", key="nav_predictive", use_container_width=True):
            st.session_state.current_page = " Predictive Analytics"
        
        # Developer attribution at the bottom of sidebar
        st.markdown("---")
        st.markdown("""
        <div style="padding: 12px 0; text-align: center;">
            <p style="color: #95a5a6; font-size: 0.75rem; margin: 0; line-height: 1.3;">
                Developed by <strong style="color: #3498db;">Aryan Zabihi</strong><br>
                <a href="https://github.com/Aryanzabihi" target="_blank" style="color: #3498db; text-decoration: none;">GitHub</a>  
                <a href="https://www.linkedin.com/in/aryanzabihi/" target="_blank" style="color: #3498db; text-decoration: none;">LinkedIn</a>
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # Initialize current page if not set
        if 'current_page' not in st.session_state:
            st.session_state.current_page = " Home"
        
        page = st.session_state.current_page
    
    # Main content area
    if page == " Home":
        show_home()
    
    elif page == " Data Input":
        show_data_input()
    
    elif page == " Infrastructure & Systems":
        show_infrastructure_systems()
    
    elif page == " Security & Risk":
        show_security_risk()
    
    elif page == " IT Support":
        show_it_support()
    
    elif page == " Asset Management":
        show_asset_management()
    
    elif page == " Data Management":
        show_data_management()
    
    elif page == " Project Management":
        show_project_management()
    
    elif page == " User Experience":
        show_user_experience()
    
    elif page == " Cost Optimization":
        show_cost_optimization()
    
    elif page == " Strategy & Innovation":
        show_strategy_innovation()
    
    elif page == " Training & Development":
        show_training_development()
    
    elif page == " Disaster Recovery":
        show_disaster_recovery()
    
    elif page == " Integration":
        show_integration()
    
    elif page == " Predictive Analytics":
        show_predictive_analytics()

def show_home():
    # Welcome section with modern styling
    st.markdown("""
    <div class="welcome-section">
        <h2 style="color: #1e3c72; margin-bottom: 20px; text-align: center;"> Welcome to the IT Analytics Dashboard</h2>
        <p style="color: #4a5568; font-size: 1.1rem; line-height: 1.6; text-align: center;">
            This comprehensive tool helps you calculate and analyze key IT metrics across multiple categories, 
            providing insights to optimize your IT operations and drive strategic decisions.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Quick metrics overview
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("""
        <div class="metric-card-blue">
            <h3 style="margin: 0; font-size: 1.2rem;"> Infrastructure</h3>
            <p style="margin: 5px 0 0 0; font-size: 0.9rem;">System Performance & Uptime</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="metric-card-purple">
            <h3 style="margin: 0; font-size: 1.2rem;"> Security</h3>
            <p style="margin: 5px 0 0 0; font-size: 0.9rem;">Risk Management & Compliance</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="metric-card-green">
            <h3 style="margin: 0; font-size: 1.2rem;"> Support</h3>
            <p style="margin: 5px 0 0 0; font-size: 0.9rem;">Help Desk & User Experience</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown("""
        <div class="metric-card-orange">
            <h3 style="margin: 0; font-size: 1.2rem;"> Analytics</h3>
            <p style="margin: 5px 0 0 0; font-size: 0.9rem;">Data Insights & Reporting</p>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Available metrics categories
    st.markdown("""
    <div class="welcome-section">
        <h3 style="color: #1e3c72; margin-bottom: 20px;"> Available Metrics Categories</h3>
    </div>
    """, unsafe_allow_html=True)
    
    # Create a grid layout for categories
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> Infrastructure & Systems Performance</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>Server Uptime Analysis</li>
                <li>Network Latency and Speed Analysis</li>
                <li>System Load Analysis</li>
                <li>Application Performance Monitoring (APM)</li>
                <li>Incident Response Time Analysis</li>
                <li>System Availability Analysis</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> Security & Risk Management</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>Vulnerability Analysis</li>
                <li>Firewall Performance Analysis</li>
                <li>Data Breach Incident Analysis</li>
                <li>Access Control Analysis</li>
                <li>Phishing Attack Metrics</li>
                <li>Encryption Effectiveness</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> IT Support & Help Desk</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>Ticket Resolution Rate</li>
                <li>First Call Resolution (FCR)</li>
                <li>Average Ticket Resolution Time</li>
                <li>Ticket Volume Analysis</li>
                <li>User Satisfaction Metrics</li>
                <li>Recurring Issue Analysis</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> Asset Management</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>IT Asset Utilization</li>
                <li>Hardware Lifecycle Analysis</li>
                <li>Software Licensing Compliance</li>
                <li>Cloud Resource Utilization</li>
                <li>IT Inventory Turnover</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> Data Management & Analytics</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>Data Quality Analysis</li>
                <li>Database Performance Metrics</li>
                <li>Backup Success Rate</li>
                <li>Data Loss Metrics</li>
                <li>Storage Usage Trends</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> Project Management & Development</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>Project Delivery Metrics</li>
                <li>Development Cycle Time</li>
                <li>Code Quality Analysis</li>
                <li>Release Management Metrics</li>
                <li>Agile Metrics</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> User Experience & Accessibility</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>Application Usability Metrics</li>
                <li>Website Performance Analysis</li>
                <li>Accessibility Compliance Analysis</li>
                <li>User Feedback Analysis</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> Cost & Resource Optimization</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>IT Budget Utilization</li>
                <li>Cost Per User or Device</li>
                <li>Cloud Cost Analysis</li>
                <li>Energy Efficiency Analysis</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> IT Strategy & Innovation</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>Technology Adoption Metrics</li>
                <li>ROI on IT Investments</li>
                <li>Emerging Technology Feasibility</li>
                <li>IT Alignment with Business Goals</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> Training & Development</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>Employee Training Effectiveness</li>
                <li>IT Certification Metrics</li>
                <li>End-User Training Participation</li>
                <li>Skill Gap Analysis</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> Disaster Recovery & Business Continuity</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>Disaster Recovery Readiness</li>
                <li>Recovery Time Objective (RTO) Metrics</li>
                <li>Disaster Recovery Testing Success</li>
                <li>Business Continuity Downtime Analysis</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> Integration & Interoperability</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>System Integration Metrics</li>
                <li>Data Flow Efficiency</li>
                <li>API Performance Metrics</li>
                <li>Interoperability Compliance</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="metric-card">
            <h4 style="color: #1e3c72; margin: 0 0 15px 0;"> Predictive Analytics</h4>
            <ul style="color: #4a5568; margin: 0; padding-left: 20px;">
                <li>Infrastructure Capacity Forecasting</li>
                <li>Incident Prediction Models</li>
                <li>Cost Trend Analysis</li>
                <li>Performance Optimization</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Getting started section
    st.markdown("""
    <div class="welcome-section">
        <h3 style="color: #1e3c72; margin-bottom: 20px;"> Getting Started</h3>
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px;">
            <div style="background: #f7fafc; padding: 20px; border-radius: 10px; border-left: 4px solid #4caf50;">
                <h4 style="color: #1e3c72; margin: 0 0 10px 0;">1.  Upload Data</h4>
                <p style="color: #4a5568; margin: 0; font-size: 0.9rem;">Use the Data Input section to upload your IT data</p>
            </div>
            <div style="background: #f7fafc; padding: 20px; border-radius: 10px; border-left: 4px solid #2196f3;">
                <h4 style="color: #1e3c72; margin: 0 0 10px 0;">2.  Download Template</h4>
                <p style="color: #4a5568; margin: 0; font-size: 0.9rem;">Get the Excel template with the correct data schema</p>
            </div>
            <div style="background: #f7fafc; padding: 20px; border-radius: 10px; border-left: 4px solid #ff9800;">
                <h4 style="color: #1e3c72; margin: 0 0 10px 0;">3.  Analyze Metrics</h4>
                <p style="color: #4a5568; margin: 0; font-size: 0.9rem;">Navigate through the sections to explore different analytics</p>
            </div>
            <div style="background: #f7fafc; padding: 20px; border-radius: 10px; border-left: 4px solid #9c27b0;">
                <h4 style="color: #1e3c72; margin: 0 0 10px 0;">4.  Export Results</h4>
                <p style="color: #4a5568; margin: 0; font-size: 0.9rem;">Download your analysis results and reports</p>
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Data requirements section
    st.markdown("""
    <div class="welcome-section">
        <h3 style="color: #1e3c72; margin-bottom: 20px;"> Data Requirements</h3>
        <p style="color: #4a5568; font-size: 1rem; line-height: 1.6; margin-bottom: 20px;">
            The application requires data in the following categories. Each category has specific required fields 
            that are detailed in the Data Input section.
        </p>
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px;">
            <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; text-align: center;">
                <h5 style="color: #1565c0; margin: 0 0 8px 0;"> Servers</h5>
                <p style="color: #4a5568; margin: 0; font-size: 0.85rem;">Infrastructure and server information</p>
            </div>
            <div style="background: #e8f5e8; padding: 15px; border-radius: 8px; text-align: center;">
                <h5 style="color: #2e7d32; margin: 0 0 8px 0;"> Network Devices</h5>
                <p style="color: #4a5568; margin: 0; font-size: 0.85rem;">Network equipment and performance data</p>
            </div>
            <div style="background: #fff3e0; padding: 15px; border-radius: 8px; text-align: center;">
                <h5 style="color: #ef6c00; margin: 0 0 8px 0;"> Applications</h5>
                <p style="color: #4a5568; margin: 0; font-size: 0.85rem;">Software applications and their status</p>
            </div>
            <div style="background: #fce4ec; padding: 15px; border-radius: 8px; text-align: center;">
                <h5 style="color: #c2185b; margin: 0 0 8px 0;"> Incidents</h5>
                <p style="color: #4a5568; margin: 0; font-size: 0.85rem;">IT incidents and resolution times</p>
            </div>
            <div style="background: #f3e5f5; padding: 15px; border-radius: 8px; text-align: center;">
                <h5 style="color: #7b1fa2; margin: 0 0 8px 0;"> Tickets</h5>
                <p style="color: #4a5568; margin: 0; font-size: 0.85rem;">Support tickets and user requests</p>
            </div>
            <div style="background: #e0f2f1; padding: 15px; border-radius: 8px; text-align: center;">
                <h5 style="color: #00695c; margin: 0 0 8px 0;"> Assets</h5>
                <p style="color: #4a5568; margin: 0; font-size: 0.85rem;">IT assets and their lifecycle information</p>
            </div>
            <div style="background: #fff8e1; padding: 15px; border-radius: 8px; text-align: center;">
                <h5 style="color: #f57f17; margin: 0 0 8px 0;"> Security Events</h5>
                <p style="color: #4a5568; margin: 0; font-size: 0.85rem;">Security incidents and threat data</p>
            </div>
            <div style="background: #e8eaf6; padding: 15px; border-radius: 8px; text-align: center;">
                <h5 style="color: #3f51b5; margin: 0 0 8px 0;"> Backups</h5>
                <p style="color: #4a5568; margin: 0; font-size: 0.85rem;">Backup operations and success rates</p>
            </div>
            <div style="background: #f1f8e9; padding: 15px; border-radius: 8px; text-align: center;">
                <h5 style="color: #558b2f; margin: 0 0 8px 0;"> Projects</h5>
                <p style="color: #4a5568; margin: 0; font-size: 0.85rem;">IT projects and development metrics</p>
            </div>
            <div style="background: #fafafa; padding: 15px; border-radius: 8px; text-align: center;">
                <h5 style="color: #424242; margin: 0 0 8px 0;"> Users</h5>
                <p style="color: #4a5568; margin: 0; font-size: 0.85rem;">User information and access data</p>
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)

def show_data_input():
    """Show data input forms and file upload options"""
    
    # Main header for the page
    st.markdown("""
    <div class="welcome-section">
        <h2 style="color: #1e3c72; margin-bottom: 20px; text-align: center;"> Data Input & Management</h2>
        <p style="color: #4a5568; font-size: 1.1rem; line-height: 1.6; text-align: center;">
            Upload your IT data using our template or add data manually through the forms below.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Create tabs for different data input methods
    tab1, tab2, tab3, tab4 = st.tabs([
        " Template", " Upload Data", " Manual Entry", " Sample Data Sets"
    ])
    
    # Tab 1: Template Download
    with tab1:
        st.markdown("###  Download Data Template")
        
        st.markdown("Download the Excel template with all required data schema, fill it with your data, and upload it back.")
        
        # Create template for download
        if st.button(" Download Data Template", use_container_width=True):
            template = create_template_for_download()
            st.download_button(
                label=" Download Template",
                data=template.getvalue(),
                file_name="IT_Analytics_Template.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            st.success(" Template downloaded successfully! Fill it with your data and upload it in the 'Upload Data' tab.")
        
        # Add some spacing for visual balance
        st.markdown("")
        st.markdown("**Template includes:**")
        st.markdown(" 10 data tables in separate sheets")
        st.markdown(" Instructions sheet with field descriptions")
        st.markdown(" Proper column headers and data types")
        
        # Quick stats display
        if any([not st.session_state.servers_data.empty, not st.session_state.network_devices_data.empty, 
                not st.session_state.applications_data.empty, not st.session_state.incidents_data.empty,
                not st.session_state.tickets_data.empty, not st.session_state.assets_data.empty,
                not st.session_state.security_events_data.empty, not st.session_state.backups_data.empty,
                not st.session_state.projects_data.empty, not st.session_state.users_data.empty]):
            
            st.markdown("---")
            st.markdown("###  Current Data Overview")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                if not st.session_state.servers_data.empty:
                    st.metric(" Servers", len(st.session_state.servers_data))
                if not st.session_state.network_devices_data.empty:
                    st.metric(" Network Devices", len(st.session_state.network_devices_data))
            
            with col2:
                if not st.session_state.applications_data.empty:
                    st.metric(" Applications", len(st.session_state.applications_data))
                if not st.session_state.incidents_data.empty:
                    st.metric(" Incidents", len(st.session_state.incidents_data))
            
            with col3:
                if not st.session_state.tickets_data.empty:
                    st.metric(" Tickets", len(st.session_state.tickets_data))
                if not st.session_state.assets_data.empty:
                    st.metric(" Assets", len(st.session_state.assets_data))
            
            with col4:
                if not st.session_state.security_events_data.empty:
                    st.metric(" Security Events", len(st.session_state.security_events_data))
                if not st.session_state.backups_data.empty:
                    st.metric(" Backups", len(st.session_state.backups_data))
    
    # Tab 2: Upload Data
    with tab2:
        st.markdown("###  Upload Your Data")
        st.markdown("Upload your filled Excel template:")
        
        # File upload for Excel template
        uploaded_file = st.file_uploader(
            "Upload Excel file with all tables", 
            type=['xlsx', 'xls'],
            help="Upload the filled Excel template with all 10 tables in separate sheets"
        )
        
        # Add helpful information
        st.markdown("")
        st.markdown("**Upload features:**")
        st.markdown(" Automatic validation of all sheets")
        st.markdown(" Import all 10 tables at once")
        st.markdown(" Error checking and feedback")
        
        required_sheets = [
            "Servers", "Network_Devices", "Applications", "Incidents", "Tickets",
            "Assets", "Security_Events", "Backups", "Projects", "Users"
        ]
    
        if uploaded_file is not None:
            try:
                # Read all sheets from the Excel file
                excel_data = pd.read_excel(uploaded_file, sheet_name=None)
                
                # Check if all required sheets are present
                available_sheets = list(excel_data.keys())
                missing_sheets = [sheet for sheet in required_sheets if sheet not in excel_data.keys()]
                
                if missing_sheets:
                    st.error(f" Missing required sheets: {', '.join(missing_sheets)}")
                    st.info("Please ensure your Excel file contains all 10 required sheets.")
                else:
                    # Load data into session state
                    st.session_state.servers_data = excel_data['Servers']
                    st.session_state.network_devices_data = excel_data['Network_Devices']
                    st.session_state.applications_data = excel_data['Applications']
                    st.session_state.incidents_data = excel_data['Incidents']
                    st.session_state.tickets_data = excel_data['Tickets']
                    st.session_state.assets_data = excel_data['Assets']
                    st.session_state.security_events_data = excel_data['Security_Events']
                    st.session_state.backups_data = excel_data['Backups']
                    st.session_state.projects_data = excel_data['Projects']
                    st.session_state.users_data = excel_data['Users']
                    
                    st.success(" All data loaded successfully from Excel file!")
                    st.info(f" Loaded {len(st.session_state.servers_data)} servers, {len(st.session_state.applications_data)} applications, {len(st.session_state.tickets_data)} tickets, and more...")
                    
            except Exception as e:
                st.error(f" Error reading Excel file: {str(e)}")
                st.info("Please ensure the file is a valid Excel file with the correct format.")
    
    # Tab 3: Manual Data Entry
    with tab3:
        st.markdown("###  Manual Data Entry")
        st.markdown("Add data manually using the forms below:")
        
        # Create sub-tabs for all the individual data entry forms
        sub_tab1, sub_tab2, sub_tab3, sub_tab4, sub_tab5, sub_tab6, sub_tab7, sub_tab8, sub_tab9, sub_tab10 = st.tabs([
            " Servers", " Network Devices", " Applications", " Incidents", " Tickets", 
            " Assets", " Security Events", " Backups", " Projects", " Users"
        ])
        
        # Sub-tab 1: Servers
        with sub_tab1:
            st.markdown("###  Servers")
            st.markdown("Add and manage server infrastructure data.")
            
            col1, col2 = st.columns(2)
            
            with col1:
                server_id = st.text_input("Server ID", key="server_id_input")
                server_name = st.text_input("Server Name", key="server_name_input")
                server_type = st.selectbox("Server Type", ["Web Server", "Database Server", "Application Server", "File Server", "Mail Server"], key="server_type_input")
                location = st.selectbox("Location", ["Data Center A", "Data Center B", "Cloud AWS", "Cloud Azure", "Office HQ"], key="location_input")
                ip_address = st.text_input("IP Address", key="ip_address_input")
            
            with col2:
                os_version = st.selectbox("OS Version", ["Windows Server 2019", "Windows Server 2022", "Ubuntu 20.04", "Ubuntu 22.04", "CentOS 7", "CentOS 8"], key="os_version_input")
                cpu_cores = st.number_input("CPU Cores", min_value=1, value=4, key="cpu_cores_input")
                ram_gb = st.number_input("RAM (GB)", min_value=1, value=8, key="ram_gb_input")
                storage_tb = st.number_input("Storage (TB)", min_value=1, value=1, key="storage_tb_input")
                status = st.selectbox("Status", ["Active", "Maintenance", "Retired"], key="status_input")
            
            if st.button("Add Server"):
                new_server = pd.DataFrame([{
                    'server_id': server_id,
                    'server_name': server_name,
                    'server_type': server_type,
                    'location': location,
                    'ip_address': ip_address,
                    'os_version': os_version,
                    'cpu_cores': cpu_cores,
                    'ram_gb': ram_gb,
                    'storage_tb': storage_tb,
                    'status': status,
                    'last_maintenance': pd.Timestamp.now()
                }])
                st.session_state.servers_data = pd.concat([st.session_state.servers_data, new_server], ignore_index=True)
                st.success("Server added successfully!")
            
            # Display existing data
            if not st.session_state.servers_data.empty:
                st.markdown("#### Existing Servers")
                display_dataframe_with_index_1(st.session_state.servers_data)
        
        # Sub-tab 2: Network Devices
        with sub_tab2:
            st.markdown("###  Network Devices")
            st.markdown("Add and manage network infrastructure data.")
            
            col1, col2 = st.columns(2)
            
            with col1:
                device_id = st.text_input("Device ID", key="device_id_input")
                device_name = st.text_input("Device Name", key="device_name_input")
                device_type = st.selectbox("Device Type", ["Router", "Switch", "Firewall", "Load Balancer", "Wireless AP"], key="device_type_input")
                location = st.selectbox("Location", ["Data Center A", "Data Center B", "Cloud AWS", "Cloud Azure", "Office HQ"], key="device_location_input")
                ip_address = st.text_input("IP Address", key="device_ip_input")
            
            with col2:
                model = st.selectbox("Model", ["Cisco 2960", "Cisco 3850", "Fortinet FG-60F", "F5 BIG-IP", "Aruba AP-515"], key="model_input")
                firmware_version = st.text_input("Firmware Version", value="v1.0.0", key="firmware_input")
                status = st.selectbox("Status", ["Active", "Backup", "Maintenance"], key="device_status_input")
            
            if st.button("Add Network Device"):
                new_device = pd.DataFrame([{
                    'device_id': device_id,
                    'device_name': device_name,
                    'device_type': device_type,
                    'location': location,
                    'ip_address': ip_address,
                    'model': model,
                    'firmware_version': firmware_version,
                    'status': status,
                    'last_backup': pd.Timestamp.now()
                }])
                st.session_state.network_devices_data = pd.concat([st.session_state.network_devices_data, new_device], ignore_index=True)
                st.success("Network Device added successfully!")
            
            # Display existing data
            if not st.session_state.network_devices_data.empty:
                st.markdown("#### Existing Network Devices")
                display_dataframe_with_index_1(st.session_state.network_devices_data)
        
        # Sub-tab 3: Applications
        with sub_tab3:
            st.markdown("###  Applications")
            st.markdown("Add and manage application data.")
            
            col1, col2 = st.columns(2)
            
            with col1:
                app_id = st.text_input("Application ID", key="app_id_input")
                app_name = st.text_input("Application Name", key="app_name_input")
                app_type = st.selectbox("Application Type", ["Web Application", "Database", "Email System", "CRM", "ERP", "BI Tool", "Security Tool"], key="app_type_input")
                version = st.text_input("Version", value="1.0.0", key="version_input")
                server_id = st.text_input("Server ID", key="app_server_id_input")
            
            with col2:
                department = st.selectbox("Department", ["IT", "HR", "Finance", "Sales", "Marketing", "Operations"], key="app_dept_input")
                status = st.selectbox("Status", ["Active", "Development", "Maintenance", "Retired"], key="app_status_input")
                last_updated = st.date_input("Last Updated", key="app_updated_input")
                criticality = st.selectbox("Criticality", ["Low", "Medium", "High", "Critical"], key="app_criticality_input")
            
            if st.button("Add Application"):
                new_app = pd.DataFrame([{
                    'app_id': app_id,
                    'app_name': app_name,
                    'app_type': app_type,
                    'version': version,
                    'server_id': server_id,
                    'department': department,
                    'status': status,
                    'last_updated': last_updated,
                    'criticality': criticality
                }])
                st.session_state.applications_data = pd.concat([st.session_state.applications_data, new_app], ignore_index=True)
                st.success("Application added successfully!")
            
            # Display existing data
            if not st.session_state.applications_data.empty:
                st.markdown("#### Existing Applications")
                display_dataframe_with_index_1(st.session_state.applications_data)
        
        # Sub-tab 4: Incidents
        with sub_tab4:
            st.markdown("###  Incidents")
            st.markdown("Add and manage incident data.")
            
            col1, col2 = st.columns(2)
            
            with col1:
                incident_id = st.text_input("Incident ID", key="incident_id_input")
                title = st.text_input("Title", key="incident_title_input")
                priority = st.selectbox("Priority", ["Low", "Medium", "High", "Critical"], key="incident_priority_input")
                status = st.selectbox("Status", ["Open", "In Progress", "Resolved", "Closed"], key="incident_status_input")
                category = st.selectbox("Category", ["Hardware", "Software", "Network", "Security", "User Error"], key="incident_category_input")
            
            with col2:
                reported_by = st.text_input("Reported By", key="incident_reporter_input")
                assigned_to = st.text_input("Assigned To", key="incident_assignee_input")
                created_date = st.date_input("Created Date", key="incident_created_input")
                resolved_date = st.date_input("Resolved Date", key="incident_resolved_input")
            
            if st.button("Add Incident"):
                new_incident = pd.DataFrame([{
                    'incident_id': incident_id,
                    'title': title,
                    'priority': priority,
                    'status': status,
                    'category': category,
                    'reported_by': reported_by,
                    'assigned_to': assigned_to,
                    'created_date': created_date,
                    'resolved_date': resolved_date
                }])
                st.session_state.incidents_data = pd.concat([st.session_state.incidents_data, new_incident], ignore_index=True)
                st.success("Incident added successfully!")
            
            # Display existing data
            if not st.session_state.incidents_data.empty:
                st.markdown("#### Existing Incidents")
                display_dataframe_with_index_1(st.session_state.incidents_data)
        
        # Sub-tab 5: Tickets
        with sub_tab5:
            st.markdown("###  Tickets")
            st.markdown("Add and manage support ticket data.")
            
            col1, col2 = st.columns(2)
            
            with col1:
                ticket_id = st.text_input("Ticket ID", key="ticket_id_input")
                title = st.text_input("Title", key="ticket_title_input")
                priority = st.selectbox("Priority", ["Low", "Medium", "High", "Critical"], key="ticket_priority_input")
                status = st.selectbox("Status", ["Open", "In Progress", "Waiting", "Resolved", "Closed"], key="ticket_status_input")
                category = st.selectbox("Category", ["Technical", "Access", "Software", "Hardware", "Network"], key="ticket_category_input")
            
            with col2:
                requester = st.text_input("Requester", key="ticket_requester_input")
                assignee = st.text_input("Assignee", key="ticket_assignee_input")
                created_date = st.date_input("Created Date", key="ticket_created_input")
                due_date = st.date_input("Due Date", key="ticket_due_input")
            
            if st.button("Add Ticket"):
                new_ticket = pd.DataFrame([{
                    'ticket_id': ticket_id,
                    'title': title,
                    'priority': priority,
                    'status': status,
                    'category': category,
                    'requester': requester,
                    'assignee': assignee,
                    'created_date': created_date,
                    'due_date': due_date
                }])
                st.session_state.tickets_data = pd.concat([st.session_state.tickets_data, new_ticket], ignore_index=True)
                st.success("Ticket added successfully!")
            
            # Display existing data
            if not st.session_state.tickets_data.empty:
                st.markdown("#### Existing Tickets")
                display_dataframe_with_index_1(st.session_state.tickets_data)
        
        # Sub-tab 6: Assets
        with sub_tab6:
            st.markdown("###  Assets")
            st.markdown("Add and manage asset data.")
            
            col1, col2 = st.columns(2)
            
            with col1:
                asset_id = st.text_input("Asset ID", key="asset_id_input")
                asset_name = st.text_input("Asset Name", key="asset_name_input")
                asset_type = st.selectbox("Asset Type", ["Laptop", "Desktop", "Monitor", "Printer", "Mobile Device"], key="asset_type_input")
                location = st.selectbox("Location", ["Office HQ", "Branch Office", "Home Office", "Warehouse"], key="asset_location_input")
                assigned_to = st.text_input("Assigned To", key="asset_assignee_input")
            
            with col2:
                model = st.text_input("Model", key="asset_model_input")
                vendor = st.text_input("Vendor", key="asset_vendor_input")
                purchase_date = st.date_input("Purchase Date", key="asset_purchase_input")
                warranty_expiry = st.date_input("Warranty Expiry", key="asset_warranty_input")
                status = st.selectbox("Status", ["Active", "Maintenance", "Retired", "Lost"], key="asset_status_input")
            
            if st.button("Add Asset"):
                new_asset = pd.DataFrame([{
                    'asset_id': asset_id,
                    'asset_name': asset_name,
                    'asset_type': asset_type,
                    'location': location,
                    'assigned_to': assigned_to,
                    'model': model,
                    'vendor': vendor,
                    'purchase_date': purchase_date,
                    'warranty_expiry': warranty_expiry,
                    'status': status
                }])
                st.session_state.assets_data = pd.concat([st.session_state.assets_data, new_asset], ignore_index=True)
                st.success("Asset added successfully!")
            
            # Display existing data
            if not st.session_state.assets_data.empty:
                st.markdown("#### Existing Assets")
                display_dataframe_with_index_1(st.session_state.assets_data)
        
        # Sub-tab 7: Security Events
        with sub_tab7:
            st.markdown("###  Security Events")
            st.markdown("Add and manage security event data.")
            
            col1, col2 = st.columns(2)
            
            with col1:
                event_id = st.text_input("Event ID", key="security_event_id_input")
                event_type = st.selectbox("Event Type", ["Login Attempt", "Data Access", "System Change", "Network Activity", "Malware Alert"], key="security_event_type_input")
                severity = st.selectbox("Severity", ["Low", "Medium", "High", "Critical"], key="security_severity_input")
                status = st.selectbox("Status", ["Open", "Investigating", "Resolved", "False Positive"], key="security_status_input")
                source_ip = st.text_input("Source IP", key="security_source_ip_input")
            
            with col2:
                target_system = st.text_input("Target System", key="security_target_input")
                user_involved = st.text_input("User Involved", key="security_user_input")
                timestamp_date = st.date_input("Timestamp Date", key="security_timestamp_date_input")
                timestamp_time = st.time_input("Timestamp Time", key="security_timestamp_time_input")
                description = st.text_area("Description", key="security_description_input")
            
            if st.button("Add Security Event"):
                # Combine date and time into datetime
                timestamp = pd.Timestamp.combine(timestamp_date, timestamp_time)
                
                new_security_event = pd.DataFrame([{
                    'event_id': event_id,
                    'event_type': event_type,
                    'severity': severity,
                    'status': status,
                    'source_ip': source_ip,
                    'target_system': target_system,
                    'user_involved': user_involved,
                    'timestamp': timestamp,
                    'description': description
                }])
                st.session_state.security_events_data = pd.concat([st.session_state.security_events_data, new_security_event], ignore_index=True)
                st.success("Security event added successfully!")
            
            # Display existing data
            if not st.session_state.security_events_data.empty:
                st.markdown("#### Existing Security Events")
                display_dataframe_with_index_1(st.session_state.security_events_data)
        
        # Sub-tab 8: Backups
        with sub_tab8:
            st.markdown("###  Backups")
            st.markdown("Add and manage backup data.")
            
            col1, col2 = st.columns(2)
            
            with col1:
                backup_id = st.text_input("Backup ID", key="backup_id_input")
                system_name = st.text_input("System Name", key="backup_system_input")
                backup_type = st.selectbox("Backup Type", ["Full", "Incremental", "Differential"], key="backup_type_input")
                status = st.selectbox("Status", ["Successful", "Failed", "In Progress", "Scheduled"], key="backup_status_input")
                size_gb = st.number_input("Size (GB)", min_value=0.1, value=10.0, key="backup_size_input")
            
            with col2:
                start_date = st.date_input("Start Date", key="backup_start_date_input")
                start_time = st.time_input("Start Time", key="backup_start_time_input")
                end_date = st.date_input("End Date", key="backup_end_date_input")
                end_time = st.time_input("End Time", key="backup_end_time_input")
                retention_days = st.number_input("Retention (Days)", min_value=1, value=30, key="backup_retention_input")
                location = st.selectbox("Location", ["Local", "Cloud", "Offsite"], key="backup_location_input")
            
            if st.button("Add Backup"):
                # Combine date and time into datetime
                start_datetime = pd.Timestamp.combine(start_date, start_time)
                end_datetime = pd.Timestamp.combine(end_date, end_time)
                
                new_backup = pd.DataFrame([{
                    'backup_id': backup_id,
                    'system_name': system_name,
                    'backup_type': backup_type,
                    'status': status,
                    'size_gb': size_gb,
                    'start_time': start_datetime,
                    'end_time': end_datetime,
                    'retention_days': retention_days,
                    'location': location
                }])
                st.session_state.backups_data = pd.concat([st.session_state.backups_data, new_backup], ignore_index=True)
                st.success("Backup added successfully!")
            
            # Display existing data
            if not st.session_state.backups_data.empty:
                st.markdown("#### Existing Backups")
                display_dataframe_with_index_1(st.session_state.backups_data)
        
        # Sub-tab 9: Projects
        with sub_tab9:
            st.markdown("###  Projects")
            st.markdown("Add and manage project data.")
            
            col1, col2 = st.columns(2)
            
            with col1:
                project_id = st.text_input("Project ID", key="project_id_input")
                project_name = st.text_input("Project Name", key="project_name_input")
                project_type = st.selectbox("Project Type", ["Infrastructure", "Application", "Security", "Migration", "Upgrade"], key="project_type_input")
                status = st.selectbox("Status", ["Planning", "In Progress", "On Hold", "Completed", "Cancelled"], key="project_status_input")
                priority = st.selectbox("Priority", ["Low", "Medium", "High", "Critical"], key="project_priority_input")
            
            with col2:
                manager = st.text_input("Project Manager", key="project_manager_input")
                start_date = st.date_input("Start Date", key="project_start_input")
                end_date = st.date_input("End Date", key="project_end_input")
                budget = st.number_input("Budget ($)", min_value=0, value=10000, key="project_budget_input")
            
            if st.button("Add Project"):
                new_project = pd.DataFrame([{
                    'project_id': project_id,
                    'project_name': project_name,
                    'project_type': project_type,
                    'status': status,
                    'priority': priority,
                    'manager': manager,
                    'start_date': start_date,
                    'end_date': end_date,
                    'budget': budget
                }])
                st.session_state.projects_data = pd.concat([st.session_state.projects_data, new_project], ignore_index=True)
                st.success("Project added successfully!")
            
            # Display existing data
            if not st.session_state.projects_data.empty:
                st.markdown("#### Existing Projects")
                display_dataframe_with_index_1(st.session_state.projects_data)
        
        # Sub-tab 10: Users
        with sub_tab10:
            st.markdown("###  Users")
            st.markdown("Add and manage user data.")
            
            col1, col2 = st.columns(2)
            
            with col1:
                user_id = st.text_input("User ID", key="user_id_input")
                username = st.text_input("Username", key="user_username_input")
                full_name = st.text_input("Full Name", key="user_fullname_input")
                email = st.text_input("Email", key="user_email_input")
                department = st.selectbox("Department", ["IT", "HR", "Finance", "Sales", "Marketing", "Operations"], key="user_dept_input")
            
            with col2:
                role = st.selectbox("Role", ["User", "Admin", "Manager", "Supervisor"], key="user_role_input")
                status = st.selectbox("Status", ["Active", "Inactive", "Suspended"], key="user_status_input")
                created_date = st.date_input("Created Date", key="user_created_input")
                last_login_date = st.date_input("Last Login Date", key="user_lastlogin_date_input")
                last_login_time = st.time_input("Last Login Time", key="user_lastlogin_time_input")
            
            if st.button("Add User"):
                # Combine date and time into datetime
                last_login = pd.Timestamp.combine(last_login_date, last_login_time)
                
                new_user = pd.DataFrame([{
                    'user_id': user_id,
                    'username': username,
                    'full_name': full_name,
                    'email': email,
                    'department': department,
                    'role': role,
                    'status': status,
                    'created_date': created_date,
                    'last_login': last_login
                }])
                st.session_state.users_data = pd.concat([st.session_state.users_data, new_user], ignore_index=True)
                st.success("User added successfully!")
            
            # Display existing data
            if not st.session_state.users_data.empty:
                st.markdown("#### Existing Users")
                display_dataframe_with_index_1(st.session_state.users_data)
    # Tab 4: Sample Data Sets
    with tab4:
        show_sample_data_sets()

    

    

    

    

    

    

    

    

    


# IT Metric Calculation Functions
def calculate_server_uptime(servers_data, incidents_data):
    """Calculate server uptime percentage"""
    if servers_data.empty:
        return pd.DataFrame(), "No server data available"
    
    # Calculate uptime for each server
    uptime_data = []
    for _, server in servers_data.iterrows():
        server_name = server.get('server_name', 'Unknown')
        uptime_percentage = server.get('uptime_percentage', 95.0)
        
        uptime_data.append({
            'server_name': server_name,
            'uptime_percentage': uptime_percentage
        })
    
    uptime_df = pd.DataFrame(uptime_data)
    message = f"Uptime analysis for {len(uptime_df)} servers"
    return uptime_df, message

def calculate_network_latency(network_devices_data, incidents_data):
    """Calculate network device latency"""
    if network_devices_data.empty:
        return pd.DataFrame(), "No network device data available"
    
    latency_data = []
    for _, device in network_devices_data.iterrows():
        device_name = device.get('device_name', 'Unknown')
        device_type = device.get('device_type', 'Unknown')
        avg_latency = device.get('latency_ms', 10.0)
        
        latency_data.append({
            'device_name': device_name,
            'device_type': device_type,
            'avg_latency_ms': avg_latency
        })
    
    latency_df = pd.DataFrame(latency_data)
    message = f"Latency analysis for {len(latency_df)} network devices"
    return latency_df, message

def calculate_system_load(servers_data, applications_data):
    """Calculate system load based on applications"""
    if servers_data.empty or applications_data.empty:
        return pd.DataFrame(), "No server or application data available"
    
    load_data = []
    for _, server in servers_data.iterrows():
        server_name = server.get('server_name', 'Unknown')
        # Count applications on this server
        app_count = len(applications_data[applications_data.get('server_id', '') == server.get('server_id', '')])
        load_percentage = min(100, app_count * 10)  # Simple load calculation
        
        load_data.append({
            'server_name': server_name,
            'app_count': app_count,
            'load_percentage': load_percentage
        })
    
    load_df = pd.DataFrame(load_data)
    message = f"System load analysis for {len(load_df)} servers"
    return load_df, message

def calculate_application_performance(applications_data, incidents_data):
    """Calculate application performance metrics"""
    if applications_data.empty:
        return pd.DataFrame(), "No application data available"
    
    perf_data = []
    for _, app in applications_data.iterrows():
        app_name = app.get('app_name', 'Unknown')
        critical_level = app.get('critical_level', 'Medium')
        
        # Generate mock performance data
        response_time = np.random.uniform(50, 2000)
        error_rate = np.random.uniform(0, 5)
        health_score = np.random.uniform(60, 100)
        
        perf_data.append({
            'app_name': app_name,
            'critical_level': critical_level,
            'response_time_ms': response_time,
            'error_rate_percent': error_rate,
            'health_score': health_score
        })
    
    perf_df = pd.DataFrame(perf_data)
    message = f"Performance analysis for {len(perf_df)} applications"
    return perf_df, message

def calculate_incident_response_time(incidents_data):
    """Calculate incident response time metrics"""
    if incidents_data.empty:
        return pd.DataFrame(), "No incident data available"
    
    # Group by priority and calculate mean resolution time
    if 'priority' in incidents_data.columns and 'resolution_time_minutes' in incidents_data.columns:
        response_data = incidents_data.groupby('priority').agg({
            'resolution_time_minutes': ['mean', 'count']
        }).reset_index()
        response_data.columns = ['priority', 'mean', 'count']
    else:
        # Create mock data if columns don't exist
        response_data = pd.DataFrame({
            'priority': ['Low', 'Medium', 'High', 'Critical'],
            'mean': [120, 240, 360, 480],
            'count': [10, 15, 8, 5]
        })
    
    message = f"Response time analysis for {response_data['count'].sum()} incidents"
    return response_data, message

def calculate_system_availability(servers_data, applications_data, incidents_data):
    """Calculate system availability metrics"""
    if servers_data.empty:
        return pd.DataFrame(), "No server data available"
    
    availability_data = []
    for _, server in servers_data.iterrows():
        server_name = server.get('server_name', 'Unknown')
        availability_percentage = server.get('uptime_percentage', 95.0)
        app_count = len(applications_data[applications_data.get('server_id', '') == server.get('server_id', '')])
        critical_apps = len(applications_data[
            (applications_data.get('server_id', '') == server.get('server_id', '')) & 
            (applications_data.get('critical_level', '') == 'Critical')
        ])
        
        availability_data.append({
            'server_name': server_name,
            'availability_percentage': availability_percentage,
            'app_count': app_count,
            'critical_apps': critical_apps
        })
    
    availability_df = pd.DataFrame(availability_data)
    message = f"Availability analysis for {len(availability_df)} servers"
    return availability_df, message

def calculate_vulnerability_analysis(security_events_data, servers_data, applications_data):
    """Calculate vulnerability analysis metrics"""
    if security_events_data.empty:
        return pd.DataFrame(), "No security event data available"
    
    total_systems = len(servers_data) + len(applications_data)
    total_vulnerabilities = len(security_events_data)
    server_vulnerabilities = len(security_events_data) // 2
    application_vulnerabilities = total_vulnerabilities - server_vulnerabilities
    vulnerability_rate = (total_vulnerabilities / max(total_systems, 1)) * 100
    
    vuln_data = [{
        'total_systems': total_systems,
        'total_vulnerabilities': total_vulnerabilities,
        'server_vulnerabilities': server_vulnerabilities,
        'application_vulnerabilities': application_vulnerabilities,
        'vulnerability_rate_percent': vulnerability_rate
    }]
    
    vuln_df = pd.DataFrame(vuln_data)
    message = f"Vulnerability analysis for {total_systems} systems"
    return vuln_df, message

def calculate_firewall_performance(security_events_data):
    """Calculate firewall performance metrics"""
    if security_events_data.empty:
        return pd.DataFrame(), "No security event data available"
    
    total_attempts = len(security_events_data)
    blocked_attempts = int(total_attempts * 0.8)  # Assume 80% blocked
    successful_attempts = total_attempts - blocked_attempts
    effectiveness = (blocked_attempts / max(total_attempts, 1)) * 100
    
    firewall_data = [{
        'total_access_attempts': total_attempts,
        'blocked_attempts': blocked_attempts,
        'successful_attempts': successful_attempts,
        'firewall_effectiveness_percent': effectiveness
    }]
    
    firewall_df = pd.DataFrame(firewall_data)
    message = f"Firewall analysis for {total_attempts} access attempts"
    return firewall_df, message

def calculate_data_breach_analysis(security_events_data, incidents_data):
    """Calculate data breach analysis metrics"""
    if security_events_data.empty:
        return pd.DataFrame(), "No security event data available"
    
    total_data_stored = 1000  # Mock value in GB
    data_breaches = len(security_events_data) // 10  # Assume 10% are breaches
    security_events = len(security_events_data)
    incident_breaches = len(incidents_data) // 20  # Assume 5% of incidents are breaches
    breach_rate = (data_breaches / max(total_data_stored, 1)) * 100
    
    breach_data = [{
        'total_data_stored': total_data_stored,
        'data_breaches_detected': data_breaches,
        'security_events': security_events,
        'incident_breaches': incident_breaches,
        'data_breach_rate_percent': breach_rate
    }]
    
    breach_df = pd.DataFrame(breach_data)
    message = f"Data breach analysis for {total_data_stored} GB of data"
    return breach_df, message

def calculate_access_control_analysis(users_data, security_events_data):
    """Calculate access control analysis metrics"""
    if users_data.empty:
        return pd.DataFrame(), "No user data available"
    
    total_users = len(users_data)
    active_users = len(users_data[users_data.get('status', '') == 'Active'])
    inactive_users = total_users - active_users
    unauthorized_attempts = len(security_events_data) // 5  # Mock calculation
    compliance_rate = (active_users / max(total_users, 1)) * 100
    
    access_data = [{
        'total_users': total_users,
        'active_users': active_users,
        'inactive_users': inactive_users,
        'unauthorized_access_attempts': unauthorized_attempts,
        'access_control_compliance_percent': compliance_rate
    }]
    
    access_df = pd.DataFrame(access_data)
    message = f"Access control analysis for {total_users} users"
    return access_df, message

def calculate_phishing_metrics(security_events_data, incidents_data):
    """Calculate phishing attack metrics"""
    if security_events_data.empty:
        return pd.DataFrame(), "No security event data available"
    
    total_interactions = len(security_events_data) * 10  # Mock calculation
    phishing_attempts = len(security_events_data) // 3
    successful_attacks = phishing_attempts // 10  # Assume 10% success rate
    attempt_rate = (phishing_attempts / max(total_interactions, 1)) * 100
    success_rate = (successful_attacks / max(phishing_attempts, 1)) * 100
    
    phishing_data = [{
        'total_user_interactions': total_interactions,
        'phishing_attempts': phishing_attempts,
        'successful_phishing_attacks': successful_attacks,
        'phishing_attempt_rate_percent': attempt_rate,
        'phishing_success_rate_percent': success_rate
    }]
    
    phishing_df = pd.DataFrame(phishing_data)
    message = f"Phishing analysis for {total_interactions} user interactions"
    return phishing_df, message

def calculate_encryption_effectiveness(security_events_data, assets_data):
    """Calculate encryption effectiveness metrics"""
    if assets_data.empty:
        return pd.DataFrame(), "No asset data available"
    
    total_assets = len(assets_data)
    encrypted_assets = int(total_assets * 0.8)  # Assume 80% encrypted
    unencrypted_assets = total_assets - encrypted_assets
    sensitive_assets = int(total_assets * 0.6)  # Assume 60% are sensitive
    effectiveness = (encrypted_assets / max(total_assets, 1)) * 100
    
    encryption_data = [{
        'total_sensitive_assets': sensitive_assets,
        'encrypted_assets': encrypted_assets,
        'unencrypted_assets': unencrypted_assets,
        'encryption_effectiveness_percent': effectiveness
    }]
    
    encryption_df = pd.DataFrame(encryption_data)
    message = f"Encryption analysis for {total_assets} assets"
    return encryption_df, message

def calculate_ticket_resolution_rate(tickets_data):
    """Calculate ticket resolution rate metrics"""
    if tickets_data.empty:
        return pd.DataFrame(), "No ticket data available"
    
    total_tickets = len(tickets_data)
    resolved_tickets = len(tickets_data[tickets_data.get('status', '') == 'Resolved'])
    pending_tickets = total_tickets - resolved_tickets
    sla_compliant = int(resolved_tickets * 0.85)  # Assume 85% SLA compliance
    resolution_rate = (resolved_tickets / max(total_tickets, 1)) * 100
    
    resolution_data = [{
        'total_tickets': total_tickets,
        'resolved_tickets': resolved_tickets,
        'pending_tickets': pending_tickets,
        'sla_compliant_tickets': sla_compliant,
        'resolution_rate_percent': resolution_rate
    }]
    
    resolution_df = pd.DataFrame(resolution_data)
    message = f"Resolution analysis for {total_tickets} tickets"
    return resolution_df, message

def calculate_first_call_resolution(tickets_data):
    """Calculate first call resolution metrics"""
    if tickets_data.empty:
        return pd.DataFrame(), "No ticket data available"
    
    total_issues = len(tickets_data)
    fcr_issues = int(total_issues * 0.7)  # Assume 70% FCR
    escalated_issues = total_issues - fcr_issues
    fcr_rate = (fcr_issues / max(total_issues, 1)) * 100
    
    fcr_data = [{
        'total_issues_reported': total_issues,
        'issues_resolved_first_call': fcr_issues,
        'escalated_issues': escalated_issues,
        'fcr_rate_percent': fcr_rate
    }]
    
    fcr_df = pd.DataFrame(fcr_data)
    message = f"FCR analysis for {total_issues} issues"
    return fcr_df, message

def calculate_average_resolution_time(tickets_data):
    """Calculate average resolution time metrics"""
    if tickets_data.empty:
        return pd.DataFrame(), "No ticket data available"
    
    # Create mock resolution time data by priority
    resolution_data = pd.DataFrame({
        'priority': ['Low', 'Medium', 'High', 'Critical'],
        'mean': [120, 240, 360, 480],
        'count': [25, 30, 20, 15]
    })
    
    message = f"Resolution time analysis for {resolution_data['count'].sum()} tickets"
    return resolution_data, message

def calculate_ticket_volume_analysis(tickets_data):
    """Calculate ticket volume analysis metrics"""
    if tickets_data.empty:
        return pd.DataFrame(), "No ticket data available"
    
    total_tickets = len(tickets_data)
    daily_volume_rate = total_tickets / 30  # Assume 30 days
    peak_day_tickets = int(daily_volume_rate * 1.5)  # Assume 50% peak
    
    volume_data = [{
        'total_tickets': total_tickets,
        'daily_volume_rate': daily_volume_rate,
        'peak_day_tickets': peak_day_tickets
    }]
    
    volume_df = pd.DataFrame(volume_data)
    message = f"Volume analysis for {total_tickets} tickets"
    return volume_df, message

def calculate_user_satisfaction_metrics(tickets_data):
    """Calculate user satisfaction metrics"""
    if tickets_data.empty:
        return pd.DataFrame(), "No ticket data available"
    
    total_users = len(tickets_data)
    satisfied_users = int(total_users * 0.8)  # Assume 80% satisfaction
    dissatisfied_users = total_users - satisfied_users
    avg_satisfaction = 4.2  # Mock average score out of 5
    satisfaction_rate = (satisfied_users / max(total_users, 1)) * 100
    
    satisfaction_data = [{
        'total_users_surveyed': total_users,
        'satisfied_users': satisfied_users,
        'dissatisfied_users': dissatisfied_users,
        'avg_satisfaction_score': avg_satisfaction,
        'satisfaction_rate_percent': satisfaction_rate
    }]
    
    satisfaction_df = pd.DataFrame(satisfaction_data)
    message = f"Satisfaction analysis for {total_users} users"
    return satisfaction_df, message

def calculate_recurring_issue_analysis(tickets_data):
    """Calculate recurring issue analysis"""
    if tickets_data.empty:
        return pd.DataFrame(), "No ticket data available"
    
    # Find recurring issues (titles that appear more than once)
    issue_counts = tickets_data.groupby('title').size().reset_index(name='occurrence_count')
    recurring_issues = issue_counts[issue_counts['occurrence_count'] > 1]
    
    message = f"Recurring issue analysis found {len(recurring_issues)} recurring issue types"
    return recurring_issues, message

def calculate_asset_utilization(assets_data):
    """Calculate asset utilization metrics"""
    if assets_data.empty:
        return pd.DataFrame(), "No asset data available"
    
    total_assets = len(assets_data)
    utilized_assets = len(assets_data[assets_data.get('status', '') == 'Active'])
    inactive_assets = len(assets_data[assets_data.get('status', '') == 'Inactive'])
    maintenance_assets = total_assets - utilized_assets - inactive_assets
    utilization_rate = (utilized_assets / max(total_assets, 1)) * 100
    
    utilization_data = [{
        'total_it_assets': total_assets,
        'utilized_assets': utilized_assets,
        'inactive_assets': inactive_assets,
        'maintenance_assets': maintenance_assets,
        'asset_utilization_percent': utilization_rate
    }]
    
    utilization_df = pd.DataFrame(utilization_data)
    message = f"Utilization analysis for {total_assets} assets"
    return utilization_df, message

def calculate_hardware_lifecycle_analysis(assets_data):
    """Calculate hardware lifecycle analysis"""
    if assets_data.empty:
        return pd.DataFrame(), "No asset data available"
    
    lifecycle_data = []
    for _, asset in assets_data.iterrows():
        asset_name = asset.get('asset_name', 'Unknown')
        asset_type = asset.get('asset_type', 'Unknown')
        purchase_date = asset.get('purchase_date', pd.Timestamp.now())
        original_value = asset.get('purchase_cost', 1000)
        
        # Calculate age and depreciation
        age_years = (pd.Timestamp.now() - purchase_date).days / 365.25
        depreciation_rate = min(100, age_years * 20)  # 20% per year
        
        lifecycle_data.append({
            'asset_name': asset_name,
            'asset_type': asset_type,
            'age_years': age_years,
            'depreciation_rate_percent': depreciation_rate,
            'original_value': original_value
        })
    
    lifecycle_df = pd.DataFrame(lifecycle_data)
    message = f"Lifecycle analysis for {len(lifecycle_df)} assets"
    return lifecycle_df, message

def calculate_software_licensing_compliance(assets_data, applications_data):
    """Calculate software licensing compliance"""
    if assets_data.empty or applications_data.empty:
        return pd.DataFrame(), "No asset or application data available"
    
    total_licenses = len(applications_data)
    compliant_licenses = int(total_licenses * 0.9)  # Assume 90% compliance
    non_compliant_licenses = total_licenses - compliant_licenses
    compliance_rate = (compliant_licenses / max(total_licenses, 1)) * 100
    
    compliance_data = [{
        'total_software_licenses': total_licenses,
        'compliant_licenses': compliant_licenses,
        'non_compliant_licenses': non_compliant_licenses,
        'licensing_compliance_rate_percent': compliance_rate
    }]
    
    compliance_df = pd.DataFrame(compliance_data)
    message = f"Licensing compliance analysis for {total_licenses} licenses"
    return compliance_df, message

def calculate_cloud_resource_utilization(assets_data, servers_data):
    """Calculate cloud resource utilization"""
    if assets_data.empty or servers_data.empty:
        return pd.DataFrame(), "No asset or server data available"
    
    cloud_utilization = 75.0  # Mock percentage
    storage_utilization = 65.0  # Mock percentage
    total_storage = servers_data.get('storage_tb', pd.Series([1])).sum()
    
    cloud_data = [{
        'cloud_utilization_percent': cloud_utilization,
        'storage_utilization_percent': storage_utilization,
        'total_storage_tb': total_storage
    }]
    
    cloud_df = pd.DataFrame(cloud_data)
    message = f"Cloud utilization analysis for {total_storage:.1f} TB storage"
    return cloud_df, message

def calculate_inventory_turnover(assets_data):
    """Calculate inventory turnover metrics"""
    if assets_data.empty:
        return pd.DataFrame(), "No asset data available"
    
    total_purchase_value = assets_data.get('purchase_cost', pd.Series([1000])).sum()
    average_inventory_value = total_purchase_value * 0.8  # Assume 80% of purchase value
    turnover_rate = 1.2  # Mock turnover rate
    
    turnover_data = [{
        'total_purchase_value': total_purchase_value,
        'average_inventory_value': average_inventory_value,
        'inventory_turnover_rate': turnover_rate
    }]
    
    turnover_df = pd.DataFrame(turnover_data)
    message = f"Inventory turnover analysis for ${total_purchase_value:,.0f} in assets"
    return turnover_df, message

def calculate_data_quality_analysis(applications_data, backups_data):
    """Calculate data quality analysis metrics"""
    if applications_data.empty or backups_data.empty:
        return pd.DataFrame(), "No application or backup data available"
    
    total_data_points = len(applications_data) + len(backups_data)
    valid_data_points = int(total_data_points * 0.95)  # Assume 95% valid
    quality_rate = (valid_data_points / max(total_data_points, 1)) * 100
    
    quality_data = [{
        'total_data_points': total_data_points,
        'valid_data_points': valid_data_points,
        'data_quality_rate_percent': quality_rate
    }]
    
    quality_df = pd.DataFrame(quality_data)
    message = f"Data quality analysis for {total_data_points} data points"
    return quality_df, message

def calculate_database_performance_metrics(servers_data, applications_data):
    """Calculate database performance metrics"""
    if servers_data.empty or applications_data.empty:
        return pd.DataFrame(), "No server or application data available"
    
    avg_query_time = 150.0  # Mock milliseconds
    index_efficiency = 85.0  # Mock percentage
    total_databases = len(applications_data[applications_data.get('app_type', '') == 'Database'])
    
    perf_data = [{
        'avg_query_execution_time_ms': avg_query_time,
        'index_efficiency_percent': index_efficiency,
        'total_databases': total_databases
    }]
    
    perf_df = pd.DataFrame(perf_data)
    message = f"Database performance analysis for {total_databases} databases"
    return perf_df, message

def calculate_backup_success_rate(backups_data):
    """Calculate backup success rate metrics"""
    if backups_data.empty:
        return pd.DataFrame(), "No backup data available"
    
    total_backups = len(backups_data)
    successful_backups = len(backups_data[backups_data.get('status', '') == 'Success'])
    failed_backups = total_backups - successful_backups
    success_rate = (successful_backups / max(total_backups, 1)) * 100
    
    backup_data = [{
        'total_backups_attempted': total_backups,
        'successful_backups': successful_backups,
        'failed_backups': failed_backups,
        'backup_success_rate_percent': success_rate
    }]
    
    backup_df = pd.DataFrame(backup_data)
    message = f"Backup analysis for {total_backups} backup attempts"
    return backup_df, message

def calculate_data_loss_metrics(incidents_data, backups_data):
    """Calculate data loss metrics"""
    if incidents_data.empty or backups_data.empty:
        return pd.DataFrame(), "No incident or backup data available"
    
    total_data_stored = 1000  # Mock GB
    data_loss_incidents = len(incidents_data) // 10  # Assume 10% are data loss
    incident_data_loss = len(incidents_data) // 20  # Assume 5% cause data loss
    backup_failures = len(backups_data) // 20  # Assume 5% backup failures
    loss_rate = (data_loss_incidents / max(total_data_stored, 1)) * 100
    
    loss_data = [{
        'total_data_stored': total_data_stored,
        'data_loss_incidents': data_loss_incidents,
        'incident_data_loss': incident_data_loss,
        'backup_failures': backup_failures,
        'data_loss_rate_percent': loss_rate
    }]
    
    loss_df = pd.DataFrame(loss_data)
    message = f"Data loss analysis for {total_data_stored} GB of data"
    return loss_df, message

def calculate_storage_usage_trends(servers_data, backups_data):
    """Calculate storage usage trends"""
    if servers_data.empty or backups_data.empty:
        return pd.DataFrame(), "No server or backup data available"
    
    current_storage = servers_data.get('storage_tb', pd.Series([1])).sum()
    backup_storage = backups_data.get('size_gb', pd.Series([10])).sum() / 1024  # Convert to TB
    forecasted_need = current_storage * 1.2  # Assume 20% growth
    usage_trend = (current_storage / max(forecasted_need, 1)) * 100
    
    storage_data = [{
        'current_storage_used_tb': current_storage,
        'backup_storage_tb': backup_storage,
        'forecasted_storage_requirement_tb': forecasted_need,
        'storage_usage_trend_percent': usage_trend
    }]
    
    storage_df = pd.DataFrame(storage_data)
    message = f"Storage trend analysis for {current_storage:.1f} TB current usage"
    return storage_df, message

def calculate_project_delivery_metrics(projects_data):
    """Calculate project delivery metrics"""
    if projects_data.empty:
        return pd.DataFrame(), "No project data available"
    
    total_projects = len(projects_data)
    completed_projects = len(projects_data[projects_data.get('status', '') == 'Completed'])
    on_time_delivery = int(completed_projects * 0.8)  # Assume 80% on-time
    budget_adherence = int(completed_projects * 0.85)  # Assume 85% budget adherence
    
    delivery_data = [{
        'total_projects': total_projects,
        'completed_projects': completed_projects,
        'on_time_delivery_percent': (on_time_delivery / max(completed_projects, 1)) * 100,
        'budget_adherence_percent': (budget_adherence / max(completed_projects, 1)) * 100
    }]
    
    delivery_df = pd.DataFrame(delivery_data)
    message = f"Project delivery analysis for {total_projects} projects"
    return delivery_df, message

def calculate_development_cycle_time(projects_data):
    """Calculate development cycle time metrics"""
    if projects_data.empty:
        return pd.DataFrame(), "No project data available"
    
    avg_cycle_time = 45.0  # Mock days
    fastest_cycle = 15.0  # Mock days
    slowest_cycle = 90.0  # Mock days
    
    cycle_data = [{
        'avg_development_cycle_time_days': avg_cycle_time,
        'fastest_cycle_days': fastest_cycle,
        'slowest_cycle_days': slowest_cycle
    }]
    
    cycle_df = pd.DataFrame(cycle_data)
    message = f"Development cycle analysis for {len(projects_data)} projects"
    return cycle_df, message

def calculate_code_quality_analysis(projects_data):
    """Calculate code quality analysis metrics"""
    if projects_data.empty:
        return pd.DataFrame(), "No project data available"
    
    total_bugs = 150  # Mock value
    total_lines_of_code = 50000  # Mock value
    bugs_per_loc = total_bugs / max(total_lines_of_code, 1)
    coding_standard_compliance = 92.0  # Mock percentage
    
    quality_data = [{
        'total_bugs': total_bugs,
        'total_lines_of_code': total_lines_of_code,
        'bugs_per_loc': bugs_per_loc,
        'coding_standard_compliance_percent': coding_standard_compliance
    }]
    
    quality_df = pd.DataFrame(quality_data)
    message = f"Code quality analysis for {total_lines_of_code:,} lines of code"
    return quality_df, message

def calculate_release_management_metrics(projects_data):
    """Calculate release management metrics"""
    if projects_data.empty:
        return pd.DataFrame(), "No project data available"
    
    total_deployments = 25  # Mock value
    deployment_success_rate = 96.0  # Mock percentage
    rollback_rate = 4.0  # Mock percentage
    
    release_data = [{
        'total_deployments': total_deployments,
        'deployment_success_rate_percent': deployment_success_rate,
        'rollback_rate_percent': rollback_rate
    }]
    
    release_df = pd.DataFrame(release_data)
    message = f"Release management analysis for {total_deployments} deployments"
    return release_df, message

def calculate_agile_metrics(projects_data):
    """Calculate agile metrics"""
    if projects_data.empty:
        return pd.DataFrame(), "No project data available"
    
    sprint_velocity = 15.5  # Mock value
    burndown_rate = 85.0  # Mock percentage
    total_story_points = 200  # Mock value
    
    agile_data = [{
        'sprint_velocity': sprint_velocity,
        'burndown_rate_percent': burndown_rate,
        'total_story_points': total_story_points
    }]
    
    agile_df = pd.DataFrame(agile_data)
    message = f"Agile metrics analysis for {len(projects_data)} projects"
    return agile_df, message

def calculate_application_usability_metrics(applications_data, tickets_data):
    """Calculate application usability metrics"""
    if applications_data.empty or tickets_data.empty:
        return pd.DataFrame(), "No application or ticket data available"
    
    total_users = len(tickets_data)
    satisfied_users = int(total_users * 0.8)  # Assume 80% satisfaction
    usability_score = 85.0  # Mock percentage
    avg_satisfaction = 4.2  # Mock score out of 5
    
    usability_data = [{
        'total_users_surveyed': total_users,
        'satisfied_users': satisfied_users,
        'usability_score_percent': usability_score,
        'avg_satisfaction_score': avg_satisfaction
    }]
    
    usability_df = pd.DataFrame(usability_data)
    message = f"Usability analysis for {total_users} users"
    return usability_df, message

def calculate_website_performance_analysis(applications_data, incidents_data):
    """Calculate website performance analysis"""
    if applications_data.empty or incidents_data.empty:
        return pd.DataFrame(), "No application or incident data available"
    
    avg_page_load_time = 2.5  # Mock seconds
    mobile_traffic = 65.0  # Mock percentage
    desktop_traffic = 35.0  # Mock percentage
    web_incidents = len(incidents_data) // 5  # Assume 20% are web incidents
    
    web_data = [{
        'avg_page_load_time_seconds': avg_page_load_time,
        'mobile_traffic_percentage': mobile_traffic,
        'desktop_traffic_percentage': desktop_traffic,
        'web_incidents': web_incidents
    }]
    
    web_df = pd.DataFrame(web_data)
    message = f"Website performance analysis for {web_incidents} web incidents"
    return web_df, message

def calculate_accessibility_compliance_analysis(applications_data):
    """Calculate accessibility compliance analysis"""
    if applications_data.empty:
        return pd.DataFrame(), "No application data available"
    
    total_features = len(applications_data) * 10  # Mock features per app
    compliant_features = int(total_features * 0.85)  # Assume 85% compliance
    non_compliant_features = total_features - compliant_features
    compliance_rate = (compliant_features / max(total_features, 1)) * 100
    
    accessibility_data = [{
        'total_features': total_features,
        'compliant_features': compliant_features,
        'non_compliant_features': non_compliant_features,
        'accessibility_compliance_rate_percent': compliance_rate
    }]
    
    accessibility_df = pd.DataFrame(accessibility_data)
    message = f"Accessibility analysis for {total_features} features"
    return accessibility_df, message

def calculate_user_feedback_analysis(tickets_data):
    """Calculate user feedback analysis"""
    if tickets_data.empty:
        return pd.DataFrame(), "No ticket data available"
    
    total_feedback = len(tickets_data)
    positive_feedback = int(total_feedback * 0.8)  # Assume 80% positive
    avg_feedback_score = 4.2  # Mock score out of 5
    feedback_score = (positive_feedback / max(total_feedback, 1)) * 100
    
    feedback_data = [{
        'total_feedback': total_feedback,
        'positive_feedback': positive_feedback,
        'avg_feedback_score': avg_feedback_score,
        'user_feedback_score_percent': feedback_score
    }]
    
    feedback_df = pd.DataFrame(feedback_data)
    message = f"Feedback analysis for {total_feedback} feedback items"
    return feedback_df, message

def calculate_it_budget_utilization(projects_data, assets_data):
    """Calculate IT budget utilization metrics"""
    if projects_data.empty and assets_data.empty:
        return pd.DataFrame(), "No project or asset data available"
    
    project_costs = projects_data.get('budget', pd.Series([0])).sum() if not projects_data.empty else 0
    asset_costs = assets_data.get('purchase_cost', pd.Series([0])).sum() if not assets_data.empty else 0
    actual_spending = project_costs + asset_costs
    budgeted_amount = actual_spending * 1.1  # Assume 10% over budget
    utilization_rate = (actual_spending / max(budgeted_amount, 1)) * 100
    
    budget_data = [{
        'budgeted_amount': budgeted_amount,
        'actual_spending': actual_spending,
        'project_costs': project_costs,
        'asset_costs': asset_costs,
        'budget_utilization_percent': utilization_rate
    }]
    
    budget_df = pd.DataFrame(budget_data)
    message = f"Budget analysis for ${actual_spending:,.0f} in spending"
    return budget_df, message

def calculate_cost_per_user_device(assets_data, users_data):
    """Calculate cost per user/device metrics"""
    if assets_data.empty and users_data.empty:
        return pd.DataFrame(), "No asset or user data available"
    
    total_asset_cost = assets_data.get('purchase_cost', pd.Series([0])).sum() if not assets_data.empty else 0
    total_users = len(users_data) if not users_data.empty else 1
    cost_per_user = total_asset_cost / max(total_users, 1)
    cost_per_device = total_asset_cost / max(len(assets_data), 1) if not assets_data.empty else 0
    
    cost_data = [{
        'total_asset_cost': total_asset_cost,
        'total_users': total_users,
        'cost_per_user': cost_per_user,
        'cost_per_device': cost_per_device
    }]
    
    cost_df = pd.DataFrame(cost_data)
    message = f"Cost analysis for {total_users} users and ${total_asset_cost:,.0f} in assets"
    return cost_df, message

def show_sample_data_sets():
    """Display sample data sets for testing the program"""
    st.title(" Sample Data Sets for Testing")
    
    st.markdown("""
    <div class="welcome-section">
        <h2> Ready-to-Use Sample Data</h2>
        <p>Download these pre-populated Excel files to test all features of the IT Analytics Dashboard without manual data entry.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Simple buttons to load sample data directly into the system
    st.markdown("###  Load Sample Data Sets")
    st.markdown("Click any button below to automatically load sample data into the system:")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button(" Load Basic Sample Data", key="load_basic_btn", use_container_width=True):
            with st.spinner("Loading basic sample data..."):
                basic_sample_data = create_basic_sample_data()
                
                # Load data directly into session state
                st.session_state.servers_data = basic_sample_data.get('Servers', pd.DataFrame())
                st.session_state.network_devices_data = basic_sample_data.get('Network_Devices', pd.DataFrame())
                st.session_state.applications_data = basic_sample_data.get('Applications', pd.DataFrame())
                st.session_state.incidents_data = basic_sample_data.get('Incidents', pd.DataFrame())
                st.session_state.tickets_data = basic_sample_data.get('Tickets', pd.DataFrame())
                st.session_state.assets_data = basic_sample_data.get('Assets', pd.DataFrame())
                st.session_state.security_events_data = basic_sample_data.get('Security_Events', pd.DataFrame())
                st.session_state.backups_data = basic_sample_data.get('Backups', pd.DataFrame())
                st.session_state.projects_data = basic_sample_data.get('Projects', pd.DataFrame())
                st.session_state.users_data = basic_sample_data.get('Users', pd.DataFrame())
                
                st.success(" Basic sample data loaded successfully!")
                st.info(f" Loaded: {len(st.session_state.servers_data)} servers, {len(st.session_state.applications_data)} applications, {len(st.session_state.tickets_data)} tickets, and more...")
    
    with col2:
        if st.button(" Load Security Sample Data", key="load_security_btn", use_container_width=True):
            with st.spinner("Loading security sample data..."):
                security_sample_data = create_security_sample_data()
                
                # Load security-focused data
                st.session_state.security_events_data = security_sample_data.get('Security_Events', pd.DataFrame())
                st.session_state.incidents_data = security_sample_data.get('Incidents', pd.DataFrame())
                st.session_state.backups_data = security_sample_data.get('Backups', pd.DataFrame())
                st.session_state.users_data = security_sample_data.get('Users', pd.DataFrame())
                
                st.success(" Security sample data loaded successfully!")
                st.info(f" Loaded: {len(st.session_state.security_events_data)} security events, {len(st.session_state.incidents_data)} incidents, and more...")
    
    with col3:
        if st.button(" Load Comprehensive Sample Data", key="load_comprehensive_btn", use_container_width=True):
            with st.spinner("Loading comprehensive sample data..."):
                comprehensive_sample_data = create_comprehensive_sample_data()
                
                # Load all comprehensive data
                st.session_state.servers_data = comprehensive_sample_data.get('Servers', pd.DataFrame())
                st.session_state.network_devices_data = comprehensive_sample_data.get('Network_Devices', pd.DataFrame())
                st.session_state.applications_data = comprehensive_sample_data.get('Applications', pd.DataFrame())
                st.session_state.incidents_data = comprehensive_sample_data.get('Incidents', pd.DataFrame())
                st.session_state.tickets_data = comprehensive_sample_data.get('Tickets', pd.DataFrame())
                st.session_state.assets_data = comprehensive_sample_data.get('Assets', pd.DataFrame())
                st.session_state.security_events_data = comprehensive_sample_data.get('Security_Events', pd.DataFrame())
                st.session_state.backups_data = comprehensive_sample_data.get('Backups', pd.DataFrame())
                st.session_state.projects_data = comprehensive_sample_data.get('Projects', pd.DataFrame())
                st.session_state.users_data = comprehensive_sample_data.get('Users', pd.DataFrame())
                
                st.success(" Comprehensive sample data loaded successfully!")
                st.info(f" Loaded: {len(st.session_state.servers_data)} servers, {len(st.session_state.applications_data)} applications, {len(st.session_state.tickets_data)} tickets, and more...")
    
    st.markdown("---")
    
    # Sample data preview section
    st.subheader(" Sample Data Preview")
    
    # Show preview button for basic sample data
    st.markdown("###  Sample Servers Data Preview")
    if st.button(" Preview Basic Sample Data", key="preview_basic_btn"):
        with st.spinner("Loading preview..."):
            basic_sample_data = create_basic_sample_data()
            servers_preview = basic_sample_data.get('Servers', pd.DataFrame())
            if not servers_preview.empty:
                st.dataframe(servers_preview.head(), use_container_width=True)
                st.caption(" This is just a preview. Click the generate buttons above to download full samples.")
            else:
                st.warning("No sample data available.")
    
    st.markdown("---")
    
    # Instructions for using sample data
    st.subheader(" How to Use Sample Data")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        <div class="info-card">
            <h4> Step 1: Load Sample Data</h4>
            <ul>
                <li>Choose the sample dataset that fits your testing needs</li>
                <li>Click the load button to automatically populate the system</li>
                <li>Data is immediately available for all dashboard features</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="info-card">
            <h4> Step 2: Start Testing</h4>
            <ul>
                <li>Navigate to any dashboard section</li>
                <li>All analytics and visualizations will work immediately</li>
                <li>No need to upload files - data is already loaded</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Data structure information
    st.subheader(" Sample Data Structure")
    
    st.markdown("""
    <div class="info-card">
        <h4> What's Included in Each Sample:</h4>
        <ul>
            <li><strong>Basic Sample:</strong> 10 data tables with essential IT metrics including servers, network devices, applications, incidents, tickets, assets, security events, backups, projects, and users</li>
            <li><strong>Security Sample:</strong> 4 focused tables for security and risk analysis including security events, incidents, backups, and users</li>
            <li><strong>Comprehensive Sample:</strong> 12 enhanced tables with enterprise-level data including 50+ servers, 40+ network devices, 60+ applications, 100+ incidents, 200+ tickets, 100+ assets, 100+ security events, 50+ backups, 30+ projects, 100+ users, cost data, and performance metrics</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)


def show_infrastructure_systems():
    """Display infrastructure and systems performance analysis"""
    st.title(" Infrastructure & Systems Performance")
    
    if st.session_state.servers_data.empty:
        st.warning(" No server data available. Please upload data first.")
        return
    
    # Advanced Infrastructure Analytics Dashboard
    st.subheader(" Advanced Infrastructure Analytics Dashboard")
    
    # Create summary metrics row
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if not st.session_state.servers_data.empty:
            total_servers = len(st.session_state.servers_data)
            st.metric("Total Servers", total_servers, delta="Active")
        else:
            st.metric("Total Servers", 0, delta="No Data")
    
    with col2:
        if not st.session_state.network_devices_data.empty:
            total_devices = len(st.session_state.network_devices_data)
            st.metric("Network Devices", total_devices, delta="Connected")
        else:
            st.metric("Network Devices", 0, delta="No Data")
    
    with col3:
        if not st.session_state.applications_data.empty:
            total_apps = len(st.session_state.applications_data)
            st.metric("Applications", total_apps, delta="Running")
        else:
            st.metric("Applications", 0, delta="No Data")
    
    with col4:
        if not st.session_state.incidents_data.empty:
            total_incidents = len(st.session_state.incidents_data)
            st.metric("Active Incidents", total_incidents, delta="Monitoring")
        else:
            st.metric("Active Incidents", 0, delta="No Data")
    
    st.markdown("---")
    
    # Infrastructure Health Overview
    st.subheader(" Infrastructure Health Overview")
    
    if not st.session_state.servers_data.empty and not st.session_state.applications_data.empty:
        # Create infrastructure health matrix
        health_data = {
            'Metric': ['Server Uptime', 'Network Latency', 'System Load', 'Application Health', 'Overall Score'],
            'Current': [0, 0, 0, 0, 0],
            'Target': [95, 50, 80, 85, 85],
            'Status': ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown']
        }
        
        # Calculate actual values if data exists
        try:
            uptime_df, _ = calculate_server_uptime(st.session_state.servers_data, st.session_state.incidents_data)
            if not uptime_df.empty:
                health_data['Current'][0] = uptime_df['uptime_percentage'].mean()
                health_data['Status'][0] = '' if health_data['Current'][0] >= 95 else '' if health_data['Current'][0] >= 90 else ''
        except:
            pass
        
        try:
            if not st.session_state.network_devices_data.empty:
                latency_df, _ = calculate_network_latency(st.session_state.network_devices_data, st.session_state.incidents_data)
                if not latency_df.empty:
                    health_data['Current'][1] = latency_df['avg_latency_ms'].mean()
                    health_data['Status'][1] = '' if health_data['Current'][1] <= 50 else '' if health_data['Current'][1] <= 100 else ''
        except:
            pass
        
        try:
            if not st.session_state.applications_data.empty:
                load_df, _ = calculate_system_load(st.session_state.servers_data, st.session_state.applications_data)
                if not load_df.empty:
                    health_data['Current'][2] = load_df['load_percentage'].mean()
                    health_data['Status'][2] = '' if health_data['Current'][2] <= 80 else '' if health_data['Current'][2] <= 90 else ''
        except:
            pass
        
        try:
            if not st.session_state.applications_data.empty:
                perf_df, _ = calculate_application_performance(st.session_state.applications_data, st.session_state.incidents_data)
                if not perf_df.empty:
                    health_data['Current'][3] = perf_df['health_score'].mean()
                    health_data['Status'][3] = '' if health_data['Current'][3] >= 85 else '' if health_data['Current'][3] >= 70 else ''
        except:
            pass
        
        # Calculate overall score
        if any(health_data['Current']):
            valid_metrics = [i for i, val in enumerate(health_data['Current']) if val > 0]
            if valid_metrics:
                overall_score = sum(health_data['Current'][i] for i in valid_metrics) / len(valid_metrics)
                health_data['Current'][4] = overall_score
                health_data['Status'][4] = '' if overall_score >= 85 else '' if overall_score >= 70 else ''
        
        # Create health overview chart
        health_df = pd.DataFrame(health_data)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Health overview radar chart
            fig = go.Figure()
            
            fig.add_trace(go.Scatterpolar(
                r=health_data['Current'][:4],  # Exclude overall score
                theta=health_data['Metric'][:4],
                fill='toself',
                name='Current Performance',
                line_color='blue',
                hovertemplate="<b>%{theta}</b><br>" +
                             "Current: %{r:.1f}<br>" +
                             "Target: %{customdata}<br>" +
                             "<extra></extra>",
                customdata=health_data['Target'][:4]
            ))
            
            fig.add_trace(go.Scatterpolar(
                r=health_data['Target'][:4],
                theta=health_data['Metric'][:4],
                fill='toself',
                name='Target Performance',
                line_color='red',
                hovertemplate="<b>%{theta}</b><br>" +
                             "Target: %{r:.1f}<br>" +
                             "<extra></extra>"
            ))
            
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(
                        visible=True,
                        range=[0, 100]
                    )),
                showlegend=True,
                title="Infrastructure Health Radar Chart",
                font=dict(family="Arial, sans-serif", size=12),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Health status summary
            st.markdown("** Health Status:**")
            for i, metric in enumerate(health_data['Metric']):
                current = health_data['Current'][i]
                target = health_data['Target'][i]
                status = health_data['Status'][i]
                
                if current > 0:
                    delta = f"{current - target:.1f}"
                    delta_color = "normal" if current >= target else "inverse"
                    st.metric(metric, f"{current:.1f}", delta=delta, delta_color=delta_color)
                else:
                    st.metric(metric, "N/A", delta="No Data")
            
            # Overall health assessment
            if health_data['Current'][4] > 0:
                overall_score = health_data['Current'][4]
                if overall_score >= 85:
                    st.success(" Infrastructure is healthy and performing well")
                elif overall_score >= 70:
                    st.warning(" Infrastructure has some issues but is generally stable")
                else:
                    st.error(" Infrastructure has critical issues requiring immediate attention")
        
        # Health summary table
        st.markdown("** Health Summary:**")
        health_summary_df = pd.DataFrame(health_data)
        st.dataframe(health_summary_df, use_container_width=True)
    
    st.markdown("---")
    
    # Enhanced Server Uptime Analysis with Advanced Analytics
    st.subheader(" Server Uptime Analysis & Performance Metrics")
    uptime_df, uptime_msg = calculate_server_uptime(st.session_state.servers_data, st.session_state.incidents_data)
    
    if not uptime_df.empty:
        # Create enhanced uptime data with additional metrics
        enhanced_uptime_df = uptime_df.copy()
        enhanced_uptime_df['downtime_hours'] = 24 * (100 - enhanced_uptime_df['uptime_percentage']) / 100
        enhanced_uptime_df['performance_score'] = enhanced_uptime_df['uptime_percentage'] * 0.7 + (100 - enhanced_uptime_df['downtime_hours']) * 0.3
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced uptime chart with performance indicators
            fig = go.Figure()
            
            # Add uptime bars
            fig.add_trace(go.Bar(
                x=enhanced_uptime_df['server_name'],
                y=enhanced_uptime_df['uptime_percentage'],
                name='Uptime %',
                marker_color='lightblue',
                hovertemplate="<b>%{x}</b><br>" +
                             "Uptime: %{y:.2f}%<br>" +
                             "Downtime: %{text:.2f} hours<br>" +
                             "Performance Score: %{customdata:.1f}<br>" +
                             "<extra></extra>",
                text=enhanced_uptime_df['downtime_hours'],
                customdata=enhanced_uptime_df['performance_score']
            ))
            
            # Add performance threshold line
            fig.add_hline(y=95, line_dash="dash", line_color="red", 
                         annotation_text="Critical Threshold (95%)",
                         annotation_position="top right")
            
            fig.update_layout(
                title='Server Uptime Performance Analysis',
                xaxis_title="Server Name",
                yaxis_title="Uptime Percentage (%)",
                hovermode='x unified',
                showlegend=True,
                legend=dict(
                    orientation="h",
                    yanchor="bottom",
                    y=1.02,
                    xanchor="right",
                    x=1
                ),
                font=dict(family="Arial, sans-serif", size=12),
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)',
                margin=dict(l=50, r=50, t=80, b=50),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced metrics with color coding
            avg_uptime = enhanced_uptime_df['uptime_percentage'].mean()
            avg_color = "green" if avg_uptime >= 95 else "orange" if avg_uptime >= 90 else "red"
            
            st.metric("Average Uptime", f"{avg_uptime:.2f}%", delta=f"{avg_uptime - 95:.2f}%", delta_color="normal")
            st.metric("Lowest Uptime", f"{enhanced_uptime_df['uptime_percentage'].min():.2f}%", 
                     delta="Critical" if enhanced_uptime_df['uptime_percentage'].min() < 90 else "Good")
            st.metric("Performance Score", f"{enhanced_uptime_df['performance_score'].mean():.1f}/100")
            
            # Add performance insights
            st.markdown("** Performance Insights:**")
            if avg_uptime >= 95:
                st.success(" Excellent uptime performance across all servers")
            elif avg_uptime >= 90:
                st.warning(" Good uptime performance with room for improvement")
            else:
                st.error(" Critical uptime issues detected - immediate attention required")
        
        # Enhanced data display with performance metrics
        st.info(f" {uptime_msg}")
        
        # Create performance summary
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Servers Above 95%", len(enhanced_uptime_df[enhanced_uptime_df['uptime_percentage'] >= 95]))
        with col2:
            st.metric("Servers 90-95%", len(enhanced_uptime_df[(enhanced_uptime_df['uptime_percentage'] >= 90) & (enhanced_uptime_df['uptime_percentage'] < 95)]))
        with col3:
            st.metric("Servers Below 90%", len(enhanced_uptime_df[enhanced_uptime_df['uptime_percentage'] < 90]))
        
        display_dataframe_with_index_1(enhanced_uptime_df)
    
    st.markdown("---")
    
    # Enhanced Network Latency Analysis with Performance Monitoring
    st.subheader(" Network Latency Analysis & Performance Monitoring")
    if not st.session_state.network_devices_data.empty:
        latency_df, latency_msg = calculate_network_latency(st.session_state.network_devices_data, st.session_state.incidents_data)
        
        if not latency_df.empty:
            # Enhance latency data with performance indicators
            enhanced_latency_df = latency_df.copy()
            enhanced_latency_df['latency_category'] = pd.cut(
                enhanced_latency_df['avg_latency_ms'], 
                bins=[0, 50, 100, 200, float('inf')], 
                labels=['Excellent', 'Good', 'Fair', 'Poor']
            )
            enhanced_latency_df['performance_score'] = 100 - (enhanced_latency_df['avg_latency_ms'] / 2)
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced network performance chart
                fig = go.Figure()
                
                # Color mapping for latency categories
                color_map = {'Excellent': 'green', 'Good': 'lightgreen', 'Fair': 'orange', 'Poor': 'red'}
                
                for category in enhanced_latency_df['latency_category'].unique():
                    if pd.notna(category):
                        category_data = enhanced_latency_df[enhanced_latency_df['latency_category'] == category]
                        fig.add_trace(go.Scatter(
                            x=category_data['device_name'],
                            y=category_data['avg_latency_ms'],
                            mode='markers',
                            name=f'{category} Latency',
                            marker=dict(
                                size=category_data['avg_latency_ms'] / 10 + 10,
                                color=color_map[category],
                                symbol='circle',
                                line=dict(width=2, color='white')
                            ),
                            hovertemplate="<b>%{x}</b><br>" +
                                         "Latency: %{y:.1f} ms<br>" +
                                         "Category: %{text}<br>" +
                                         "Performance Score: %{customdata:.1f}<br>" +
                                         "<extra></extra>",
                            text=category_data['latency_category'],
                            customdata=category_data['performance_score']
                        ))
                
                # Add latency thresholds
                fig.add_hline(y=50, line_dash="dash", line_color="green", 
                             annotation_text="Excellent (<50ms)", annotation_position="top right")
                fig.add_hline(y=100, line_dash="dash", line_color="orange", 
                             annotation_text="Good (<100ms)", annotation_position="top right")
                fig.add_hline(y=200, line_dash="dash", line_color="red", 
                             annotation_text="Poor (>200ms)", annotation_position="top right")
                
                fig.update_layout(
                    title='Network Device Latency Performance Analysis',
                    xaxis_title="Network Device",
                    yaxis_title="Average Latency (ms)",
                    hovermode='closest',
                    showlegend=True,
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    ),
                    font=dict(family="Arial, sans-serif", size=12),
                    plot_bgcolor='rgba(0,0,0,0)',
                    paper_bgcolor='rgba(0,0,0,0)',
                    margin=dict(l=50, r=50, t=80, b=50),
                    height=500
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced metrics with performance insights
                avg_latency = enhanced_latency_df['avg_latency_ms'].mean()
                avg_color = "green" if avg_latency <= 50 else "orange" if avg_latency <= 100 else "red"
                
                st.metric("Average Latency", f"{avg_latency:.1f} ms", 
                         delta=f"{avg_latency - 50:.1f} ms", delta_color="normal")
                st.metric("Min Latency", f"{enhanced_latency_df['avg_latency_ms'].min():.1f} ms", 
                         delta="Excellent")
                st.metric("Max Latency", f"{enhanced_latency_df['avg_latency_ms'].max():.1f} ms", 
                         delta="Critical" if enhanced_latency_df['avg_latency_ms'].max() > 200 else "Good")
                
                # Performance insights
                st.markdown("** Network Insights:**")
                excellent_count = len(enhanced_latency_df[enhanced_latency_df['latency_category'] == 'Excellent'])
                poor_count = len(enhanced_latency_df[enhanced_latency_df['latency_category'] == 'Poor'])
                
                if excellent_count >= len(enhanced_latency_df) * 0.8:
                    st.success(f" {excellent_count}/{len(enhanced_latency_df)} devices have excellent latency")
                elif poor_count > 0:
                    st.error(f" {poor_count} devices have poor latency - immediate attention required")
                else:
                    st.warning(" Network performance is acceptable but could be improved")
            
            # Enhanced data display with performance metrics
            st.info(f" {latency_msg}")
            
            # Performance summary
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Excellent (<50ms)", len(enhanced_latency_df[enhanced_latency_df['latency_category'] == 'Excellent']))
            with col2:
                st.metric("Good (50-100ms)", len(enhanced_latency_df[enhanced_latency_df['latency_category'] == 'Good']))
            with col3:
                st.metric("Fair (100-200ms)", len(enhanced_latency_df[enhanced_latency_df['latency_category'] == 'Fair']))
            with col4:
                st.metric("Poor (>200ms)", len(enhanced_latency_df[enhanced_latency_df['latency_category'] == 'Poor']))
            
            display_dataframe_with_index_1(enhanced_latency_df)
    
    st.markdown("---")
    
    # Enhanced System Load Analysis with Capacity Planning
    st.subheader(" System Load Analysis & Capacity Planning")
    if not st.session_state.applications_data.empty:
        load_df, load_msg = calculate_system_load(st.session_state.servers_data, st.session_state.applications_data)
        
        if not load_df.empty:
            # Enhance load data with capacity indicators
            enhanced_load_df = load_df.copy()
            enhanced_load_df['capacity_status'] = pd.cut(
                enhanced_load_df['load_percentage'], 
                bins=[0, 60, 80, 90, float('inf')], 
                labels=['Underutilized', 'Optimal', 'High', 'Critical']
            )
            enhanced_load_df['efficiency_score'] = 100 - enhanced_load_df['load_percentage']
            enhanced_load_df['recommendation'] = enhanced_load_df['capacity_status'].map({
                'Underutilized': 'Consider consolidation',
                'Optimal': 'Maintain current state',
                'High': 'Monitor closely',
                'Critical': 'Immediate scaling required'
            })
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced system load chart with capacity indicators
                fig = go.Figure()
                
                # Color mapping for capacity status
                color_map = {'Underutilized': 'lightgreen', 'Optimal': 'green', 'High': 'orange', 'Critical': 'red'}
                
                for status in enhanced_load_df['capacity_status'].unique():
                    if pd.notna(status):
                        status_data = enhanced_load_df[enhanced_load_df['capacity_status'] == status]
                        fig.add_trace(go.Bar(
                            x=status_data['server_name'],
                            y=status_data['load_percentage'],
                            name=f'{status} Load',
                            marker_color=color_map[status],
                            hovertemplate="<b>%{x}</b><br>" +
                                         "Load: %{y:.1f}%<br>" +
                                         "Status: %{text}<br>" +
                                         "Apps: %{customdata[0]}<br>" +
                                         "Efficiency: %{customdata[1]:.1f}%<br>" +
                                         "<extra></extra>",
                            text=status_data['capacity_status'],
                            customdata=list(zip(status_data['app_count'], status_data['efficiency_score']))
                        ))
                
                # Add capacity thresholds
                fig.add_hline(y=60, line_dash="dash", line_color="green", 
                             annotation_text="Underutilized (<60%)", annotation_position="top right")
                fig.add_hline(y=80, line_dash="dash", line_color="orange", 
                             annotation_text="Optimal (60-80%)", annotation_position="top right")
                fig.add_hline(y=90, line_dash="dash", line_color="red", 
                             annotation_text="Critical (>90%)", annotation_position="top right")
                
                fig.update_layout(
                    title='System Load Analysis with Capacity Indicators',
                    xaxis_title="Server Name",
                    yaxis_title="System Load (%)",
                    barmode='group',
                    hovermode='x unified',
                    showlegend=True,
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    ),
                    font=dict(family="Arial, sans-serif", size=12),
                    plot_bgcolor='rgba(0,0,0,0)',
                    paper_bgcolor='rgba(0,0,0,0)',
                    margin=dict(l=50, r=50, t=80, b=50),
                    height=500
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced metrics with capacity insights
                avg_load = enhanced_load_df['load_percentage'].mean()
                critical_count = len(enhanced_load_df[enhanced_load_df['capacity_status'] == 'Critical'])
                
                st.metric("Average Load", f"{avg_load:.1f}%", 
                         delta=f"{avg_load - 80:.1f}%", delta_color="normal")
                st.metric("Peak Load", f"{enhanced_load_df['load_percentage'].max():.1f}%", 
                         delta="Critical" if enhanced_load_df['load_percentage'].max() > 90 else "Good")
                st.metric("Total Apps", f"{enhanced_load_df['app_count'].sum()}")
                st.metric("Critical Servers", critical_count, 
                         delta="Immediate Action Required" if critical_count > 0 else "All Good")
                
                # Capacity insights
                st.markdown("** Capacity Insights:**")
                if critical_count > 0:
                    st.error(f" {critical_count} servers are at critical capacity")
                elif avg_load > 80:
                    st.warning(" System load is high - consider scaling")
                elif avg_load < 40:
                    st.info(" System is underutilized - consider consolidation")
                else:
                    st.success(" System load is optimal")
            
            # Enhanced data display with capacity metrics
            st.info(f" {load_msg}")
            
            # Capacity summary
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Underutilized", len(enhanced_load_df[enhanced_load_df['capacity_status'] == 'Underutilized']))
            with col2:
                st.metric("Optimal", len(enhanced_load_df[enhanced_load_df['capacity_status'] == 'Optimal']))
            with col3:
                st.metric("High", len(enhanced_load_df[enhanced_load_df['capacity_status'] == 'High']))
            with col4:
                st.metric("Critical", len(enhanced_load_df[enhanced_load_df['capacity_status'] == 'Critical']))
            
            # Recommendations table
            st.markdown("** Capacity Recommendations:**")
            recommendations_df = enhanced_load_df[['server_name', 'load_percentage', 'capacity_status', 'recommendation']].copy()
            recommendations_df = recommendations_df.sort_values('load_percentage', ascending=False)
            st.dataframe(recommendations_df, use_container_width=True)
            
            display_dataframe_with_index_1(enhanced_load_df)
    
    st.markdown("---")
    
    # Enhanced Application Performance Monitoring with Health Analytics
    st.subheader(" Application Performance Monitoring & Health Analytics")
    if not st.session_state.applications_data.empty:
        perf_df, perf_msg = calculate_application_performance(st.session_state.applications_data, st.session_state.incidents_data)
        
        if not perf_df.empty:
            # Enhance performance data with health indicators
            enhanced_perf_df = perf_df.copy()
            enhanced_perf_df['performance_category'] = pd.cut(
                enhanced_perf_df['health_score'], 
                bins=[0, 60, 80, 90, float('inf')], 
                labels=['Critical', 'Poor', 'Good', 'Excellent']
            )
            enhanced_perf_df['response_category'] = pd.cut(
                enhanced_perf_df['response_time_ms'], 
                bins=[0, 100, 300, 500, float('inf')], 
                labels=['Excellent', 'Good', 'Fair', 'Poor']
            )
            enhanced_perf_df['overall_score'] = (
                enhanced_perf_df['health_score'] * 0.4 + 
                (100 - enhanced_perf_df['response_time_ms'] / 10) * 0.3 + 
                (100 - enhanced_perf_df['error_rate_percent']) * 0.3
            )
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced performance scatter plot with health indicators
                fig = go.Figure()
                
                # Color mapping for performance categories
                perf_color_map = {'Critical': 'red', 'Poor': 'orange', 'Good': 'yellow', 'Excellent': 'green'}
                
                for category in enhanced_perf_df['performance_category'].unique():
                    if pd.notna(category):
                        category_data = enhanced_perf_df[enhanced_perf_df['performance_category'] == category]
                        fig.add_trace(go.Scatter(
                            x=category_data['response_time_ms'],
                            y=category_data['error_rate_percent'],
                            mode='markers',
                            name=f'{category} Performance',
                            marker=dict(
                                size=category_data['health_score'] / 5 + 10,
                                color=perf_color_map[category],
                                symbol='circle',
                                line=dict(width=2, color='white')
                            ),
                            text=category_data['app_name'],
                            hovertemplate="<b>%{text}</b><br>" +
                                         "Response Time: %{x:.1f} ms<br>" +
                                         "Error Rate: %{y:.2f}%<br>" +
                                         "Health Score: %{marker.size:.1f}<br>" +
                                         "Overall Score: %{customdata:.1f}<br>" +
                                         "<extra></extra>",
                            customdata=category_data['overall_score']
                        ))
                
                # Add performance thresholds
                fig.add_vline(x=100, line_dash="dash", line_color="green", 
                             annotation_text="Excellent (<100ms)", annotation_position="top")
                fig.add_vline(x=300, line_dash="dash", line_color="orange", 
                             annotation_text="Good (<300ms)", annotation_position="top")
                fig.add_hline(y=1, line_dash="dash", line_color="red", 
                             annotation_text="Error Threshold (1%)", annotation_position="left")
                
                fig.update_layout(
                    title='Application Performance Health Matrix',
                    xaxis_title="Response Time (ms)",
                    yaxis_title="Error Rate (%)",
                    hovermode='closest',
                    showlegend=True,
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    ),
                    font=dict(family="Arial, sans-serif", size=12),
                    plot_bgcolor='rgba(0,0,0,0)',
                    paper_bgcolor='rgba(0,0,0,0)',
                    margin=dict(l=50, r=50, t=80, b=50),
                    height=500
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced health score chart
                fig = go.Figure()
                
                # Color mapping for health scores
                health_colors = ['red' if score < 60 else 'orange' if score < 80 else 'yellow' if score < 90 else 'green' 
                               for score in enhanced_perf_df['health_score']]
                
                fig.add_trace(go.Bar(
                    x=enhanced_perf_df['app_name'],
                    y=enhanced_perf_df['health_score'],
                    marker_color=health_colors,
                    hovertemplate="<b>%{x}</b><br>" +
                                 "Health Score: %{y:.1f}<br>" +
                                 "Overall Score: %{customdata:.1f}<br>" +
                                 "Performance: %{text}<br>" +
                                 "<extra></extra>",
                    customdata=enhanced_perf_df['overall_score'],
                    text=enhanced_perf_df['performance_category']
                ))
                
                # Add health threshold
                fig.add_hline(y=80, line_dash="dash", line_color="orange", 
                             annotation_text="Good Health (80+)", annotation_position="top right")
                
                fig.update_layout(
                    title='Application Health Score Analysis',
                    xaxis_title="Application",
                    yaxis_title="Health Score",
                    xaxis_tickangle=-45,
                    hovermode='x unified',
                    showlegend=False,
                    font=dict(family="Arial, sans-serif", size=12),
                    plot_bgcolor='rgba(0,0,0,0)',
                    paper_bgcolor='rgba(0,0,0,0)',
                    margin=dict(l=50, r=50, t=80, b=50),
                    height=500
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            # Enhanced metrics and insights
            col1, col2, col3 = st.columns(3)
            with col1:
                avg_health = enhanced_perf_df['health_score'].mean()
                st.metric("Average Health", f"{avg_health:.1f}/100", 
                         delta=f"{avg_health - 80:.1f}", delta_color="normal")
            with col2:
                avg_response = enhanced_perf_df['response_time_ms'].mean()
                st.metric("Avg Response", f"{avg_response:.1f} ms", 
                         delta=f"{avg_response - 200:.1f} ms", delta_color="normal")
            with col3:
                avg_error = enhanced_perf_df['error_rate_percent'].mean()
                st.metric("Avg Error Rate", f"{avg_error:.2f}%", 
                         delta=f"{avg_error - 1:.2f}%", delta_color="normal")
            
            # Performance insights
            st.markdown("** Performance Insights:**")
            critical_apps = len(enhanced_perf_df[enhanced_perf_df['performance_category'] == 'Critical'])
            excellent_apps = len(enhanced_perf_df[enhanced_perf_df['performance_category'] == 'Excellent'])
            
            if critical_apps > 0:
                st.error(f" {critical_apps} applications have critical performance issues")
            elif excellent_apps >= len(enhanced_perf_df) * 0.7:
                st.success(f" {excellent_apps}/{len(enhanced_perf_df)} applications have excellent performance")
            else:
                st.warning(" Application performance needs improvement")
            
            # Enhanced data display with performance metrics
            st.info(f" {perf_msg}")
            
            # Performance summary
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Excellent", len(enhanced_perf_df[enhanced_perf_df['performance_category'] == 'Excellent']))
            with col2:
                st.metric("Good", len(enhanced_perf_df[enhanced_perf_df['performance_category'] == 'Good']))
            with col3:
                st.metric("Poor", len(enhanced_perf_df[enhanced_perf_df['performance_category'] == 'Poor']))
            with col4:
                st.metric("Critical", len(enhanced_perf_df[enhanced_perf_df['performance_category'] == 'Critical']))
            
            # Top performers and issues
            st.markdown("** Top Performers:**")
            top_performers = enhanced_perf_df.nlargest(3, 'overall_score')[['app_name', 'overall_score', 'health_score', 'response_time_ms']]
            st.dataframe(top_performers, use_container_width=True)
            
            st.markdown("** Critical Issues:**")
            critical_issues = enhanced_perf_df[enhanced_perf_df['performance_category'] == 'Critical'][['app_name', 'overall_score', 'health_score', 'response_time_ms', 'error_rate_percent']]
            if not critical_issues.empty:
                st.dataframe(critical_issues, use_container_width=True)
            else:
                st.success("No critical performance issues detected!")
            
            display_dataframe_with_index_1(enhanced_perf_df)
    
    st.markdown("---")
    
    # Incident Response Time Analysis
    st.subheader(" Incident Response Time Analysis")
    if not st.session_state.incidents_data.empty:
        response_df, response_msg = calculate_incident_response_time(st.session_state.incidents_data)
        
        if not response_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                fig = px.bar(response_df, x='priority', y='mean', 
                           title='Average Resolution Time by Priority',
                           color='count',
                           color_continuous_scale='Reds')
                fig.update_layout(xaxis_title="Priority", yaxis_title="Resolution Time (minutes)")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.metric("Avg Resolution Time", f"{response_df['mean'].mean():.1f} min")
                st.metric("Total Incidents", f"{response_df['count'].sum()}")
                st.metric("High Priority Avg", f"{response_df[response_df['priority'] == 'High']['mean'].iloc[0] if not response_df[response_df['priority'] == 'High'].empty else 0:.1f} min")
            
            st.info(f" {response_msg}")
            display_dataframe_with_index_1(response_df)
    
    st.markdown("---")
    
    # System Availability Analysis
    st.subheader(" System Availability Analysis")
    if not st.session_state.applications_data.empty:
        availability_df, availability_msg = calculate_system_availability(st.session_state.servers_data, st.session_state.applications_data, st.session_state.incidents_data)
        
        if not availability_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                fig = px.bar(availability_df, x='server_name', y='availability_percentage', 
                           title='System Availability by Server',
                           color='app_count',
                           color_continuous_scale='Greens')
                fig.update_layout(xaxis_title="Server", yaxis_title="Availability %")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.metric("Average Availability", f"{availability_df['availability_percentage'].mean():.2f}%")
                st.metric("Lowest Availability", f"{availability_df['availability_percentage'].min():.2f}%")
                st.metric("Total Critical Apps", f"{availability_df['critical_apps'].sum()}")
            
            st.info(f" {availability_msg}")
            display_dataframe_with_index_1(availability_df)

def show_security_risk():
    """Display world-class security and risk management analysis with advanced threat intelligence"""
    st.title(" Enterprise Security & Risk Management Dashboard")
    
    if st.session_state.security_events_data.empty:
        st.warning(" No security event data available. Please upload data first.")
        return
    
    # Enterprise Security Overview Dashboard
    st.subheader(" Enterprise Security Overview Dashboard")
    
    # Create comprehensive security metrics row
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if not st.session_state.security_events_data.empty:
            total_events = len(st.session_state.security_events_data)
            st.metric("Security Events", total_events, delta="Monitoring", delta_color="normal")
        else:
            st.metric("Security Events", 0, delta="No Data")
    
    with col2:
        if not st.session_state.incidents_data.empty:
            security_incidents = len(st.session_state.incidents_data)
            st.metric("Security Incidents", security_incidents, delta="Active", delta_color="normal")
        else:
            st.metric("Security Incidents", 0, delta="No Data")
    
    with col3:
        if not st.session_state.servers_data.empty:
            total_servers = len(st.session_state.servers_data)
            st.metric("Protected Assets", total_servers, delta="Secured", delta_color="normal")
        else:
            st.metric("Protected Assets", 0, delta="No Data")
    
    with col4:
        if not st.session_state.users_data.empty:
            total_users = len(st.session_state.users_data)
            st.metric("User Accounts", total_users, delta="Protected", delta_color="normal")
        else:
            st.metric("User Accounts", 0, delta="No Data")
    
    st.markdown("---")
    
    # Security Posture Assessment
    st.subheader(" Security Posture Assessment & Risk Scoring")
    
    if not st.session_state.security_events_data.empty:
        # Create comprehensive security posture matrix
        security_data = {
            'Security Domain': ['Vulnerability Management', 'Network Security', 'Access Control', 'Data Protection', 'Incident Response', 'Overall Score'],
            'Current Score': [0, 0, 0, 0, 0, 0],
            'Target Score': [90, 95, 88, 92, 85, 90],
            'Risk Level': ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown'],
            'Status': ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown']
        }
        
        # Calculate actual security scores if data exists
        try:
            vuln_df, _ = calculate_vulnerability_analysis(st.session_state.security_events_data, st.session_state.servers_data, st.session_state.applications_data)
            if not vuln_df.empty:
                vuln_rate = vuln_df['vulnerability_rate_percent'].iloc[0]
                security_data['Current Score'][0] = max(0, 100 - vuln_rate)
                security_data['Risk Level'][0] = 'High' if vuln_rate > 10 else 'Medium' if vuln_rate > 5 else 'Low'
                security_data['Status'][0] = '' if vuln_rate > 10 else '' if vuln_rate > 5 else ''
        except:
            pass
        
        try:
            if not st.session_state.security_events_data.empty:
                firewall_df, _ = calculate_firewall_performance(st.session_state.security_events_data)
                if not firewall_df.empty:
                    firewall_eff = firewall_df['firewall_effectiveness_percent'].iloc[0]
                    security_data['Current Score'][1] = firewall_eff
                    security_data['Risk Level'][1] = 'Low' if firewall_eff > 95 else 'Medium' if firewall_eff > 90 else 'High'
                    security_data['Status'][1] = '' if firewall_eff > 95 else '' if firewall_eff > 90 else ''
        except:
            pass
        
        try:
            if not st.session_state.users_data.empty:
                access_df, _ = calculate_access_control_analysis(st.session_state.users_data, st.session_state.security_events_data)
                if not access_df.empty:
                    access_compliance = access_df['access_control_compliance_percent'].iloc[0]
                    security_data['Current Score'][2] = access_compliance
                    security_data['Risk Level'][2] = 'Low' if access_compliance > 90 else 'Medium' if access_compliance > 80 else 'High'
                    security_data['Status'][2] = '' if access_compliance > 90 else '' if access_compliance > 80 else ''
        except:
            pass
        
        try:
            if not st.session_state.assets_data.empty:
                encryption_df, _ = calculate_encryption_effectiveness(st.session_state.security_events_data, st.session_state.assets_data)
                if not encryption_df.empty:
                    encryption_eff = encryption_df['encryption_effectiveness_percent'].iloc[0]
                    security_data['Current Score'][3] = encryption_eff
                    security_data['Risk Level'][3] = 'Low' if encryption_eff > 90 else 'Medium' if encryption_eff > 80 else 'High'
                    security_data['Status'][3] = '' if encryption_eff > 90 else '' if encryption_eff > 80 else ''
        except:
            pass
        
        try:
            if not st.session_state.incidents_data.empty:
                # Calculate incident response score based on resolution time
                incident_df = st.session_state.incidents_data.copy()
                if 'resolution_time_minutes' in incident_df.columns:
                    avg_resolution = incident_df['resolution_time_minutes'].mean()
                    response_score = max(0, 100 - (avg_resolution / 10))  # Score decreases with longer resolution times
                    security_data['Current Score'][4] = min(100, response_score)
                    security_data['Risk Level'][4] = 'Low' if response_score > 80 else 'Medium' if response_score > 60 else 'High'
                    security_data['Status'][4] = '' if response_score > 80 else '' if response_score > 60 else ''
        except:
            pass
        
        # Calculate overall security score
        if any(security_data['Current Score']):
            valid_scores = [i for i, val in enumerate(security_data['Current Score']) if val > 0]
            if valid_scores:
                overall_score = sum(security_data['Current Score'][i] for i in valid_scores) / len(valid_scores)
                security_data['Current Score'][5] = overall_score
                security_data['Risk Level'][5] = 'Low' if overall_score > 85 else 'Medium' if overall_score > 70 else 'High'
                security_data['Status'][5] = '' if overall_score > 85 else '' if overall_score > 70 else ''
        
        # Create security posture visualization
        security_df = pd.DataFrame(security_data)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Security posture radar chart
            fig = go.Figure()
            
            fig.add_trace(go.Scatterpolar(
                r=security_data['Current Score'][:5],  # Exclude overall score
                theta=security_data['Security Domain'][:5],
                fill='toself',
                name='Current Security Score',
                line_color='blue',
                hovertemplate="<b>%{theta}</b><br>" +
                             "Current Score: %{r:.1f}<br>" +
                             "Target Score: %{customdata}<br>" +
                             "Risk Level: %{text}<br>" +
                             "<extra></extra>",
                customdata=security_data['Target Score'][:5],
                text=[security_data['Risk Level'][i] for i in range(5)]
            ))
            
            fig.add_trace(go.Scatterpolar(
                r=security_data['Target Score'][:5],
                theta=security_data['Security Domain'][:5],
                fill='toself',
                name='Target Security Score',
                line_color='red',
                hovertemplate="<b>%{theta}</b><br>" +
                             "Target Score: %{r:.1f}<br>" +
                             "<extra></extra>"
            ))
            
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(
                        visible=True,
                        range=[0, 100]
                    )),
                showlegend=True,
                title="Enterprise Security Posture Assessment",
                font=dict(family="Arial, sans-serif", size=12),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Security status summary
            st.markdown("** Security Status:**")
            for i, domain in enumerate(security_data['Security Domain']):
                current = security_data['Current Score'][i]
                target = security_data['Target Score'][i]
                status = security_data['Status'][i]
                risk_level = security_data['Risk Level'][i]
                
                if current > 0:
                    delta = f"{current - target:.1f}"
                    delta_color = "normal" if current >= target else "inverse"
                    st.metric(domain, f"{current:.1f}/100", delta=delta, delta_color=delta_color)
                    
                    # Risk level indicator
                    if risk_level == 'High':
                        st.error(f" {risk_level} Risk")
                    elif risk_level == 'Medium':
                        st.warning(f" {risk_level} Risk")
                    elif risk_level == 'Low':
                        st.success(f" {risk_level} Risk")
                else:
                    st.metric(domain, "N/A", delta="No Data")
            
            # Overall security assessment
            if security_data['Current Score'][5] > 0:
                overall_score = security_data['Current Score'][5]
                if overall_score >= 85:
                    st.success(" Enterprise security posture is strong")
                elif overall_score >= 70:
                    st.warning(" Security posture needs improvement")
                else:
                    st.error(" Critical security vulnerabilities detected")
        
        # Security posture summary table
        st.markdown("** Security Posture Summary:**")
        security_summary_df = pd.DataFrame(security_data)
        st.dataframe(security_summary_df, use_container_width=True)
    
    st.markdown("---")
    
    # Advanced Vulnerability Analysis & Threat Intelligence
    st.subheader(" Advanced Vulnerability Analysis & Threat Intelligence")
    vuln_df, vuln_msg = calculate_vulnerability_analysis(st.session_state.security_events_data, st.session_state.servers_data, st.session_state.applications_data)
    
    if not vuln_df.empty:
        # Create enhanced vulnerability data with risk scoring
        enhanced_vuln_df = vuln_df.copy()
        enhanced_vuln_df['risk_score'] = enhanced_vuln_df['vulnerability_rate_percent'].apply(
            lambda x: 'Critical' if x > 15 else 'High' if x > 10 else 'Medium' if x > 5 else 'Low'
        )
        enhanced_vuln_df['priority'] = enhanced_vuln_df['risk_score'].map({
            'Critical': 1, 'High': 2, 'Medium': 3, 'Low': 4
        })
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced vulnerability analysis with risk categorization
            fig = go.Figure()
            
            # Create vulnerability categories with enhanced data
            vuln_categories = ['Critical', 'High', 'Medium', 'Low']
            vuln_counts = []
            vuln_colors = ['red', 'orange', 'yellow', 'green']
            
            for category in vuln_categories:
                if category == 'Critical':
                    count = enhanced_vuln_df['total_vulnerabilities'].iloc[0] * 0.3  # Mock critical vulnerabilities
                elif category == 'High':
                    count = enhanced_vuln_df['total_vulnerabilities'].iloc[0] * 0.4  # Mock high vulnerabilities
                elif category == 'Medium':
                    count = enhanced_vuln_df['total_vulnerabilities'].iloc[0] * 0.2  # Mock medium vulnerabilities
                else:
                    count = enhanced_vuln_df['total_vulnerabilities'].iloc[0] * 0.1  # Mock low vulnerabilities
                vuln_counts.append(int(count))
            
            # Create enhanced pie chart with detailed tooltips
            fig.add_trace(go.Pie(
                labels=vuln_categories,
                values=vuln_counts,
                hole=0.4,
                marker_colors=vuln_colors,
                hovertemplate="<b>%{label}</b><br>" +
                             "Count: %{value}<br>" +
                             "Percentage: %{percent:.1%}<br>" +
                             "Risk Level: %{label}<br>" +
                             "<extra></extra>"
            ))
            
            fig.update_layout(
                title='Vulnerability Risk Distribution & Severity Analysis',
                showlegend=True,
                legend=dict(
                    orientation="v",
                    yanchor="middle",
                    y=0.5,
                    xanchor="left",
                    x=1.02
                ),
                font=dict(family="Arial, sans-serif", size=12),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced vulnerability metrics with risk insights
            vuln_rate = enhanced_vuln_df['vulnerability_rate_percent'].iloc[0]
            risk_level = enhanced_vuln_df['risk_score'].iloc[0]
            
            st.metric("Vulnerability Rate", f"{vuln_rate:.1f}%", 
                     delta=f"{vuln_rate - 5:.1f}%", delta_color="inverse")
            st.metric("Total Systems", enhanced_vuln_df['total_systems'].iloc[0], delta="Protected")
            st.metric("Total Vulnerabilities", enhanced_vuln_df['total_vulnerabilities'].iloc[0], delta="Detected")
            st.metric("Risk Level", risk_level, 
                     delta="Critical" if risk_level == 'Critical' else "High" if risk_level == 'High' else "Medium" if risk_level == 'Medium' else "Low")
            
            # Risk assessment insights
            st.markdown("** Risk Assessment:**")
            if risk_level == 'Critical':
                st.error(" Critical risk level - Immediate remediation required")
            elif risk_level == 'High':
                st.warning(" High risk level - Prioritize remediation")
            elif risk_level == 'Medium':
                st.info(" Medium risk level - Schedule remediation")
            else:
                st.success(" Low risk level - Monitor and maintain")
        
        # Enhanced vulnerability insights and recommendations
        st.info(f" {vuln_msg}")
        
        # Vulnerability summary with risk categories
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Critical", vuln_counts[0], delta="Immediate Action")
        with col2:
            st.metric("High", vuln_counts[1], delta="High Priority")
        with col3:
            st.metric("Medium", vuln_counts[2], delta="Schedule")
        with col4:
            st.metric("Low", vuln_counts[3], delta="Monitor")
        
        # Threat intelligence insights
        st.markdown("** Threat Intelligence Insights:**")
        if vuln_rate > 15:
            st.error(" **CRITICAL THREAT LEVEL**: Multiple high-severity vulnerabilities detected. Immediate security team engagement required.")
        elif vuln_rate > 10:
            st.warning(" **ELEVATED THREAT LEVEL**: Several vulnerabilities require attention. Review security protocols and patch management.")
        elif vuln_rate > 5:
            st.info(" **MODERATE THREAT LEVEL**: Some vulnerabilities detected. Implement regular security scanning and updates.")
        else:
            st.success(" **LOW THREAT LEVEL**: Minimal vulnerabilities detected. Maintain current security posture.")
        
        # Remediation recommendations
        st.markdown("** Remediation Recommendations:**")
        if vuln_rate > 10:
            st.markdown("""
            - **Immediate Actions**: Patch critical vulnerabilities within 24 hours
            - **Short-term**: Implement automated vulnerability scanning
            - **Medium-term**: Establish vulnerability management program
            - **Long-term**: Develop security-first development practices
            """)
        elif vuln_rate > 5:
            st.markdown("""
            - **Short-term**: Address high-priority vulnerabilities within 7 days
            - **Medium-term**: Enhance security monitoring and alerting
            - **Long-term**: Implement security training programs
            """)
        else:
            st.markdown("""
            - **Maintenance**: Continue regular security assessments
            - **Enhancement**: Implement proactive security measures
            - **Training**: Regular security awareness training
            """)
        
        display_dataframe_with_index_1(enhanced_vuln_df)
    
    st.markdown("---")
    
    # Advanced Network Security & Firewall Analytics
    st.subheader(" Advanced Network Security & Firewall Analytics")
    firewall_df, firewall_msg = calculate_firewall_performance(st.session_state.security_events_data)
    
    if not firewall_df.empty:
        # Create enhanced firewall data with threat analysis
        enhanced_firewall_df = firewall_df.copy()
        enhanced_firewall_df['threat_level'] = enhanced_firewall_df['firewall_effectiveness_percent'].apply(
            lambda x: 'Low' if x > 95 else 'Medium' if x > 90 else 'High'
        )
        enhanced_firewall_df['attack_pattern'] = 'Distributed' if enhanced_firewall_df['total_access_attempts'].iloc[0] > 1000 else 'Targeted'
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced network security dashboard
            fig = go.Figure()
            
            # Create comprehensive firewall analysis
            attack_types = ['Blocked Attempts', 'Successful Attempts', 'Suspicious Patterns', 'DDoS Attempts']
            attack_counts = [
                enhanced_firewall_df['blocked_attempts'].iloc[0],
                enhanced_firewall_df['successful_attempts'].iloc[0],
                int(enhanced_firewall_df['total_access_attempts'].iloc[0] * 0.1),  # Mock suspicious patterns
                int(enhanced_firewall_df['total_access_attempts'].iloc[0] * 0.05)   # Mock DDoS attempts
            ]
            attack_colors = ['red', 'green', 'orange', 'purple']
            
            # Create enhanced bar chart with threat indicators
            fig.add_trace(go.Bar(
                x=attack_types,
                y=attack_counts,
                marker_color=attack_colors,
                hovertemplate="<b>%{x}</b><br>" +
                             "Count: %{y}<br>" +
                             "Threat Level: %{customdata}<br>" +
                             "<extra></extra>",
                customdata=[enhanced_firewall_df['threat_level'].iloc[0]] * len(attack_types)
            ))
            
            # Add threat threshold lines
            fig.add_hline(y=enhanced_firewall_df['total_access_attempts'].iloc[0] * 0.8, 
                         line_dash="dash", line_color="orange", 
                         annotation_text="High Activity Threshold", annotation_position="top right")
            
            fig.update_layout(
                title='Network Security Threat Analysis & Firewall Performance',
                xaxis_title="Attack Type",
                yaxis_title="Count",
                hovermode='x unified',
                showlegend=False,
                font=dict(family="Arial, sans-serif", size=12),
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)',
                margin=dict(l=50, r=50, t=80, b=50),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced network security metrics
            firewall_eff = enhanced_firewall_df['firewall_effectiveness_percent'].iloc[0]
            threat_level = enhanced_firewall_df['threat_level'].iloc[0]
            attack_pattern = enhanced_firewall_df['attack_pattern'].iloc[0]
            
            st.metric("Firewall Effectiveness", f"{firewall_eff:.1f}%", 
                     delta=f"{firewall_eff - 95:.1f}%", delta_color="normal")
            st.metric("Total Attempts", enhanced_firewall_df['total_access_attempts'].iloc[0], delta="Monitored")
            st.metric("Blocked Attempts", enhanced_firewall_df['blocked_attempts'].iloc[0], delta="Protected")
            st.metric("Threat Level", threat_level, 
                     delta="High" if threat_level == 'High' else "Medium" if threat_level == 'Medium' else "Low")
            
            # Network security insights
            st.markdown("** Security Insights:**")
            if threat_level == 'High':
                st.error(" High threat level - Review firewall rules and security policies")
            elif threat_level == 'Medium':
                st.warning(" Medium threat level - Monitor network activity closely")
            else:
                st.success(" Low threat level - Network security is effective")
            
            st.markdown(f"** Attack Pattern:** {attack_pattern}")
        
        # Enhanced network security insights and recommendations
        st.info(f" {firewall_msg}")
        
        # Network security summary
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Blocked", enhanced_firewall_df['blocked_attempts'].iloc[0], delta="Protected")
        with col2:
            st.metric("Successful", enhanced_firewall_df['successful_attempts'].iloc[0], delta="Investigate")
        with col3:
            st.metric("Suspicious", int(enhanced_firewall_df['total_access_attempts'].iloc[0] * 0.1), delta="Monitor")
        with col4:
            st.metric("DDoS", int(enhanced_firewall_df['total_access_attempts'].iloc[0] * 0.05), delta="Mitigate")
        
        # Network security recommendations
        st.markdown("** Network Security Recommendations:**")
        if threat_level == 'High':
            st.markdown("""
            - **Immediate**: Review and update firewall rules
            - **Short-term**: Implement intrusion detection systems
            - **Medium-term**: Conduct security architecture review
            - **Long-term**: Establish security operations center (SOC)
            """)
        elif threat_level == 'Medium':
            st.markdown("""
            - **Short-term**: Enhance monitoring and alerting
            - **Medium-term**: Implement advanced threat detection
            - **Long-term**: Develop incident response procedures
            """)
        else:
            st.markdown("""
            - **Maintenance**: Continue current security practices
            - **Enhancement**: Implement proactive threat hunting
            - **Training**: Regular security team training
            """)
        
        display_dataframe_with_index_1(enhanced_firewall_df)
    
    st.markdown("---")
    
    # Advanced Data Breach Analysis & Incident Response
    st.subheader(" Advanced Data Breach Analysis & Incident Response")
    breach_df, breach_msg = calculate_data_breach_analysis(st.session_state.security_events_data, st.session_state.incidents_data)
    
    if not breach_df.empty:
        # Create enhanced breach data with incident analysis
        enhanced_breach_df = breach_df.copy()
        enhanced_breach_df['breach_severity'] = enhanced_breach_df['data_breach_rate_percent'].apply(
            lambda x: 'Critical' if x > 0.1 else 'High' if x > 0.05 else 'Medium' if x > 0.01 else 'Low'
        )
        enhanced_breach_df['response_time'] = 'Immediate' if enhanced_breach_df['data_breaches_detected'].iloc[0] > 0 else 'None'
        enhanced_breach_df['data_exposure'] = enhanced_breach_df['total_data_stored'].iloc[0] * enhanced_breach_df['data_breach_rate_percent'].iloc[0] / 100
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced breach analysis dashboard
            fig = go.Figure()
            
            # Create comprehensive breach analysis
            breach_categories = ['Data Breaches', 'Security Events', 'Incident Breaches', 'Potential Threats', 'False Positives']
            breach_counts = [
                enhanced_breach_df['data_breaches_detected'].iloc[0],
                enhanced_breach_df['security_events'].iloc[0],
                enhanced_breach_df['incident_breaches'].iloc[0],
                int(enhanced_breach_df['security_events'].iloc[0] * 0.3),  # Mock potential threats
                int(enhanced_breach_df['security_events'].iloc[0] * 0.1)   # Mock false positives
            ]
            breach_colors = ['red', 'orange', 'yellow', 'purple', 'gray']
            
            # Create enhanced bar chart with severity indicators
            fig.add_trace(go.Bar(
                x=breach_categories,
                y=breach_counts,
                marker_color=breach_colors,
                hovertemplate="<b>%{x}</b><br>" +
                             "Count: %{y}<br>" +
                             "Severity: %{customdata[0]}<br>" +
                             "Data Exposure: %{customdata[1]:.0f} GB<br>" +
                             "<extra></extra>",
                customdata=list(zip(
                    [enhanced_breach_df['breach_severity'].iloc[0]] * len(breach_categories),
                    [enhanced_breach_df['data_exposure']] * len(breach_categories)
                ))
            ))
            
            # Add severity threshold lines
            fig.add_hline(y=enhanced_breach_df['security_events'].iloc[0] * 0.5, 
                         line_dash="dash", line_color="orange", 
                         annotation_text="High Activity Threshold", annotation_position="top right")
            
            fig.update_layout(
                title='Data Breach Analysis & Incident Response Dashboard',
                xaxis_title="Incident Type",
                yaxis_title="Count",
                hovermode='x unified',
                showlegend=False,
                font=dict(family="Arial, sans-serif", size=12),
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)',
                margin=dict(l=50, r=50, t=80, b=50),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced breach metrics with incident insights
            breach_rate = enhanced_breach_df['data_breach_rate_percent'].iloc[0]
            breach_severity = enhanced_breach_df['breach_severity'].iloc[0]
            response_time = enhanced_breach_df['response_time'].iloc[0]
            data_exposure = enhanced_breach_df['data_exposure'].iloc[0]
            
            st.metric("Data Breach Rate", f"{breach_rate:.3f}%", 
                     delta=f"{breach_rate - 0.01:.3f}%", delta_color="inverse")
            st.metric("Total Data Stored", enhanced_breach_df['total_data_stored'].iloc[0], delta="Protected")
            st.metric("Breaches Detected", enhanced_breach_df['data_breaches_detected'].iloc[0], delta="Investigated")
            st.metric("Data Exposure", f"{data_exposure:.0f} GB", 
                     delta="Critical" if data_exposure > 100 else "High" if data_exposure > 50 else "Medium" if data_exposure > 10 else "Low")
            
            # Incident response insights
            st.markdown("** Incident Response:**")
            if breach_severity == 'Critical':
                st.error(" Critical breach severity - Immediate incident response required")
            elif breach_severity == 'High':
                st.warning(" High breach severity - Escalate to security team")
            elif breach_severity == 'Medium':
                st.info(" Medium breach severity - Monitor and investigate")
            else:
                st.success(" Low breach severity - Standard monitoring")
            
            st.markdown(f"** Response Time:** {response_time}")
        
        # Enhanced breach insights and incident response
        st.info(f" {breach_msg}")
        
        # Breach analysis summary
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Data Breaches", enhanced_breach_df['data_breaches_detected'].iloc[0], delta="Investigate")
        with col2:
            st.metric("Security Events", enhanced_breach_df['security_events'].iloc[0], delta="Monitor")
        with col3:
            st.metric("Incident Breaches", enhanced_breach_df['incident_breaches'].iloc[0], delta="Respond")
        with col4:
            st.metric("Potential Threats", int(enhanced_breach_df['security_events'].iloc[0] * 0.3), delta="Analyze")
        
        # Incident response recommendations
        st.markdown("** Incident Response Recommendations:**")
        if breach_severity == 'Critical':
            st.markdown("""
            - **Immediate (0-1 hour)**: Activate incident response team, isolate affected systems
            - **Short-term (1-4 hours)**: Assess scope, notify stakeholders, begin containment
            - **Medium-term (4-24 hours)**: Complete investigation, implement remediation
            - **Long-term (24+ hours)**: Post-incident review, update security policies
            """)
        elif breach_severity == 'High':
            st.markdown("""
            - **Immediate (0-4 hours)**: Activate incident response procedures
            - **Short-term (4-24 hours)**: Investigate and contain incident
            - **Medium-term (1-7 days)**: Remediate vulnerabilities, update procedures
            - **Long-term (1-4 weeks)**: Conduct lessons learned, improve security
            """)
        else:
            st.markdown("""
            - **Short-term**: Investigate and document incident
            - **Medium-term**: Implement preventive measures
            - **Long-term**: Review and improve security controls
            """)
        
        # Data protection insights
        st.markdown("** Data Protection Insights:**")
        if data_exposure > 100:
            st.error(" **CRITICAL DATA EXPOSURE**: Large amount of data potentially compromised. Immediate action required.")
        elif data_exposure > 50:
            st.warning(" **HIGH DATA EXPOSURE**: Significant data potentially at risk. Prioritize investigation.")
        elif data_exposure > 10:
            st.info(" **MODERATE DATA EXPOSURE**: Some data potentially compromised. Investigate thoroughly.")
        else:
            st.success(" **LOW DATA EXPOSURE**: Minimal data potentially compromised. Continue monitoring.")
        
        display_dataframe_with_index_1(enhanced_breach_df)
    
    st.markdown("---")
    
    # Advanced Threat Intelligence & Security Analytics
    st.subheader(" Advanced Threat Intelligence & Security Analytics")
    
    if not st.session_state.security_events_data.empty:
        # Create comprehensive threat intelligence dashboard
        threat_data = {
            'Threat Category': ['Malware', 'Phishing', 'DDoS', 'Insider Threats', 'Advanced Persistent Threats', 'Zero-Day Exploits'],
            'Threat Count': [0, 0, 0, 0, 0, 0],
            'Risk Level': ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown'],
            'Detection Rate': [0, 0, 0, 0, 0, 0],
            'Response Time': [0, 0, 0, 0, 0, 0]
        }
        
        # Calculate threat metrics if data exists
        try:
            if not st.session_state.security_events_data.empty:
                # Mock threat intelligence data based on security events
                total_events = len(st.session_state.security_events_data)
                threat_data['Threat Count'] = [
                    int(total_events * 0.25),  # Malware
                    int(total_events * 0.20),  # Phishing
                    int(total_events * 0.15),  # DDoS
                    int(total_events * 0.10),  # Insider Threats
                    int(total_events * 0.20),  # APT
                    int(total_events * 0.10)   # Zero-Day
                ]
                
                # Calculate risk levels and detection rates
                for i in range(len(threat_data['Threat Category'])):
                    count = threat_data['Threat Count'][i]
                    if count > total_events * 0.3:
                        threat_data['Risk Level'][i] = 'Critical'
                        threat_data['Detection Rate'][i] = 95
                        threat_data['Response Time'][i] = 15
                    elif count > total_events * 0.2:
                        threat_data['Risk Level'][i] = 'High'
                        threat_data['Detection Rate'][i] = 88
                        threat_data['Response Time'][i] = 30
                    elif count > total_events * 0.1:
                        threat_data['Risk Level'][i] = 'Medium'
                        threat_data['Detection Rate'][i] = 75
                        threat_data['Response Time'][i] = 60
                    else:
                        threat_data['Risk Level'][i] = 'Low'
                        threat_data['Detection Rate'][i] = 90
                        threat_data['Response Time'][i] = 120
        except:
            pass
        
        # Create threat intelligence visualization
        threat_df = pd.DataFrame(threat_data)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Threat intelligence heatmap
            fig = go.Figure()
            
            # Create heatmap data
            threat_matrix = []
            for i, category in enumerate(threat_data['Threat Category']):
                threat_matrix.append([
                    threat_data['Threat Count'][i],
                    threat_data['Detection Rate'][i],
                    threat_data['Response Time'][i]
                ])
            
            # Create heatmap
            fig.add_trace(go.Heatmap(
                z=threat_matrix,
                x=['Threat Count', 'Detection Rate (%)', 'Response Time (min)'],
                y=threat_data['Threat Category'],
                colorscale='RdYlGn_r',
                hovertemplate="<b>%{y}</b><br>" +
                             "Metric: %{x}<br>" +
                             "Value: %{z}<br>" +
                             "<extra></extra>"
            ))
            
            fig.update_layout(
                title='Threat Intelligence Matrix & Security Analytics',
                xaxis_title="Security Metrics",
                yaxis_title="Threat Categories",
                font=dict(family="Arial, sans-serif", size=12),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Threat intelligence summary
            st.markdown("** Threat Intelligence Summary:**")
            
            # Calculate overall threat score
            total_threats = sum(threat_data['Threat Count'])
            avg_detection = sum(threat_data['Detection Rate']) / len(threat_data['Detection Rate'])
            avg_response = sum(threat_data['Response Time']) / len(threat_data['Response Time'])
            
            st.metric("Total Threats", total_threats, delta="Detected")
            st.metric("Avg Detection Rate", f"{avg_detection:.1f}%", 
                     delta=f"{avg_detection - 85:.1f}%", delta_color="normal")
            st.metric("Avg Response Time", f"{avg_response:.0f} min", 
                     delta=f"{avg_response - 60:.0f} min", delta_color="inverse")
            
            # Threat level assessment
            critical_threats = sum(1 for level in threat_data['Risk Level'] if level == 'Critical')
            if critical_threats > 0:
                st.error(f" {critical_threats} critical threats detected")
            elif total_threats > total_events * 0.5:
                st.warning(" Elevated threat activity detected")
            else:
                st.success(" Threat level is manageable")
        
        # Threat intelligence insights and recommendations
        st.markdown("** Threat Intelligence Insights:**")
        
        # Identify top threats
        top_threats = sorted(zip(threat_data['Threat Category'], threat_data['Threat Count']), 
                            key=lambda x: x[1], reverse=True)[:3]
        
        st.markdown("** Top Threat Categories:**")
        for i, (category, count) in enumerate(top_threats, 1):
            st.markdown(f"{i}. **{category}**: {count} incidents detected")
        
        # Security recommendations based on threat analysis
        st.markdown("** Security Recommendations:**")
        if critical_threats > 0:
            st.markdown("""
            - **Immediate**: Activate security incident response team
            - **Short-term**: Implement additional security controls
            - **Medium-term**: Conduct security architecture review
            - **Long-term**: Establish threat hunting program
            """)
        elif total_threats > total_events * 0.5:
            st.markdown("""
            - **Short-term**: Enhance security monitoring and alerting
            - **Medium-term**: Implement advanced threat detection
            - **Long-term**: Develop security operations center
            """)
        else:
            st.markdown("""
            - **Maintenance**: Continue current security practices
            - **Enhancement**: Implement proactive threat hunting
            - **Training**: Regular security team training
            """)
        
        # Threat intelligence summary table
        st.markdown("** Threat Intelligence Summary:**")
        st.dataframe(threat_df, use_container_width=True)
    
    st.markdown("---")
    
    # Access Control Analysis
    st.subheader(" Access Control Analysis")
    if not st.session_state.users_data.empty:
        access_df, access_msg = calculate_access_control_analysis(st.session_state.users_data, st.session_state.security_events_data)
        
        if not access_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Create data for the pie chart
                access_data = pd.DataFrame({
                    'Status': ['Active Users', 'Inactive Users'],
                    'Count': [access_df['active_users'].iloc[0], access_df['inactive_users'].iloc[0]]
                })
                
                fig = create_chart("pie", access_data, values='Count',
                           names='Status',
                           title='User Access Status')
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.metric("Access Control Compliance", f"{access_df['access_control_compliance_percent'].iloc[0]:.1f}%")
                st.metric("Total Users", access_df['total_users'].iloc[0])
                st.metric("Unauthorized Attempts", access_df['unauthorized_access_attempts'].iloc[0])
            
            st.info(f" {access_msg}")
            display_dataframe_with_index_1(access_df)
    
    st.markdown("---")
    
    # Phishing Metrics
    st.subheader(" Phishing Attack Metrics")
    phishing_df, phishing_msg = calculate_phishing_metrics(st.session_state.security_events_data, st.session_state.incidents_data)
    
    if not phishing_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            fig = px.bar(x=['Phishing Attempts', 'Successful Attacks'], 
                        y=[phishing_df['phishing_attempts'].iloc[0], phishing_df['successful_phishing_attacks'].iloc[0]],
                        title='Phishing Attack Analysis',
                        color=['Attempts', 'Successful'],
                        color_discrete_map={'Attempts': 'orange', 'Successful': 'red'})
            fig.update_layout(xaxis_title="Attack Type", yaxis_title="Count")
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.metric("Phishing Attempt Rate", f"{phishing_df['phishing_attempt_rate_percent'].iloc[0]:.2f}%")
            st.metric("Phishing Success Rate", f"{phishing_df['phishing_success_rate_percent'].iloc[0]:.1f}%")
            st.metric("Total User Interactions", phishing_df['total_user_interactions'].iloc[0])
        
        st.info(f" {phishing_msg}")
        display_dataframe_with_index_1(phishing_df)
    
    st.markdown("---")
    
    # Encryption Effectiveness
    st.subheader(" Encryption Effectiveness")
    if not st.session_state.assets_data.empty:
        encryption_df, encryption_msg = calculate_encryption_effectiveness(st.session_state.security_events_data, st.session_state.assets_data)
        
        if not encryption_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                fig = px.pie(values=[encryption_df['encrypted_assets'].iloc[0], encryption_df['unencrypted_assets'].iloc[0]], 
                           names=['Encrypted', 'Unencrypted'],
                           title='Asset Encryption Status')
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.metric("Encryption Effectiveness", f"{encryption_df['encryption_effectiveness_percent'].iloc[0]:.1f}%")
                st.metric("Sensitive Assets", encryption_df['total_sensitive_assets'].iloc[0])
                st.metric("Encrypted Assets", encryption_df['encrypted_assets'].iloc[0])
            
            st.info(f" {encryption_msg}")
            display_dataframe_with_index_1(encryption_df)

def show_it_support():
    """Display enhanced IT support and help desk analysis with world-class analytics"""
    st.title(" Enterprise IT Support & Help Desk Excellence Dashboard")
    
    if st.session_state.tickets_data.empty:
        st.warning(" No ticket data available. Please upload data first.")
        return
    
    # Enhanced IT Support Overview Dashboard
    st.subheader(" Enterprise IT Support Overview Dashboard")
    
    # Calculate comprehensive overview metrics
    total_tickets = len(st.session_state.tickets_data)
    resolved_tickets = len(st.session_state.tickets_data[st.session_state.tickets_data['status'] == 'resolved'])
    pending_tickets = len(st.session_state.tickets_data[st.session_state.tickets_data['status'] == 'pending'])
    open_tickets = len(st.session_state.tickets_data[st.session_state.tickets_data['status'] == 'open'])
    
    # Create overview metrics with enhanced styling
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Total Tickets", 
            f"{total_tickets:,}",
            delta=f"{total_tickets - (total_tickets * 0.95):.0f}",
            delta_color="normal"
        )
    
    with col2:
        resolution_rate = (resolved_tickets / total_tickets * 100) if total_tickets > 0 else 0
        st.metric(
            "Resolution Rate", 
            f"{resolution_rate:.1f}%",
            delta=f"{resolution_rate - 85:.1f}%",
            delta_color="normal" if resolution_rate >= 85 else "inverse"
        )
    
    with col3:
        st.metric(
            "Pending Tickets", 
            f"{pending_tickets:,}",
            delta=f"{pending_tickets - (total_tickets * 0.1):.0f}",
            delta_color="inverse" if pending_tickets > (total_tickets * 0.1) else "normal"
        )
    
    with col4:
        avg_resolution_time = st.session_state.tickets_data[st.session_state.tickets_data['status'] == 'resolved']['resolution_time'].mean() if resolved_tickets > 0 else 0
        st.metric(
            "Avg Resolution Time", 
            f"{avg_resolution_time:.1f} min",
            delta=f"{avg_resolution_time - 120:.1f} min",
            delta_color="normal" if avg_resolution_time <= 120 else "inverse"
        )
    
    st.markdown("---")
    
    # Enhanced Ticket Resolution Rate with Advanced Analytics
    st.subheader(" Advanced Ticket Resolution & SLA Performance Analytics")
    resolution_df, resolution_msg = calculate_ticket_resolution_rate(st.session_state.tickets_data)
    
    if not resolution_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced SLA compliance analysis with detailed breakdown
            sla_compliant = resolution_df['sla_compliant_tickets'].iloc[0]
            sla_violation = resolution_df['resolved_tickets'].iloc[0] - resolution_df['sla_compliant_tickets'].iloc[0]
            total_resolved = resolution_df['resolved_tickets'].iloc[0]
            
            # Create enhanced pie chart with detailed tooltips
            fig = go.Figure()
            fig.add_trace(go.Pie(
                labels=['SLA Compliant', 'SLA Violation'],
                values=[sla_compliant, sla_violation],
                hole=0.4,
                marker_colors=['#00ff88', '#ff4444'],
                hovertemplate="<b>%{label}</b><br>" +
                             "Count: %{value}<br>" +
                             "Percentage: %{percent:.1%}<br>" +
                             "<extra></extra>",
                textinfo='label+percent',
                textfont_size=14
            ))
            
            fig.update_layout(
                title="Enterprise SLA Compliance Performance",
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                font=dict(family="Arial, sans-serif", size=12),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced metrics with performance indicators
            sla_rate = (sla_compliant / total_resolved * 100) if total_resolved > 0 else 0
            
            st.metric(
                "SLA Compliance Rate", 
                f"{sla_rate:.1f}%",
                delta=f"{sla_rate - 90:.1f}%",
                delta_color="normal" if sla_rate >= 90 else "inverse"
            )
            
            st.metric(
                "Resolution Rate", 
                f"{resolution_df['resolution_rate_percent'].iloc[0]:.1f}%",
                delta=f"{resolution_df['resolution_rate_percent'].iloc[0] - 85:.1f}%",
                delta_color="normal" if resolution_df['resolution_rate_percent'].iloc[0] >= 85 else "inverse"
            )
            
            st.metric(
                "Total Tickets", 
                f"{resolution_df['total_tickets'].iloc[0]:,}",
                delta="Active"
            )
            
            st.metric(
                "Pending Tickets", 
                f"{resolution_df['pending_tickets'].iloc[0]:,}",
                delta="Require Attention" if resolution_df['pending_tickets'].iloc[0] > 0 else "All Clear"
            )
            
            # SLA performance insights
            if sla_rate >= 95:
                st.success(" Excellent SLA compliance - exceeding enterprise standards")
            elif sla_rate >= 90:
                st.info(" Good SLA compliance - meeting enterprise standards")
            elif sla_rate >= 80:
                st.warning(" Moderate SLA compliance - needs improvement")
            else:
                st.error(" Poor SLA compliance - immediate action required")
        
        # Enhanced resolution insights
        st.markdown("** Resolution Performance Insights:**")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("SLA Compliant", f"{sla_compliant:,}", delta="On Target")
        
        with col2:
            st.metric("SLA Violations", f"{sla_violation:,}", delta="Needs Improvement")
        
        with col3:
            st.metric("Compliance Gap", f"{90 - sla_rate:.1f}%", delta="To Target")
        
        st.info(f" {resolution_msg}")
        display_dataframe_with_index_1(resolution_df)
    
    st.markdown("---")
    
    # Enhanced First Call Resolution with Predictive Analytics
    st.subheader(" Advanced First Call Resolution & Escalation Analytics")
    fcr_df, fcr_msg = calculate_first_call_resolution(st.session_state.tickets_data)
    
    if not fcr_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced FCR analysis with performance indicators
            fcr_issues = fcr_df['issues_resolved_first_call'].iloc[0]
            escalated_issues = fcr_df['escalated_issues'].iloc[0]
            total_issues = fcr_df['total_issues_reported'].iloc[0]
            fcr_rate = fcr_df['fcr_rate_percent'].iloc[0]
            
            # Create enhanced bar chart with performance thresholds
            fig = go.Figure()
            
            fig.add_trace(go.Bar(
                x=['FCR Issues', 'Escalated Issues'],
                y=[fcr_issues, escalated_issues],
                marker_color=['#00ff88', '#ff8800'],
                hovertemplate="<b>%{x}</b><br>" +
                             "Count: %{y}<br>" +
                             "Percentage: %{customdata:.1f}%<br>" +
                             "<extra></extra>",
                customdata=[(fcr_issues/total_issues)*100, (escalated_issues/total_issues)*100]
            ))
            
            # Add performance threshold lines
            fig.add_hline(y=total_issues * 0.8, line_dash="dash", line_color="green", 
                         annotation_text="Target FCR Rate (80%)", annotation_position="top right")
            fig.add_hline(y=total_issues * 0.6, line_dash="dash", line_color="orange", 
                         annotation_text="Minimum FCR Rate (60%)", annotation_position="top right")
            
            fig.update_layout(
                title="First Call Resolution Performance Analysis",
                xaxis_title="Issue Resolution Type",
                yaxis_title="Issue Count",
                showlegend=False,
                font=dict(family="Arial, sans-serif", size=12),
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)',
                margin=dict(l=50, r=50, t=80, b=50),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced FCR metrics with performance indicators
            st.metric(
                "FCR Rate", 
                f"{fcr_rate:.1f}%",
                delta=f"{fcr_rate - 80:.1f}%",
                delta_color="normal" if fcr_rate >= 80 else "inverse"
            )
            
            st.metric(
                "Total Issues", 
                f"{total_issues:,}",
                delta="Reported"
            )
            
            st.metric(
                "FCR Issues", 
                f"{fcr_issues:,}",
                delta="Efficient Resolution"
            )
            
            st.metric(
                "Escalated Issues", 
                f"{escalated_issues:,}",
                delta="Require Escalation"
            )
            
            # FCR performance insights
            if fcr_rate >= 85:
                st.success(" Excellent FCR rate - exceeding enterprise standards")
            elif fcr_rate >= 80:
                st.info(" Good FCR rate - meeting enterprise standards")
            elif fcr_rate >= 70:
                st.warning(" Moderate FCR rate - needs improvement")
            else:
                st.error(" Poor FCR rate - immediate action required")
        
        # Enhanced FCR insights
        st.markdown("** FCR Performance Insights:**")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Efficiency Rate", f"{fcr_rate:.1f}%", delta="Performance")
        
        with col2:
            escalation_rate = (escalated_issues / total_issues * 100) if total_issues > 0 else 0
            st.metric("Escalation Rate", f"{escalation_rate:.1f}%", delta="Reduction Target")
        
        with col3:
            st.metric("Improvement Opportunity", f"{85 - fcr_rate:.1f}%", delta="To Excellence")
        
        st.info(f" {fcr_msg}")
        display_dataframe_with_index_1(fcr_df)
    
    st.markdown("---")
    
    # Enhanced Average Resolution Time with Capacity Planning
    st.subheader(" Advanced Resolution Time Analytics & Capacity Planning")
    time_df, time_msg = calculate_average_resolution_time(st.session_state.tickets_data)
    
    if not time_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced resolution time analysis with performance thresholds
            fig = go.Figure()
            
            # Create enhanced bar chart with performance indicators
            fig.add_trace(go.Bar(
                x=time_df['priority'],
                y=time_df['mean'],
                marker_color=time_df['count'],
                hovertemplate="<b>Priority: %{x}</b><br>" +
                             "Avg Time: %{y:.1f} min<br>" +
                             "Ticket Count: %{customdata}<br>" +
                             "<extra></extra>",
                customdata=time_df['count']
            ))
            
            # Add performance threshold lines
            fig.add_hline(y=120, line_dash="dash", line_color="green", 
                         annotation_text="Target Resolution Time (2h)", annotation_position="top right")
            fig.add_hline(y=240, line_dash="dash", line_color="orange", 
                         annotation_text="Warning Threshold (4h)", annotation_position="top right")
            fig.add_hline(y=480, line_dash="dash", line_color="red", 
                         annotation_text="Critical Threshold (8h)", annotation_position="top right")
            
            fig.update_layout(
                title="Resolution Time Performance by Priority",
                xaxis_title="Ticket Priority",
                yaxis_title="Resolution Time (minutes)",
                showlegend=False,
                font=dict(family="Arial, sans-serif", size=12),
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)',
                margin=dict(l=50, r=50, t=80, b=50),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced resolution time metrics
            avg_resolution = time_df['mean'].mean()
            fastest_resolution = time_df['mean'].min()
            slowest_resolution = time_df['mean'].max()
            total_resolved = time_df['count'].sum()
            
            st.metric(
                "Avg Resolution Time", 
                f"{avg_resolution:.1f} min",
                delta=f"{avg_resolution - 120:.1f} min",
                delta_color="normal" if avg_resolution <= 120 else "inverse"
            )
            
            st.metric(
                "Total Resolved", 
                f"{total_resolved:,}",
                delta="Completed"
            )
            
            st.metric(
                "Fastest Resolution", 
                f"{fastest_resolution:.1f} min",
                delta="Best Performance"
            )
            
            st.metric(
                "Slowest Resolution", 
                f"{slowest_resolution:.1f} min",
                delta="Improvement Target"
            )
            
            # Resolution time performance insights
            if avg_resolution <= 120:
                st.success(" Excellent resolution time - exceeding enterprise standards")
            elif avg_resolution <= 180:
                st.info(" Good resolution time - meeting enterprise standards")
            elif avg_resolution <= 240:
                st.warning(" Moderate resolution time - needs improvement")
            else:
                st.error(" Poor resolution time - immediate action required")
        
        # Enhanced resolution time insights
        st.markdown("** Resolution Time Performance Insights:**")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Performance Score", f"{max(0, 100 - (avg_resolution - 120) / 2):.0f}/100", delta="Efficiency")
        
        with col2:
            st.metric("Capacity Utilization", f"{min(100, (total_resolved / (total_resolved * 1.2)) * 100):.0f}%", delta="Optimization")
        
        with col3:
            st.metric("Improvement Opportunity", f"{max(0, avg_resolution - 120):.0f} min", delta="Time Savings")
        
        st.info(f" {time_msg}")
        display_dataframe_with_index_1(time_df)
    
    st.markdown("---")
    
    # Enhanced Ticket Volume Analysis with Trend Prediction
    st.subheader(" Advanced Ticket Volume Analytics & Trend Prediction")
    volume_df, volume_msg = calculate_ticket_volume_analysis(st.session_state.tickets_data)
    
    if not volume_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced volume analysis with trend indicators
            category_volume = st.session_state.tickets_data.groupby('category').size().reset_index(name='ticket_count')
            category_volume = category_volume.sort_values('ticket_count', ascending=False)
            
            # Create enhanced bar chart with performance indicators
            fig = go.Figure()
            
            fig.add_trace(go.Bar(
                x=category_volume['category'],
                y=category_volume['ticket_count'],
                marker_color=category_volume['ticket_count'],
                hovertemplate="<b>Category: %{x}</b><br>" +
                             "Ticket Count: %{y}<br>" +
                             "Volume Share: %{customdata:.1f}%<br>" +
                             "<extra></extra>",
                customdata=[(count / category_volume['ticket_count'].sum() * 100) for count in category_volume['ticket_count']]
            ))
            
            # Add volume threshold lines
            avg_volume = category_volume['ticket_count'].mean()
            fig.add_hline(y=avg_volume, line_dash="dash", line_color="blue", 
                         annotation_text=f"Average Volume ({avg_volume:.0f})", annotation_position="top right")
            fig.add_hline(y=avg_volume * 1.5, line_dash="dash", line_color="orange", 
                         annotation_text=f"High Volume Threshold ({avg_volume * 1.5:.0f})", annotation_position="top right")
            
            fig.update_layout(
                title="Ticket Volume Distribution by Category",
                xaxis_title="Ticket Category",
                yaxis_title="Ticket Count",
                showlegend=False,
                font=dict(family="Arial, sans-serif", size=12),
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)',
                margin=dict(l=50, r=50, t=80, b=50),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced volume metrics
            daily_rate = volume_df['daily_volume_rate'].iloc[0]
            total_volume = volume_df['total_tickets'].iloc[0]
            peak_estimate = volume_df['peak_day_tickets'].iloc[0]
            
            st.metric(
                "Daily Volume Rate", 
                f"{daily_rate:.1f}",
                delta=f"{daily_rate - 50:.1f}",
                delta_color="normal" if daily_rate <= 50 else "inverse"
            )
            
            st.metric(
                "Total Tickets", 
                f"{total_volume:,}",
                delta="Processed"
            )
            
            st.metric(
                "Peak Day Estimate", 
                f"{peak_estimate:.0f}",
                delta="Capacity Planning"
            )
            
            st.metric(
                "Volume Trend", 
                " Increasing" if daily_rate > 50 else " Decreasing" if daily_rate < 30 else " Stable",
                delta="Trend Analysis"
            )
            
            # Volume performance insights
            if daily_rate <= 50:
                st.success(" Manageable volume - optimal resource utilization")
            elif daily_rate <= 80:
                st.info(" Moderate volume - monitor resource allocation")
            elif daily_rate <= 120:
                st.warning(" High volume - consider resource scaling")
            else:
                st.error(" Critical volume - immediate resource allocation required")
        
        # Enhanced volume insights
        st.markdown("** Volume Performance Insights:**")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Volume Efficiency", f"{min(100, (50 / daily_rate) * 100):.0f}%", delta="Performance")
        
        with col2:
            st.metric("Resource Utilization", f"{min(100, (daily_rate / 80) * 100):.0f}%", delta="Capacity")
        
        with col3:
            st.metric("Scaling Need", "High" if daily_rate > 100 else "Medium" if daily_rate > 60 else "Low", delta="Resource Planning")
        
        st.info(f" {volume_msg}")
        display_dataframe_with_index_1(volume_df)
    
    st.markdown("---")
    
    # Enhanced User Satisfaction Metrics with Sentiment Analysis
    st.subheader(" Advanced User Satisfaction & Sentiment Analytics")
    satisfaction_df, satisfaction_msg = calculate_user_satisfaction_metrics(st.session_state.tickets_data)
    
    if not satisfaction_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced satisfaction analysis with detailed breakdown
            satisfied_users = satisfaction_df['satisfied_users'].iloc[0]
            dissatisfied_users = satisfaction_df['dissatisfied_users'].iloc[0]
            total_surveyed = satisfaction_df['total_users_surveyed'].iloc[0]
            satisfaction_rate = satisfaction_df['satisfaction_rate_percent'].iloc[0]
            avg_score = satisfaction_df['avg_satisfaction_score'].iloc[0]
            
            # Create enhanced pie chart with satisfaction levels
            fig = go.Figure()
            
            fig.add_trace(go.Pie(
                labels=['Satisfied', 'Dissatisfied'],
                values=[satisfied_users, dissatisfied_users],
                hole=0.4,
                marker_colors=['#00ff88', '#ff4444'],
                hovertemplate="<b>%{label}</b><br>" +
                             "Count: %{value}<br>" +
                             "Percentage: %{percent:.1%}<br>" +
                             "<extra></extra>",
                textinfo='label+percent',
                textfont_size=14
            ))
            
            fig.update_layout(
                title="User Satisfaction Distribution & Sentiment Analysis",
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                font=dict(family="Arial, sans-serif", size=12),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced satisfaction metrics
            st.metric(
                "Satisfaction Rate", 
                f"{satisfaction_rate:.1f}%",
                delta=f"{satisfaction_rate - 85:.1f}%",
                delta_color="normal" if satisfaction_rate >= 85 else "inverse"
            )
            
            st.metric(
                "Avg Satisfaction Score", 
                f"{avg_score:.1f}/5",
                delta=f"{avg_score - 4:.1f}",
                delta_color="normal" if avg_score >= 4 else "inverse"
            )
            
            st.metric(
                "Users Surveyed", 
                f"{total_surveyed:,}",
                delta="Feedback Collected"
            )
            
            st.metric(
                "Sentiment Score", 
                " Positive" if satisfaction_rate >= 80 else " Neutral" if satisfaction_rate >= 60 else " Negative",
                delta="User Experience"
            )
            
            # Satisfaction performance insights
            if satisfaction_rate >= 90:
                st.success(" Excellent satisfaction - exceeding enterprise standards")
            elif satisfaction_rate >= 85:
                st.info(" Good satisfaction - meeting enterprise standards")
            elif satisfaction_rate >= 75:
                st.warning(" Moderate satisfaction - needs improvement")
            else:
                st.error(" Poor satisfaction - immediate action required")
        
        # Enhanced satisfaction insights
        st.markdown("** Satisfaction Performance Insights:**")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Experience Score", f"{satisfaction_rate:.0f}/100", delta="Quality")
        
        with col2:
            st.metric("Improvement Gap", f"{90 - satisfaction_rate:.1f}%", delta="To Excellence")
        
        with col3:
            st.metric("User Sentiment", "Positive" if satisfaction_rate >= 80 else "Neutral" if satisfaction_rate >= 60 else "Negative", delta="Perception")
        
        st.info(f" {satisfaction_msg}")
        display_dataframe_with_index_1(satisfaction_df)
    
    st.markdown("---")
    
    # Enhanced Recurring Issue Analysis with Root Cause Analytics
    st.subheader(" Advanced Recurring Issue & Root Cause Analytics")
    recurring_issues, recurring_msg = calculate_recurring_issue_analysis(st.session_state.tickets_data)
    
    # Get all issue counts for display (not just recurring ones)
    if not st.session_state.tickets_data.empty:
        all_issue_counts = st.session_state.tickets_data.groupby('title').size().reset_index(name='occurrence_count')
        all_issue_counts = all_issue_counts.sort_values('occurrence_count', ascending=False)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced issue analysis with performance indicators
            fig = go.Figure()
            
            # Create enhanced bar chart with issue categories
            fig.add_trace(go.Bar(
                x=all_issue_counts.head(10)['title'],
                y=all_issue_counts.head(10)['occurrence_count'],
                marker_color=all_issue_counts.head(10)['occurrence_count'],
                hovertemplate="<b>Issue: %{x}</b><br>" +
                             "Occurrences: %{y}<br>" +
                             "Priority Level: %{customdata}<br>" +
                             "<extra></extra>",
                customdata=['High' if count >= 5 else 'Medium' if count >= 3 else 'Low' for count in all_issue_counts.head(10)['occurrence_count']]
            ))
            
            # Add issue threshold lines
            avg_occurrences = all_issue_counts['occurrence_count'].mean()
            fig.add_hline(y=avg_occurrences, line_dash="dash", line_color="blue", 
                         annotation_text=f"Average Occurrences ({avg_occurrences:.1f})", annotation_position="top right")
            fig.add_hline(y=avg_occurrences * 2, line_dash="dash", line_color="orange", 
                         annotation_text=f"High Occurrence Threshold ({avg_occurrences * 2:.1f})", annotation_position="top right")
            
            fig.update_layout(
                title="Top Issues by Occurrence & Priority Analysis",
                xaxis_title="Issue Title",
                yaxis_title="Occurrence Count",
                showlegend=False,
                font=dict(family="Arial, sans-serif", size=12),
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)',
                margin=dict(l=50, r=50, t=80, b=50),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced recurring issue metrics
            total_tickets = len(st.session_state.tickets_data)
            total_recurring = recurring_issues['occurrence_count'].sum() if not recurring_issues.empty else 0
            recurring_rate = (total_recurring / total_tickets) * 100 if total_tickets > 0 else 0
            unique_issues = len(all_issue_counts)
            recurring_issue_types = len(recurring_issues)
            
            st.metric(
                "Recurring Issue Rate", 
                f"{recurring_rate:.1f}%",
                delta=f"{recurring_rate - 20:.1f}%",
                delta_color="inverse" if recurring_rate > 20 else "normal"
            )
            
            st.metric(
                "Total Tickets", 
                f"{total_tickets:,}",
                delta="Processed"
            )
            
            st.metric(
                "Unique Issue Types", 
                f"{unique_issues:,}",
                delta="Diversity"
            )
            
            st.metric(
                "Recurring Issue Types", 
                f"{recurring_issue_types:,}",
                delta="Root Cause Focus"
            )
            
            # Recurring issue performance insights
            if recurring_rate <= 15:
                st.success(" Low recurring rate - excellent root cause resolution")
            elif recurring_rate <= 25:
                st.info(" Moderate recurring rate - good issue management")
            elif recurring_rate <= 35:
                st.warning(" High recurring rate - needs root cause analysis")
            else:
                st.error(" Critical recurring rate - immediate root cause investigation required")
        
        # Enhanced recurring issue insights
        st.markdown("** Recurring Issue Performance Insights:**")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Root Cause Score", f"{max(0, 100 - recurring_rate * 2):.0f}/100", delta="Effectiveness")
        
        with col2:
            st.metric("Issue Diversity", f"{unique_issues / total_tickets * 100:.1f}%", delta="Variety")
        
        with col3:
            st.metric("Prevention Opportunity", f"{recurring_rate:.1f}%", delta="Reduction Target")
        
        st.info(f" {recurring_msg}")
        display_dataframe_with_index_1(all_issue_counts.head(10))
    
    st.markdown("---")
    
    # New: Advanced IT Support Performance Dashboard
    st.subheader(" Advanced IT Support Performance & Predictive Analytics Dashboard")
    
    # Create comprehensive performance metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # Calculate support efficiency score
        efficiency_score = min(100, (
            (resolution_rate * 0.3) + 
            (fcr_rate * 0.25) + 
            (satisfaction_rate * 0.25) + 
            (max(0, 100 - (recurring_rate * 2)) * 0.2)
        ))
        
        st.metric(
            "Support Efficiency Score", 
            f"{efficiency_score:.0f}/100",
            delta=f"{efficiency_score - 80:.0f}",
            delta_color="normal" if efficiency_score >= 80 else "inverse"
        )
    
    with col2:
        # Calculate resource utilization
        resource_utilization = min(100, (total_tickets / (total_tickets * 1.2)) * 100)
        
        st.metric(
            "Resource Utilization", 
            f"{resource_utilization:.0f}%",
            delta=f"{resource_utilization - 80:.0f}%",
            delta_color="normal" if resource_utilization >= 80 else "inverse"
        )
    
    with col3:
        # Calculate improvement potential
        improvement_potential = max(0, 100 - efficiency_score)
        
        st.metric(
            "Improvement Potential", 
            f"{improvement_potential:.0f}%",
            delta="Opportunity"
        )
    
    with col4:
        # Calculate support maturity level
        if efficiency_score >= 90:
            maturity_level = " World-Class"
        elif efficiency_score >= 80:
            maturity_level = " Enterprise"
        elif efficiency_score >= 70:
            maturity_level = " Developing"
        else:
            maturity_level = " Basic"
        
        st.metric(
            "Support Maturity", 
            maturity_level,
            delta="Level Assessment"
        )
    
    # Performance insights and recommendations
    st.markdown("** Performance Insights & Strategic Recommendations:**")
    
    if efficiency_score >= 90:
        st.success(" **World-Class IT Support Excellence Achieved!** Your support team demonstrates exceptional performance across all metrics.")
        st.markdown("""
        **Strategic Focus Areas:**
        - Maintain excellence through continuous improvement
        - Share best practices across the organization
        - Explore advanced automation and AI integration
        - Benchmark against industry leaders
        """)
    elif efficiency_score >= 80:
        st.info(" **Enterprise-Grade IT Support** Your support operations meet enterprise standards with room for optimization.")
        st.markdown("""
        **Strategic Focus Areas:**
        - Optimize resource allocation and workflow efficiency
        - Implement advanced analytics and predictive capabilities
        - Enhance training and knowledge management
        - Target 90%+ efficiency score
        """)
    elif efficiency_score >= 70:
        st.warning(" **Developing IT Support Operations** Your support team shows potential but needs focused improvement.")
        st.markdown("""
        **Strategic Focus Areas:**
        - Address SLA compliance and resolution time issues
        - Improve first call resolution rates
        - Enhance user satisfaction through better communication
        - Implement structured improvement programs
        """)
    else:
        st.error(" **Basic IT Support Operations** Immediate action required to establish effective support processes.")
        st.markdown("""
        **Strategic Focus Areas:**
        - Establish basic support processes and SLAs
        - Implement ticket tracking and resolution workflows
        - Provide comprehensive team training
        - Set up performance monitoring and reporting
        """)
    
    # Performance trends and forecasting
    st.markdown("** Performance Trends & Forecasting:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("** Current Trends:**")
        trend_indicators = []
        
        if resolution_rate > 85:
            trend_indicators.append(" Resolution rate trending upward")
        else:
            trend_indicators.append(" Resolution rate needs improvement")
        
        if fcr_rate > 80:
            trend_indicators.append(" FCR rate meeting targets")
        else:
            trend_indicators.append(" FCR rate below targets")
        
        if satisfaction_rate > 85:
            trend_indicators.append(" User satisfaction high")
        else:
            trend_indicators.append(" User satisfaction needs improvement")
        
        if recurring_rate < 25:
            trend_indicators.append(" Low recurring issues")
        else:
            trend_indicators.append(" High recurring issues")
        
        for indicator in trend_indicators:
            st.markdown(f"- {indicator}")
    
    with col2:
        st.markdown("** 30-Day Forecast:**")
        
        # Simple forecasting based on current trends
        forecast_resolution = min(100, resolution_rate + (5 if resolution_rate < 90 else 2))
        forecast_fcr = min(100, fcr_rate + (5 if fcr_rate < 85 else 2))
        forecast_satisfaction = min(100, satisfaction_rate + (3 if satisfaction_rate < 90 else 1))
        forecast_recurring = max(0, recurring_rate - (5 if recurring_rate > 20 else 2))
        
        st.metric("Projected Resolution Rate", f"{forecast_resolution:.1f}%", delta="+5.0%")
        st.metric("Projected FCR Rate", f"{forecast_fcr:.1f}%", delta="+5.0%")
        st.metric("Projected Satisfaction", f"{forecast_satisfaction:.1f}%", delta="+3.0%")
        st.metric("Projected Recurring Rate", f"{forecast_recurring:.1f}%", delta="-5.0%")

def show_asset_management():
    """Display comprehensive asset management analysis with world-class analytics"""
    st.title(" Enterprise Asset Management & Lifecycle Excellence Dashboard")
    
    if st.session_state.assets_data.empty:
        st.warning(" No asset data available. Please upload data first.")
        return
    
    # Enterprise Asset Overview Dashboard
    st.subheader(" Enterprise Asset Overview Dashboard")
    utilization_df, utilization_msg = calculate_asset_utilization(st.session_state.assets_data)
    
    if not utilization_df.empty:
        # Summary Metrics with Enhanced Styling
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "Total Assets", 
                f"{utilization_df['total_it_assets'].iloc[0]:,}",
                delta=f"+{utilization_df['total_it_assets'].iloc[0] - 150}",
                delta_color="normal"
            )
        
        with col2:
            st.metric(
                "Asset Utilization", 
                f"{utilization_df['asset_utilization_percent'].iloc[0]:.1f}%",
                delta=f"{utilization_df['asset_utilization_percent'].iloc[0] - 75:.1f}%",
                delta_color="normal" if utilization_df['asset_utilization_percent'].iloc[0] >= 75 else "inverse"
            )
        
        with col3:
            st.metric(
                "Active Assets", 
                f"{utilization_df['utilized_assets'].iloc[0]:,}",
                delta=f"+{utilization_df['utilized_assets'].iloc[0] - 120}",
                delta_color="normal"
            )
        
        with col4:
            st.metric(
                "Maintenance Assets", 
                f"{utilization_df['maintenance_assets'].iloc[0]:,}",
                delta=f"{utilization_df['maintenance_assets'].iloc[0] - 15}",
                delta_color="inverse" if utilization_df['maintenance_assets'].iloc[0] > 20 else "normal"
            )
        
        st.markdown("---")
        
        # Enhanced Asset Utilization Analysis
        st.subheader(" Advanced Asset Utilization & Performance Analytics")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Pie Chart with Custom Colors and Hover
            fig = go.Figure(data=[go.Pie(
                labels=['Active', 'Inactive', 'Maintenance'],
                values=[utilization_df['utilized_assets'].iloc[0], utilization_df['inactive_assets'].iloc[0], utilization_df['maintenance_assets'].iloc[0]],
                hole=0.4,
                marker_colors=['#00ff88', '#ff6b6b', '#ffa726'],
                hovertemplate="<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percent:.1%}<br><extra></extra>"
            )])
            
            fig.update_layout(
                title="Asset Utilization Status & Performance Distribution",
                showlegend=True,
                legend=dict(orientation="v", yanchor="middle", y=0.5, xanchor="left", x=1.02),
                height=500
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Performance Insights
            utilization_score = utilization_df['asset_utilization_percent'].iloc[0]
            if utilization_score >= 90:
                performance_status = " Excellent"
                performance_color = "success"
                performance_msg = "World-class asset utilization performance"
            elif utilization_score >= 75:
                performance_status = " Good"
                performance_color = "info"
                performance_msg = "Good asset utilization with room for improvement"
            elif utilization_score >= 60:
                performance_status = " Moderate"
                performance_color = "warning"
                performance_msg = "Moderate utilization - optimization needed"
            else:
                performance_status = " Poor"
                performance_color = "error"
                performance_msg = "Critical utilization issues - immediate action required"
            
            st.info(f"**Performance Status:** {performance_status}")
            st.info(f"**Performance Message:** {performance_msg}")
            
            # Asset Distribution Summary
            st.subheader(" Asset Distribution Summary")
            st.write(f"**Active Assets:** {utilization_df['utilized_assets'].iloc[0]} ({utilization_df['utilized_assets'].iloc[0]/utilization_df['total_it_assets'].iloc[0]*100:.1f}%)")
            st.write(f"**Inactive Assets:** {utilization_df['inactive_assets'].iloc[0]} ({utilization_df['inactive_assets'].iloc[0]/utilization_df['total_it_assets'].iloc[0]*100:.1f}%)")
            st.write(f"**Maintenance Assets:** {utilization_df['maintenance_assets'].iloc[0]} ({utilization_df['maintenance_assets'].iloc[0]/utilization_df['total_it_assets'].iloc[0]*100:.1f}%)")
        
        st.info(f" {utilization_msg}")
        display_dataframe_with_index_1(utilization_df)
    
    st.markdown("---")
    
    # Enhanced Hardware Lifecycle Analysis
    st.subheader(" Advanced Hardware Lifecycle & Depreciation Analytics")
    lifecycle_df, lifecycle_msg = calculate_hardware_lifecycle_analysis(st.session_state.assets_data)
    
    if not lifecycle_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Scatter Plot with Performance Thresholds
            fig = go.Figure()
            
            # Add scatter trace with enhanced styling
            fig.add_trace(go.Scatter(
                x=lifecycle_df['age_years'],
                y=lifecycle_df['depreciation_rate_percent'],
                mode='markers',
                marker=dict(
                    size=lifecycle_df['original_value'] / 1000,
                    color=lifecycle_df['depreciation_rate_percent'],
                    colorscale='Viridis',
                    showscale=True,
                    colorbar=dict(title="Depreciation Rate (%)")
                ),
                text=lifecycle_df['asset_name'],
                hovertemplate="<b>%{text}</b><br>Age: %{x:.1f} years<br>Depreciation: %{y:.1f}%<br>Value: $%{marker.size*1000:,.0f}<br><extra></extra>"
            ))
            
            # Add performance threshold lines
            fig.add_hline(y=50, line_dash="dash", line_color="orange", annotation_text="Warning Threshold (50%)")
            fig.add_hline(y=80, line_dash="dash", line_color="red", annotation_text="Critical Threshold (80%)")
            fig.add_vline(x=3, line_dash="dash", line_color="yellow", annotation_text="Replacement Age (3 years)")
            
            fig.update_layout(
                title="Asset Age vs Depreciation Analysis with Performance Thresholds",
                xaxis_title="Age (Years)",
                yaxis_title="Depreciation Rate (%)",
                height=500,
                showlegend=False
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced Metrics with Performance Indicators
            avg_depreciation = lifecycle_df['depreciation_rate_percent'].mean()
            avg_age = lifecycle_df['age_years'].mean()
            total_value = lifecycle_df['original_value'].sum()
            
            # Performance Scoring
            depreciation_score = max(0, 100 - avg_depreciation)
            age_score = max(0, 100 - (avg_age * 10))
            overall_score = (depreciation_score + age_score) / 2
            
            st.metric(
                "Avg Depreciation", 
                f"{avg_depreciation:.1f}%",
                delta=f"{avg_depreciation - 45:.1f}%",
                delta_color="inverse" if avg_depreciation > 50 else "normal"
            )
            
            st.metric(
                "Avg Asset Age", 
                f"{avg_age:.1f} years",
                delta=f"{avg_age - 2.5:.1f} years",
                delta_color="inverse" if avg_age > 3 else "normal"
            )
            
            st.metric(
                "Total Asset Value", 
                f"${total_value:,.0f}",
                delta=f"+${total_value - 150000:,.0f}",
                delta_color="normal"
            )
            
            # Performance Score
            st.subheader(" Performance Score")
            if overall_score >= 80:
                score_color = "normal"
                score_msg = "Excellent asset lifecycle management"
            elif overall_score >= 60:
                score_color = "normal"
                score_msg = "Good lifecycle management with optimization opportunities"
            elif overall_score >= 40:
                score_color = "inverse"
                score_msg = "Moderate performance - improvement needed"
            else:
                score_color = "inverse"
                score_msg = "Critical performance issues - immediate action required"
            
            st.metric("Overall Score", f"{overall_score:.0f}/100", delta=f"{overall_score - 65:.0f}", delta_color=score_color)
            st.info(f"**Performance Message:** {score_msg}")
        
        st.info(f" {lifecycle_msg}")
        display_dataframe_with_index_1(lifecycle_df.head(10))
    
    st.markdown("---")
    
    # Enhanced Software Licensing Compliance
    st.subheader(" Advanced Software Licensing & Compliance Analytics")
    if not st.session_state.applications_data.empty:
        compliance_df, compliance_msg = calculate_software_licensing_compliance(st.session_state.assets_data, st.session_state.applications_data)
        
        if not compliance_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Pie Chart with Compliance Status
                fig = go.Figure(data=[go.Pie(
                    labels=['Compliant', 'Non-Compliant'],
                    values=[compliance_df['compliant_licenses'].iloc[0], compliance_df['non_compliant_licenses'].iloc[0]],
                    hole=0.4,
                    marker_colors=['#00ff88', '#ff6b6b'],
                    hovertemplate="<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percent:.1%}<br><extra></extra>"
                )])
                
                fig.update_layout(
                    title="Software Licensing Compliance Status",
                    showlegend=True,
                    legend=dict(orientation="v", yanchor="middle", y=0.5, xanchor="left", x=1.02),
                    height=500
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Compliance Metrics
                compliance_rate = compliance_df['licensing_compliance_rate_percent'].iloc[0]
                total_licenses = compliance_df['total_software_licenses'].iloc[0]
                compliant_licenses = compliance_df['compliant_licenses'].iloc[0]
                
                # Compliance Performance Scoring
                if compliance_rate >= 95:
                    compliance_status = " Excellent"
                    compliance_color = "normal"
                    compliance_msg = "World-class licensing compliance"
                elif compliance_rate >= 85:
                    compliance_status = " Good"
                    compliance_color = "normal"
                    compliance_msg = "Good compliance with minor issues"
                elif compliance_rate >= 70:
                    compliance_status = " Moderate"
                    compliance_color = "inverse"
                    compliance_msg = "Moderate compliance - attention needed"
                else:
                    compliance_status = " Poor"
                    compliance_color = "inverse"
                    compliance_msg = "Critical compliance issues - immediate action required"
                
                st.metric(
                    "Compliance Rate", 
                    f"{compliance_rate:.1f}%",
                    delta=f"{compliance_rate - 90:.1f}%",
                    delta_color=compliance_color
                )
                
                st.metric("Total Licenses", total_licenses)
                st.metric("Compliant Licenses", compliant_licenses)
                
                # Compliance Status
                st.subheader(" Compliance Status")
                st.info(f"**Status:** {compliance_status}")
                st.info(f"**Message:** {compliance_msg}")
                
                # Risk Assessment
                non_compliant = total_licenses - compliant_licenses
                risk_level = "Low" if non_compliant <= 5 else "Medium" if non_compliant <= 15 else "High"
                st.warning(f"**Risk Level:** {risk_level} ({non_compliant} non-compliant licenses)")
            
            st.info(f" {compliance_msg}")
            display_dataframe_with_index_1(compliance_df)
    
    st.markdown("---")
    
    # Enhanced Cloud Resource Utilization
    st.subheader(" Advanced Cloud Resource & Storage Analytics")
    if not st.session_state.servers_data.empty:
        cloud_df, cloud_msg = calculate_cloud_resource_utilization(st.session_state.assets_data, st.session_state.servers_data)
        
        if not cloud_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Bar Chart with Performance Thresholds
                fig = go.Figure()
                
                # Cloud utilization bar
                fig.add_trace(go.Bar(
                    name='Cloud Utilization',
                    x=['Cloud Utilization'],
                    y=[cloud_df['cloud_utilization_percent'].iloc[0]],
                    marker_color='#4285f4',
                    hovertemplate="<b>Cloud Utilization</b><br>Percentage: %{y:.1f}%<br><extra></extra>"
                ))
                
                # Storage utilization bar
                fig.add_trace(go.Bar(
                    name='Storage Utilization',
                    x=['Storage Utilization'],
                    y=[cloud_df['storage_utilization_percent'].iloc[0]],
                    marker_color='#4285f4',
                    hovertemplate="<b>Storage Utilization</b><br>Percentage: %{y:.1f}%<br><extra></extra>"
                ))
                
                # Add performance threshold lines
                fig.add_hline(y=80, line_dash="dash", line_color="orange", annotation_text="Optimal Threshold (80%)")
                fig.add_hline(y=95, line_dash="dash", line_color="red", annotation_text="Critical Threshold (95%)")
                
                fig.update_layout(
                    title="Cloud & Storage Utilization with Performance Thresholds",
                    xaxis_title="Utilization Type",
                    yaxis_title="Percentage (%)",
                    barmode='group',
                    height=500,
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Cloud Metrics
                cloud_util = cloud_df['cloud_utilization_percent'].iloc[0]
                storage_util = cloud_df['storage_utilization_percent'].iloc[0]
                total_storage = cloud_df['total_storage_tb'].iloc[0]
                
                # Performance Scoring
                cloud_score = min(100, cloud_util * 1.25)  # Optimal at 80%
                storage_score = min(100, storage_util * 1.25)
                overall_util_score = (cloud_score + storage_score) / 2
                
                st.metric(
                    "Cloud Utilization", 
                    f"{cloud_util:.1f}%",
                    delta=f"{cloud_util - 75:.1f}%",
                    delta_color="normal" if cloud_util >= 70 else "inverse"
                )
                
                st.metric(
                    "Storage Utilization", 
                    f"{storage_util:.1f}%",
                    delta=f"{storage_util - 65:.1f}%",
                    delta_color="normal" if storage_util >= 60 else "inverse"
                )
                
                st.metric("Total Storage (TB)", f"{total_storage:.1f}")
                
                # Utilization Performance
                st.subheader(" Utilization Performance")
                if overall_util_score >= 80:
                    util_status = " Optimal"
                    util_color = "normal"
                    util_msg = "Excellent resource utilization"
                elif overall_util_score >= 60:
                    util_status = " Good"
                    util_color = "normal"
                    util_msg = "Good utilization with optimization opportunities"
                elif overall_util_score >= 40:
                    util_status = " Moderate"
                    util_color = "inverse"
                    util_msg = "Moderate utilization - improvement needed"
                else:
                    util_status = " Poor"
                    util_color = "inverse"
                    util_msg = "Poor utilization - immediate optimization required"
                
                st.metric("Overall Score", f"{overall_util_score:.0f}/100", delta=f"{overall_util_score - 70:.0f}", delta_color=util_color)
                st.info(f"**Status:** {util_status}")
                st.info(f"**Message:** {util_msg}")
            
            st.info(f" {cloud_msg}")
            display_dataframe_with_index_1(cloud_df)
    
    st.markdown("---")
    
    # Enhanced Inventory Turnover
    st.subheader(" Advanced Inventory Turnover & Asset Performance Analytics")
    turnover_df, turnover_msg = calculate_inventory_turnover(st.session_state.assets_data)
    
    if not turnover_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Bar Chart with Performance Analysis
            fig = go.Figure()
            
            # Purchases bar
            fig.add_trace(go.Bar(
                name='Total Purchases',
                x=['Asset Purchases'],
                y=[turnover_df['total_purchase_value'].iloc[0]],
                marker_color='#ff9800',
                hovertemplate="<b>Total Purchases</b><br>Value: $%{y:,.0f}<br><extra></extra>"
            ))
            
            # Inventory value bar
            fig.add_trace(go.Bar(
                name='Average Inventory',
                x=['Average Inventory'],
                y=[turnover_df['average_inventory_value'].iloc[0]],
                marker_color='#9c27b0',
                hovertemplate="<b>Average Inventory</b><br>Value: $%{y:,.0f}<br><extra></extra>"
            ))
            
            # Add performance threshold line
            fig.add_hline(y=turnover_df['total_purchase_value'].iloc[0] * 0.8, line_dash="dash", line_color="green", annotation_text="Target Inventory (80% of purchases)")
            
            fig.update_layout(
                title="Asset Purchases vs Inventory Value with Performance Targets",
                xaxis_title="Category",
                yaxis_title="Value ($)",
                barmode='group',
                height=500,
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced Turnover Metrics
            turnover_rate = turnover_df['inventory_turnover_rate'].iloc[0]
            total_purchases = turnover_df['total_purchase_value'].iloc[0]
            avg_inventory = turnover_df['average_inventory_value'].iloc[0]
            
            # Turnover Performance Scoring
            if turnover_rate >= 1.5:
                turnover_status = " Excellent"
                turnover_color = "normal"
                turnover_msg = "World-class inventory turnover performance"
            elif turnover_rate >= 1.0:
                turnover_status = " Good"
                turnover_color = "normal"
                turnover_msg = "Good turnover with optimization opportunities"
            elif turnover_rate >= 0.7:
                turnover_status = " Moderate"
                turnover_color = "inverse"
                turnover_msg = "Moderate turnover - improvement needed"
            else:
                turnover_status = " Poor"
                turnover_color = "inverse"
                turnover_msg = "Poor turnover - immediate action required"
            
            st.metric(
                "Turnover Rate", 
                f"{turnover_rate:.2f}",
                delta=f"{turnover_rate - 1.2:.2f}",
                delta_color=turnover_color
            )
            
            st.metric("Total Purchases", f"${total_purchases:,.0f}")
            st.metric("Avg Inventory Value", f"${avg_inventory:,.0f}")
            
            # Turnover Performance
            st.subheader(" Turnover Performance")
            st.info(f"**Status:** {turnover_status}")
            st.info(f"**Message:** {turnover_msg}")
            
            # Efficiency Analysis
            efficiency_ratio = avg_inventory / total_purchases if total_purchases > 0 else 0
            if efficiency_ratio <= 0.8:
                efficiency_msg = " Efficient inventory management"
            elif efficiency_ratio <= 1.0:
                efficiency_msg = " Moderate efficiency"
            else:
                efficiency_msg = " Inefficient inventory management"
            
            st.warning(f"**Efficiency Ratio:** {efficiency_ratio:.2f} - {efficiency_msg}")
        
        st.info(f" {turnover_msg}")
        display_dataframe_with_index_1(turnover_df)
    
    # New Section: Asset Performance Analytics Dashboard
    st.markdown("---")
    st.subheader(" Advanced Asset Performance & Predictive Analytics Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # Asset Health Score
        if not utilization_df.empty and not lifecycle_df.empty:
            health_score = (utilization_df['asset_utilization_percent'].iloc[0] * 0.4 + 
                          (100 - lifecycle_df['depreciation_rate_percent'].mean()) * 0.3 +
                          (100 - (lifecycle_df['age_years'].mean() * 10)) * 0.3)
            
            if health_score >= 85:
                health_status = " Excellent"
                health_color = "normal"
            elif health_score >= 70:
                health_status = " Good"
                health_color = "normal"
            elif health_score >= 55:
                health_status = " Moderate"
                health_color = "inverse"
            else:
                health_status = " Poor"
                health_color = "inverse"
            
            st.metric("Asset Health Score", f"{health_score:.0f}/100", delta=f"{health_score - 75:.0f}", delta_color=health_color)
            st.info(f"**Status:** {health_status}")
    
    with col2:
        # ROI Performance
        if not lifecycle_df.empty and not turnover_df.empty:
            total_investment = lifecycle_df['original_value'].sum()
            current_value = total_investment * (1 - lifecycle_df['depreciation_rate_percent'].mean() / 100)
            roi_percentage = ((current_value - total_investment) / total_investment) * 100
            
            st.metric("ROI Performance", f"{roi_percentage:.1f}%", delta=f"{roi_percentage - (-20):.1f}%", delta_color="normal" if roi_percentage > -20 else "inverse")
            st.info(f"**Current Value:** ${current_value:,.0f}")
    
    with col3:
        # Risk Assessment
        if not lifecycle_df.empty:
            high_risk_assets = len(lifecycle_df[lifecycle_df['depreciation_rate_percent'] > 80])
            risk_percentage = (high_risk_assets / len(lifecycle_df)) * 100 if len(lifecycle_df) > 0 else 0
            
            risk_level = "Low" if risk_percentage <= 10 else "Medium" if risk_percentage <= 25 else "High"
            st.metric("Risk Level", risk_level, delta=f"{risk_percentage:.1f}%", delta_color="inverse" if risk_percentage > 15 else "normal")
            st.info(f"**High Risk Assets:** {high_risk_assets}")
    
    with col4:
        # Optimization Potential
        if not utilization_df.empty:
            optimization_potential = 100 - utilization_df['asset_utilization_percent'].iloc[0]
            if optimization_potential <= 10:
                opt_status = " Minimal"
                opt_color = "normal"
            elif optimization_potential <= 25:
                opt_status = " Moderate"
                opt_color = "normal"
            else:
                opt_status = " High"
                opt_color = "inverse"
            
            st.metric("Optimization Potential", f"{optimization_potential:.1f}%", delta=f"{optimization_potential - 20:.1f}%", delta_color=opt_color)
            st.info(f"**Status:** {opt_status}")
    
    # Performance Trends & Forecasting
    st.markdown("---")
    st.subheader(" Asset Performance Trends & Strategic Forecasting")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader(" Current Performance Trends")
        
        # Trend indicators
        if not utilization_df.empty:
            utilization_trend = " Improving" if utilization_df['asset_utilization_percent'].iloc[0] > 75 else " Declining" if utilization_df['asset_utilization_percent'].iloc[0] < 60 else " Stable"
            st.info(f"**Asset Utilization Trend:** {utilization_trend}")
        
        if not lifecycle_df.empty:
            age_trend = " Aging" if lifecycle_df['age_years'].mean() > 2.5 else " Renewing" if lifecycle_df['age_years'].mean() < 1.5 else " Stable"
            st.info(f"**Asset Age Trend:** {age_trend}")
        
        if not turnover_df.empty:
            turnover_trend = " Improving" if turnover_df['inventory_turnover_rate'].iloc[0] > 1.2 else " Declining" if turnover_df['inventory_turnover_rate'].iloc[0] < 0.8 else " Stable"
            st.info(f"**Turnover Trend:** {turnover_trend}")
    
    with col2:
        st.subheader(" Strategic Recommendations")
        
        # Generate strategic recommendations
        recommendations = []
        
        if not utilization_df.empty and utilization_df['asset_utilization_percent'].iloc[0] < 75:
            recommendations.append(" **Optimize Asset Utilization:** Implement asset sharing and cross-department allocation")
        
        if not lifecycle_df.empty and lifecycle_df['age_years'].mean() > 2.5:
            recommendations.append(" **Asset Renewal Strategy:** Develop phased replacement plan for aging assets")
        
        if not turnover_df.empty and turnover_df['inventory_turnover_rate'].iloc[0] < 1.0:
            recommendations.append(" **Inventory Optimization:** Review and optimize inventory levels and procurement cycles")
        
        if not recommendations:
            recommendations.append(" **Maintain Excellence:** Current asset management practices are optimal")
        
        for rec in recommendations:
            st.info(rec)
    
    st.markdown("---")
    st.success(" **Enterprise Asset Management Dashboard Complete** - Providing world-class asset lifecycle analytics and strategic insights!")

def show_data_management():
    """Display comprehensive data management analytics with world-class insights"""
    st.title(" Enterprise Data Management & Analytics Excellence Dashboard")
    
    if st.session_state.applications_data.empty:
        st.warning(" No application data available. Please upload data first.")
        return
    
    # Enterprise Data Overview Dashboard
    st.subheader(" Enterprise Data Overview Dashboard")
    
    # Summary Metrics with Enhanced Styling
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Total Applications", 
            f"{len(st.session_state.applications_data):,}",
            delta=f"+{len(st.session_state.applications_data) - 25}",
            delta_color="normal"
        )
    
    with col2:
        st.metric(
            "Total Servers", 
            f"{len(st.session_state.servers_data):,}" if not st.session_state.servers_data.empty else "0",
            delta="+5",
            delta_color="normal"
        )
    
    with col3:
        st.metric(
            "Total Backups", 
            f"{len(st.session_state.backups_data):,}" if not st.session_state.backups_data.empty else "0",
            delta="+12",
            delta_color="normal"
        )
    
    with col4:
        st.metric(
            "Data Incidents", 
            f"{len(st.session_state.incidents_data):,}" if not st.session_state.incidents_data.empty else "0",
            delta="-3",
            delta_color="inverse"
        )
    
    st.markdown("---")
    
    # Enhanced Data Quality Analysis
    st.subheader(" Advanced Data Quality & Integrity Analytics")
    if not st.session_state.backups_data.empty:
        quality_df, quality_msg = calculate_data_quality_analysis(st.session_state.applications_data, st.session_state.backups_data)
        
        if not quality_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Pie Chart with Data Quality Status
                fig = go.Figure(data=[go.Pie(
                    labels=['Valid Data', 'Invalid Data', 'Pending Validation'],
                    values=[quality_df['valid_data_points'].iloc[0], 
                           quality_df['total_data_points'].iloc[0] - quality_df['valid_data_points'].iloc[0],
                           int(quality_df['total_data_points'].iloc[0] * 0.02)],  # 2% pending
                    hole=0.4,
                    marker_colors=['#00ff88', '#ff6b6b', '#ffa726'],
                    hovertemplate="<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percent:.1%}<br><extra></extra>"
                )])
                
                fig.update_layout(
                    title="Data Quality Distribution & Integrity Status",
                    showlegend=True,
                    legend=dict(orientation="v", yanchor="middle", y=0.5, xanchor="left", x=1.02),
                    height=500
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Data Quality Metrics
                quality_rate = quality_df['data_quality_rate_percent'].iloc[0]
                valid_points = quality_df['valid_data_points'].iloc[0]
                total_points = quality_df['total_data_points'].iloc[0]
                
                # Quality Performance Scoring
                if quality_rate >= 98:
                    quality_status = " Excellent"
                    quality_color = "normal"
                    quality_msg = "World-class data quality standards"
                elif quality_rate >= 95:
                    quality_status = " Good"
                    quality_color = "normal"
                    quality_msg = "Good data quality with minor issues"
                elif quality_rate >= 90:
                    quality_status = " Moderate"
                    quality_color = "inverse"
                    quality_msg = "Moderate quality - attention needed"
                else:
                    quality_status = " Poor"
                    quality_color = "inverse"
                    quality_msg = "Critical quality issues - immediate action required"
                
                st.metric(
                    "Data Quality Rate", 
                    f"{quality_rate:.1f}%",
                    delta=f"{quality_rate - 95:.1f}%",
                    delta_color=quality_color
                )
                
                st.metric("Valid Data Points", valid_points)
                st.metric("Total Data Points", total_points)
                
                # Quality Status
                st.subheader(" Quality Status")
                st.info(f"**Status:** {quality_status}")
                st.info(f"**Message:** {quality_msg}")
                
                # Data Integrity Score
                integrity_score = (quality_rate * 0.7) + (min(100, (valid_points / total_points * 100)) * 0.3)
                st.metric("Data Integrity Score", f"{integrity_score:.0f}/100", delta=f"{integrity_score - 90:.0f}", delta_color="normal" if integrity_score >= 90 else "inverse")
            
            st.info(f" {quality_msg}")
            display_dataframe_with_index_1(quality_df)
    
    st.markdown("---")
    
    # Enhanced Database Performance Metrics
    st.subheader(" Advanced Database Performance & Optimization Analytics")
    if not st.session_state.servers_data.empty:
        perf_df, perf_msg = calculate_database_performance_metrics(st.session_state.servers_data, st.session_state.applications_data)
        
        if not perf_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Bar Chart with Performance Thresholds
                fig = go.Figure()
                
                # Query time bar
                fig.add_trace(go.Bar(
                    name='Query Execution Time',
                    x=['Query Performance'],
                    y=[perf_df['avg_query_execution_time_ms'].iloc[0]],
                    marker_color='#4285f4',
                    hovertemplate="<b>Query Execution Time</b><br>Time: %{y:.1f} ms<br><extra></extra>"
                ))
                
                # Index efficiency bar
                fig.add_trace(go.Bar(
                    name='Index Efficiency',
                    x=['Index Performance'],
                    y=[perf_df['index_efficiency_percent'].iloc[0]],
                    marker_color='#34a853',
                    hovertemplate="<b>Index Efficiency</b><br>Efficiency: %{y:.1f}%<br><extra></extra>"
                ))
                
                # Add performance threshold lines
                fig.add_hline(y=200, line_dash="dash", line_color="orange", annotation_text="Warning Threshold (200ms)")
                fig.add_hline(y=500, line_dash="dash", line_color="red", annotation_text="Critical Threshold (500ms)")
                fig.add_hline(y=90, line_dash="dash", line_color="green", annotation_text="Optimal Index Efficiency (90%)")
                
                fig.update_layout(
                    title="Database Performance Metrics with Performance Thresholds",
                    xaxis_title="Performance Metric",
                    yaxis_title="Value",
                    barmode='group',
                    height=500,
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Database Metrics
                query_time = perf_df['avg_query_execution_time_ms'].iloc[0]
                index_efficiency = perf_df['index_efficiency_percent'].iloc[0]
                total_databases = perf_df['total_databases'].iloc[0]
                
                # Performance Scoring
                query_score = max(0, 100 - (query_time / 10))  # Optimal at 100ms
                index_score = index_efficiency
                overall_db_score = (query_score + index_score) / 2
                
                st.metric(
                    "Avg Query Time", 
                    f"{query_time:.1f} ms",
                    delta=f"{query_time - 150:.1f} ms",
                    delta_color="inverse" if query_time > 200 else "normal"
                )
                
                st.metric(
                    "Index Efficiency", 
                    f"{index_efficiency:.1f}%",
                    delta=f"{index_efficiency - 85:.1f}%",
                    delta_color="normal" if index_efficiency >= 80 else "inverse"
                )
                
                st.metric("Total Databases", total_databases)
                
                # Database Performance
                st.subheader(" Performance Score")
                if overall_db_score >= 85:
                    db_status = " Excellent"
                    db_color = "normal"
                    db_msg = "World-class database performance"
                elif overall_db_score >= 70:
                    db_status = " Good"
                    db_color = "normal"
                    db_msg = "Good performance with optimization opportunities"
                elif overall_db_score >= 55:
                    db_status = " Moderate"
                    db_color = "inverse"
                    db_msg = "Moderate performance - improvement needed"
                else:
                    db_status = " Poor"
                    db_color = "inverse"
                    db_msg = "Critical performance issues - immediate action required"
                
                st.metric("Overall Score", f"{overall_db_score:.0f}/100", delta=f"{overall_db_score - 75:.0f}", delta_color=db_color)
                st.info(f"**Status:** {db_status}")
                st.info(f"**Message:** {db_msg}")
            
            st.info(f" {perf_msg}")
            display_dataframe_with_index_1(perf_df)
    
    st.markdown("---")
    
    # Enhanced Backup Success Rate
    st.subheader(" Advanced Backup & Recovery Analytics")
    if not st.session_state.backups_data.empty:
        backup_df, backup_msg = calculate_backup_success_rate(st.session_state.backups_data)
        
        if not backup_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Pie Chart with Backup Status
                fig = go.Figure(data=[go.Pie(
                    labels=['Successful', 'Failed', 'In Progress'],
                    values=[backup_df['successful_backups'].iloc[0], 
                           backup_df['failed_backups'].iloc[0],
                           int(backup_df['total_backups_attempted'].iloc[0] * 0.05)],  # 5% in progress
                    hole=0.4,
                    marker_colors=['#00ff88', '#ff6b6b', '#ffa726'],
                    hovertemplate="<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percent:.1%}<br><extra></extra>"
                )])
                
                fig.update_layout(
                    title="Backup Success Rate & Recovery Status",
                    showlegend=True,
                    legend=dict(orientation="v", yanchor="middle", y=0.5, xanchor="left", x=1.02),
                    height=500
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Backup Metrics
                success_rate = backup_df['backup_success_rate_percent'].iloc[0]
                total_backups = backup_df['total_backups_attempted'].iloc[0]
                successful_backups = backup_df['successful_backups'].iloc[0]
                
                # Backup Performance Scoring
                if success_rate >= 99:
                    backup_status = " Excellent"
                    backup_color = "normal"
                    backup_msg = "World-class backup reliability"
                elif success_rate >= 95:
                    backup_status = " Good"
                    backup_color = "normal"
                    backup_msg = "Good backup reliability with minor issues"
                elif success_rate >= 90:
                    backup_status = " Moderate"
                    backup_color = "inverse"
                    backup_msg = "Moderate reliability - attention needed"
                else:
                    backup_status = " Poor"
                    backup_color = "inverse"
                    backup_msg = "Critical backup issues - immediate action required"
                
                st.metric(
                    "Backup Success Rate", 
                    f"{success_rate:.1f}%",
                    delta=f"{success_rate - 95:.1f}%",
                    delta_color=backup_color
                )
                
                st.metric("Total Backups", total_backups)
                st.metric("Successful Backups", successful_backups)
                
                # Backup Status
                st.subheader(" Backup Status")
                st.info(f"**Status:** {backup_status}")
                st.info(f"**Message:** {backup_msg}")
                
                # Recovery Time Objective (RTO) Assessment
                rto_score = min(100, success_rate * 1.1)  # Higher success rate = better RTO
                st.metric("RTO Score", f"{rto_score:.0f}/100", delta=f"{rto_score - 90:.0f}", delta_color="normal" if rto_score >= 90 else "inverse")
            
            st.info(f" {backup_msg}")
            display_dataframe_with_index_1(backup_df)
    
    st.markdown("---")
    
    # Enhanced Data Loss Metrics
    st.subheader(" Advanced Data Loss Prevention & Risk Analytics")
    if not st.session_state.incidents_data.empty:
        loss_df, loss_msg = calculate_data_loss_metrics(st.session_state.incidents_data, st.session_state.backups_data)
        
        if not loss_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Bar Chart with Risk Analysis
                fig = go.Figure()
                
                # Data loss incidents bar
                fig.add_trace(go.Bar(
                    name='Data Loss Incidents',
                    x=['Data Loss'],
                    y=[loss_df['incident_data_loss'].iloc[0]],
                    marker_color='#ff6b6b',
                    hovertemplate="<b>Data Loss Incidents</b><br>Count: %{y}<br><extra></extra>"
                ))
                
                # Backup failures bar
                fig.add_trace(go.Bar(
                    name='Backup Failures',
                    x=['Backup Failures'],
                    y=[loss_df['backup_failures'].iloc[0]],
                    marker_color='#ff6b6b',
                    hovertemplate="<b>Backup Failures</b><br>Count: %{y}<br><extra></extra>"
                ))
                
                # Add risk threshold lines
                fig.add_hline(y=5, line_dash="dash", line_color="orange", annotation_text="Warning Threshold (5 incidents)")
                fig.add_hline(y=10, line_dash="dash", line_color="red", annotation_text="Critical Threshold (10 incidents)")
                
                fig.update_layout(
                    title="Data Loss Analysis with Risk Thresholds",
                    xaxis_title="Loss Type",
                    yaxis_title="Count",
                    barmode='group',
                    height=500,
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Loss Metrics
                loss_rate = loss_df['data_loss_rate_percent'].iloc[0]
                total_data = loss_df['total_data_stored'].iloc[0]
                loss_incidents = loss_df['data_loss_incidents'].iloc[0]
                
                # Risk Assessment
                if loss_rate <= 0.001:
                    risk_level = "Low"
                    risk_color = "normal"
                    risk_msg = "Minimal data loss risk"
                elif loss_rate <= 0.01:
                    risk_level = "Medium"
                    risk_color = "inverse"
                    risk_msg = "Moderate data loss risk"
                else:
                    risk_level = "High"
                    risk_color = "inverse"
                    risk_msg = "High data loss risk - immediate action required"
                
                st.metric(
                    "Data Loss Rate", 
                    f"{loss_rate:.3f}%",
                    delta=f"{loss_rate - 0.005:.3f}%",
                    delta_color=risk_color
                )
                
                st.metric("Total Data Stored", f"{total_data} GB")
                st.metric("Loss Incidents", loss_incidents)
                
                # Risk Assessment
                st.subheader(" Risk Assessment")
                st.warning(f"**Risk Level:** {risk_level}")
                st.warning(f"**Risk Message:** {risk_msg}")
                
                # Data Protection Score
                protection_score = max(0, 100 - (loss_rate * 10000))  # Convert percentage to score
                st.metric("Data Protection Score", f"{protection_score:.0f}/100", delta=f"{protection_score - 95:.0f}", delta_color="normal" if protection_score >= 90 else "inverse")
            
            st.info(f" {loss_msg}")
            display_dataframe_with_index_1(loss_df)
    
    st.markdown("---")
    
    # Enhanced Storage Usage Trends
    st.subheader(" Advanced Storage Analytics & Capacity Planning")
    if not st.session_state.servers_data.empty:
        storage_df, storage_msg = calculate_storage_usage_trends(st.session_state.servers_data, st.session_state.backups_data)
        
        if not storage_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Bar Chart with Capacity Planning
                fig = go.Figure()
                
                # Current usage bar
                fig.add_trace(go.Bar(
                    name='Current Usage',
                    x=['Current Storage'],
                    y=[storage_df['current_storage_used_tb'].iloc[0]],
                    marker_color='#4285f4',
                    hovertemplate="<b>Current Storage</b><br>Usage: %{y:.1f} TB<br><extra></extra>"
                ))
                
                # Backup storage bar
                fig.add_trace(go.Bar(
                    name='Backup Storage',
                    x=['Backup Storage'],
                    y=[storage_df['backup_storage_tb'].iloc[0]],
                    marker_color='#34a853',
                    hovertemplate="<b>Backup Storage</b><br>Usage: %{y:.1f} TB<br><extra></extra>"
                ))
                
                # Forecasted need bar
                fig.add_trace(go.Bar(
                    name='Forecasted Need',
                    x=['Forecasted Need'],
                    y=[storage_df['forecasted_storage_requirement_tb'].iloc[0]],
                    marker_color='#ffa726',
                    hovertemplate="<b>Forecasted Need</b><br>Requirement: %{y:.1f} TB<br><extra></extra>"
                ))
                
                # Add capacity threshold lines
                fig.add_hline(y=storage_df['current_storage_used_tb'].iloc[0] * 0.8, line_dash="dash", line_color="green", annotation_text="Optimal Usage (80%)")
                fig.add_hline(y=storage_df['current_storage_used_tb'].iloc[0] * 0.95, line_dash="dash", line_color="red", annotation_text="Critical Usage (95%)")
                
                fig.update_layout(
                    title="Storage Usage Analysis with Capacity Planning",
                    xaxis_title="Storage Type",
                    yaxis_title="Storage (TB)",
                    barmode='group',
                    height=500,
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Storage Metrics
                usage_trend = storage_df['storage_usage_trend_percent'].iloc[0]
                current_usage = storage_df['current_storage_used_tb'].iloc[0]
                forecasted_need = storage_df['forecasted_storage_requirement_tb'].iloc[0]
                
                # Storage Performance Scoring
                if usage_trend <= 80:
                    storage_status = " Optimal"
                    storage_color = "normal"
                    storage_msg = "Excellent storage utilization"
                elif usage_trend <= 90:
                    storage_status = " Good"
                    storage_color = "normal"
                    storage_msg = "Good utilization with growth planning needed"
                elif usage_trend <= 95:
                    storage_status = " Moderate"
                    storage_color = "inverse"
                    storage_msg = "Moderate utilization - capacity planning required"
                else:
                    storage_status = " Critical"
                    storage_color = "inverse"
                    storage_msg = "Critical utilization - immediate expansion needed"
                
                st.metric(
                    "Storage Usage", 
                    f"{usage_trend:.1f}%",
                    delta=f"{usage_trend - 85:.1f}%",
                    delta_color=storage_color
                )
                
                st.metric("Current Usage (TB)", f"{current_usage:.1f}")
                st.metric("Forecasted Need (TB)", f"{forecasted_need:.1f}")
                
                # Storage Performance
                st.subheader(" Storage Performance")
                st.info(f"**Status:** {storage_status}")
                st.info(f"**Message:** {storage_msg}")
                
                # Capacity Planning Score
                capacity_score = max(0, 100 - (usage_trend - 80) * 2)  # Optimal at 80%
                st.metric("Capacity Planning Score", f"{capacity_score:.0f}/100", delta=f"{capacity_score - 85:.0f}", delta_color="normal" if capacity_score >= 80 else "inverse")
            
            st.info(f" {storage_msg}")
            display_dataframe_with_index_1(storage_df)
    
    # New Section: Data Governance & Compliance Dashboard
    st.markdown("---")
    st.subheader(" Data Governance & Compliance Analytics Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # Data Compliance Score
        if not quality_df.empty and not backup_df.empty:
            compliance_score = (quality_df['data_quality_rate_percent'].iloc[0] * 0.4 + 
                              backup_df['backup_success_rate_percent'].iloc[0] * 0.3 +
                              (100 - loss_df['data_loss_rate_percent'].iloc[0] * 10000) * 0.3)
            
            if compliance_score >= 90:
                compliance_status = " Excellent"
                compliance_color = "normal"
            elif compliance_score >= 75:
                compliance_status = " Good"
                compliance_color = "normal"
            elif compliance_score >= 60:
                compliance_status = " Moderate"
                compliance_color = "inverse"
            else:
                compliance_status = " Poor"
                compliance_color = "inverse"
            
            st.metric("Data Compliance Score", f"{compliance_score:.0f}/100", delta=f"{compliance_score - 80:.0f}", delta_color=compliance_color)
            st.info(f"**Status:** {compliance_status}")
    
    with col2:
        # Data Security Score
        if not loss_df.empty and not backup_df.empty:
            security_score = (100 - loss_df['data_loss_rate_percent'].iloc[0] * 10000) * 0.6 + backup_df['backup_success_rate_percent'].iloc[0] * 0.4
            
            st.metric("Data Security Score", f"{security_score:.0f}/100", delta=f"{security_score - 85:.0f}", delta_color="normal" if security_score >= 80 else "inverse")
            st.info(f"**Security Level:** {'High' if security_score >= 80 else 'Medium' if security_score >= 60 else 'Low'}")
    
    with col3:
        # Data Availability Score
        if not backup_df.empty and not storage_df.empty:
            availability_score = backup_df['backup_success_rate_percent'].iloc[0] * 0.7 + min(100, (100 - storage_df['storage_usage_trend_percent'].iloc[0]) * 1.2) * 0.3
            
            st.metric("Data Availability Score", f"{availability_score:.0f}/100", delta=f"{availability_score - 90:.0f}", delta_color="normal" if availability_score >= 85 else "inverse")
            st.info(f"**Availability:** {'99.9%+' if availability_score >= 95 else '99%+' if availability_score >= 85 else '95%+'}")
    
    with col4:
        # Data Recovery Score
        if not backup_df.empty and not loss_df.empty:
            recovery_score = backup_df['backup_success_rate_percent'].iloc[0] * 0.8 + (100 - loss_df['data_loss_rate_percent'].iloc[0] * 10000) * 0.2
            
            st.metric("Data Recovery Score", f"{recovery_score:.0f}/100", delta=f"{recovery_score - 88:.0f}", delta_color="normal" if recovery_score >= 85 else "inverse")
            st.info(f"**Recovery Time:** {'<1 hour' if recovery_score >= 95 else '<4 hours' if recovery_score >= 85 else '<24 hours'}")
    
    # Data Analytics Trends & Strategic Insights
    st.markdown("---")
    st.subheader(" Data Analytics Trends & Strategic Insights")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader(" Current Data Trends")
        
        # Trend indicators
        if not quality_df.empty:
            quality_trend = " Improving" if quality_df['data_quality_rate_percent'].iloc[0] > 95 else " Declining" if quality_df['data_quality_rate_percent'].iloc[0] < 90 else " Stable"
            st.info(f"**Data Quality Trend:** {quality_trend}")
        
        if not backup_df.empty:
            backup_trend = " Improving" if backup_df['backup_success_rate_percent'].iloc[0] > 95 else " Declining" if backup_df['backup_success_rate_percent'].iloc[0] < 90 else " Stable"
            st.info(f"**Backup Success Trend:** {backup_trend}")
        
        if not storage_df.empty:
            storage_trend = " Growing" if storage_df['storage_usage_trend_percent'].iloc[0] > 85 else " Optimizing" if storage_df['storage_usage_trend_percent'].iloc[0] < 70 else " Stable"
            st.info(f"**Storage Usage Trend:** {storage_trend}")
    
    with col2:
        st.subheader(" Strategic Recommendations")
        
        # Generate strategic recommendations
        recommendations = []
        
        if not quality_df.empty and quality_df['data_quality_rate_percent'].iloc[0] < 95:
            recommendations.append(" **Data Quality Enhancement:** Implement automated data validation and cleansing processes")
        
        if not backup_df.empty and backup_df['backup_success_rate_percent'].iloc[0] < 95:
            recommendations.append(" **Backup Optimization:** Review backup schedules and implement redundant backup strategies")
        
        if not storage_df.empty and storage_df['storage_usage_trend_percent'].iloc[0] > 90:
            recommendations.append(" **Storage Expansion:** Plan for storage capacity expansion and implement data archiving")
        
        if not loss_df.empty and loss_df['data_loss_rate_percent'].iloc[0] > 0.01:
            recommendations.append(" **Data Protection:** Strengthen data loss prevention measures and incident response")
        
        if not recommendations:
            recommendations.append(" **Maintain Excellence:** Current data management practices are optimal")
        
        for rec in recommendations:
            st.info(rec)
    
    st.markdown("---")
    st.success(" **Enterprise Data Management Dashboard Complete** - Providing world-class data analytics, governance, and strategic insights!")

# Add placeholder functions for remaining pages
def show_project_management():
    """Display comprehensive project management analytics with world-class insights"""
    st.title(" Enterprise Project Management & Development Excellence Dashboard")
    
    if st.session_state.projects_data.empty:
        st.warning(" No project data available. Please upload data first.")
        return
    
    # Enterprise Project Overview Dashboard
    st.subheader(" Enterprise Project Overview Dashboard")
    
    # Summary Metrics with Enhanced Styling
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Total Projects", 
            f"{len(st.session_state.projects_data):,}",
            delta="+3",
            delta_color="normal"
        )
    
    with col2:
        completed_count = len(st.session_state.projects_data[st.session_state.projects_data.get('status', '') == 'Completed'])
        st.metric(
            "Completed Projects", 
            f"{completed_count:,}",
            delta=f"+{completed_count - 8}",
            delta_color="normal"
        )
    
    with col3:
        in_progress_count = len(st.session_state.projects_data[st.session_state.projects_data.get('status', '') == 'In Progress'])
        st.metric(
            "Active Projects", 
            f"{in_progress_count:,}",
            delta="+2",
            delta_color="normal"
        )
    
    with col4:
        planning_count = len(st.session_state.projects_data[st.session_state.projects_data.get('status', '') == 'Planning'])
        st.metric(
            "Planning Projects", 
            f"{planning_count:,}",
            delta="+1",
            delta_color="normal"
        )
    
    st.markdown("---")
    
    # Enhanced Project Delivery Metrics
    st.subheader(" Advanced Project Delivery & Performance Analytics")
    delivery_df, delivery_msg = calculate_project_delivery_metrics(st.session_state.projects_data)
    
    if not delivery_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Project Status Distribution with Advanced Analytics
            status_counts = st.session_state.projects_data['status'].value_counts()
            status_data = pd.DataFrame({
                'Status': status_counts.index,
                'Count': status_counts.values
            })
            
            # Enhanced Bar Chart with Performance Indicators
            fig = go.Figure()
            
            # Add bars with custom colors and enhanced styling
            colors = {'Completed': '#00ff88', 'In Progress': '#4285f4', 'Planning': '#ffa726', 'On Hold': '#ff6b6b'}
            
            for status in status_data['Status']:
                count = status_data[status_data['Status'] == status]['Count'].iloc[0]
                fig.add_trace(go.Bar(
                    name=status,
                    x=[status],
                    y=[count],
                    marker_color=colors.get(status, '#9c27b0'),
                    hovertemplate="<b>%{x}</b><br>Count: %{y}<br><extra></extra>"
                ))
            
            # Add performance threshold lines
            total_projects = len(st.session_state.projects_data)
            fig.add_hline(y=total_projects * 0.7, line_dash="dash", line_color="green", annotation_text="Target Completion (70%)")
            fig.add_hline(y=total_projects * 0.5, line_dash="dash", line_color="orange", annotation_text="Warning Level (50%)")
            
            fig.update_layout(
                title="Project Status Distribution with Performance Targets",
                xaxis_title="Project Status",
                yaxis_title="Number of Projects",
                barmode='group',
                height=500,
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced Project Metrics with Performance Scoring
            total_projects = delivery_df['total_projects'].iloc[0]
            completed_projects = delivery_df['completed_projects'].iloc[0]
            on_time_delivery = delivery_df['on_time_delivery_percent'].iloc[0]
            budget_adherence = delivery_df['budget_adherence_percent'].iloc[0]
            
            # Project Performance Scoring
            completion_rate = (completed_projects / max(total_projects, 1)) * 100
            overall_score = (completion_rate * 0.4 + on_time_delivery * 0.3 + budget_adherence * 0.3)
            
            if overall_score >= 85:
                project_status = " Excellent"
                project_color = "normal"
                project_msg = "World-class project delivery performance"
            elif overall_score >= 70:
                project_status = " Good"
                project_color = "normal"
                project_msg = "Good project delivery with optimization opportunities"
            elif overall_score >= 55:
                project_status = " Moderate"
                project_color = "inverse"
                project_msg = "Moderate performance - improvement needed"
            else:
                project_status = " Poor"
                project_color = "inverse"
                project_msg = "Critical performance issues - immediate action required"
            
            st.metric(
                "Total Projects", 
                total_projects,
                delta="+3",
                delta_color="normal"
            )
            
            st.metric(
                "Completed Projects", 
                completed_projects,
                delta=f"+{completed_projects - 8}",
                delta_color="normal"
            )
            
            st.metric(
                "On-Time Delivery", 
                f"{on_time_delivery:.1f}%",
                delta=f"{on_time_delivery - 80:.1f}%",
                delta_color="normal" if on_time_delivery >= 80 else "inverse"
            )
            
            st.metric(
                "Budget Adherence", 
                f"{budget_adherence:.1f}%",
                delta=f"{budget_adherence - 85:.1f}%",
                delta_color="normal" if budget_adherence >= 85 else "inverse"
            )
            
            # Project Performance Score
            st.subheader(" Performance Score")
            st.metric("Overall Score", f"{overall_score:.0f}/100", delta=f"{overall_score - 75:.0f}", delta_color=project_color)
            st.info(f"**Status:** {project_status}")
            st.info(f"**Message:** {project_msg}")
            
            # Project Health Indicators
            st.subheader(" Project Health")
            completion_health = " Healthy" if completion_rate >= 70 else " Moderate" if completion_rate >= 50 else " Critical"
            st.info(f"**Completion Health:** {completion_health}")
            
            delivery_health = " Healthy" if on_time_delivery >= 80 else " Moderate" if on_time_delivery >= 60 else " Critical"
            st.info(f"**Delivery Health:** {delivery_health}")
        
        st.info(f" {delivery_msg}")
        display_dataframe_with_index_1(delivery_df)
    
    st.markdown("---")
    
    # Enhanced Development Cycle Time
    st.subheader(" Advanced Development Cycle & Efficiency Analytics")
    cycle_df, cycle_msg = calculate_development_cycle_time(st.session_state.projects_data)
    
    if not cycle_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Scatter Plot with Advanced Analytics
            completed_projects = st.session_state.projects_data[st.session_state.projects_data.get('status', '') == 'Completed']
            if not completed_projects.empty:
                fig = go.Figure()
                
                # Enhanced scatter plot with performance thresholds
                fig.add_trace(go.Scatter(
                    x=completed_projects.get('budget', pd.Series([100000])),
                    y=completed_projects.get('actual_cost', pd.Series([95000])),
                    mode='markers',
                    marker=dict(
                        size=completed_projects.get('team_size', pd.Series([5])) * 2,
                        color=completed_projects.get('team_size', pd.Series([5])),
                        colorscale='viridis',
                        showscale=True,
                        colorbar=dict(title="Team Size")
                    ),
                    text=completed_projects.get('project_name', pd.Series(['Project'])),
                    hovertemplate="<b>%{text}</b><br>Budget: $%{x:,.0f}<br>Actual Cost: $%{y:,.0f}<br>Team Size: %{marker.color}<br><extra></extra>"
                ))
                
                # Add budget vs actual cost reference line
                max_budget = completed_projects.get('budget', pd.Series([100000])).max()
                max_cost = completed_projects.get('actual_cost', pd.Series([95000])).max()
                max_val = max(max_budget, max_cost)
                
                fig.add_trace(go.Scatter(
                    x=[0, max_val],
                    y=[0, max_val],
                    mode='lines',
                    line=dict(dash='dash', color='red'),
                    name='Budget = Actual Cost',
                    showlegend=True
                ))
                
                # Add performance zones
                fig.add_hrect(y0=0, y1=max_val * 0.9, fillcolor="green", opacity=0.1, annotation_text="Under Budget Zone")
                fig.add_hrect(y0=max_val * 0.9, y1=max_val * 1.1, fillcolor="orange", opacity=0.1, annotation_text="Budget Range Zone")
                fig.add_hrect(y0=max_val * 1.1, y1=max_val * 1.5, fillcolor="red", opacity=0.1, annotation_text="Over Budget Zone")
                
                fig.update_layout(
                    title="Budget vs Actual Cost Analysis with Performance Zones",
                    xaxis_title="Budget ($)",
                    yaxis_title="Actual Cost ($)",
                    height=500,
                    showlegend=True
                )
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced Cycle Time Metrics
            avg_cycle_time = cycle_df['avg_development_cycle_time_days'].iloc[0]
            fastest_cycle = cycle_df['fastest_cycle_days'].iloc[0]
            slowest_cycle = cycle_df['slowest_cycle_days'].iloc[0]
            
            # Cycle Time Performance Scoring
            cycle_efficiency = max(0, 100 - (avg_cycle_time - 30) * 2)  # Optimal at 30 days
            cycle_variance = ((slowest_cycle - fastest_cycle) / avg_cycle_time) * 100
            
            st.metric(
                "Avg Cycle Time", 
                f"{avg_cycle_time:.1f} days",
                delta=f"{avg_cycle_time - 45:.1f} days",
                delta_color="inverse" if avg_cycle_time > 60 else "normal"
            )
            
            st.metric(
                "Fastest Cycle", 
                f"{fastest_cycle:.1f} days",
                delta=f"{fastest_cycle - 20:.1f} days",
                delta_color="normal"
            )
            
            st.metric(
                "Slowest Cycle", 
                f"{slowest_cycle:.1f} days",
                delta=f"{slowest_cycle - 75:.1f} days",
                delta_color="inverse" if slowest_cycle > 90 else "normal"
            )
            
            # Cycle Time Performance
            st.subheader(" Cycle Performance")
            if cycle_efficiency >= 80:
                cycle_status = " Excellent"
                cycle_color = "normal"
                cycle_msg = "World-class cycle time efficiency"
            elif cycle_efficiency >= 60:
                cycle_status = " Good"
                cycle_color = "normal"
                cycle_msg = "Good cycle time with optimization opportunities"
            elif cycle_efficiency >= 40:
                cycle_status = " Moderate"
                cycle_color = "inverse"
                cycle_msg = "Moderate efficiency - improvement needed"
            else:
                cycle_status = " Poor"
                cycle_color = "inverse"
                cycle_msg = "Critical efficiency issues - immediate action required"
            
            st.metric("Efficiency Score", f"{cycle_efficiency:.0f}/100", delta=f"{cycle_efficiency - 70:.0f}", delta_color=cycle_color)
            st.info(f"**Status:** {cycle_status}")
            st.info(f"**Message:** {cycle_msg}")
            
            # Cycle Variance Analysis
            st.subheader(" Cycle Variance")
            variance_status = " Low" if cycle_variance <= 50 else " Moderate" if cycle_variance <= 100 else " High"
            st.info(f"**Variance Level:** {variance_status}")
            st.info(f"**Variance:** {cycle_variance:.1f}%")
        
        st.info(f" {cycle_msg}")
        display_dataframe_with_index_1(cycle_df)
    
    st.markdown("---")
    
    # Enhanced Code Quality Analysis
    st.subheader(" Advanced Code Quality & Standards Analytics")
    quality_df, quality_msg = calculate_code_quality_analysis(st.session_state.projects_data)
    
    if not quality_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Quality Metrics Visualization
            quality_metrics = {
                'Bug Density': quality_df['bugs_per_loc'].iloc[0],
                'Code Compliance': quality_df['coding_standard_compliance_percent'].iloc[0],
                'Total Bugs': quality_df['total_bugs'].iloc[0],
                'Lines of Code': quality_df['total_lines_of_code'].iloc[0]
            }
            
            # Enhanced Bar Chart with Quality Thresholds
            fig = go.Figure()
            
            # Add bars with quality-based colors
            colors = ['#ff6b6b', '#00ff88', '#ffa726', '#4285f4']
            
            for i, (metric, value) in enumerate(quality_metrics.items()):
                fig.add_trace(go.Bar(
                    name=metric,
                    x=[metric],
                    y=[value],
                    marker_color=colors[i],
                    hovertemplate="<b>%{x}</b><br>Value: %{y}<br><extra></extra>"
                ))
            
            # Add quality threshold lines
            fig.add_hline(y=0.003, line_dash="dash", line_color="green", annotation_text="Excellent Bug Density (<0.003)")
            fig.add_hline(y=0.005, line_dash="dash", line_color="orange", annotation_text="Good Bug Density (<0.005)")
            fig.add_hline(y=95, line_dash="dash", line_color="green", annotation_text="Excellent Compliance (95%)")
            fig.add_hline(y=85, line_dash="dash", line_color="orange", annotation_text="Good Compliance (85%)")
            
            fig.update_layout(
                title="Code Quality Metrics with Quality Thresholds",
                xaxis_title="Quality Metric",
                yaxis_title="Value",
                barmode='group',
                height=500,
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced Quality Metrics with Performance Scoring
            total_bugs = quality_df['total_bugs'].iloc[0]
            bugs_per_loc = quality_df['bugs_per_loc'].iloc[0]
            code_compliance = quality_df['coding_standard_compliance_percent'].iloc[0]
            total_lines = quality_df['total_lines_of_code'].iloc[0]
            
            # Quality Performance Scoring
            bug_score = max(0, 100 - (bugs_per_loc * 10000))  # Optimal at 0.001
            compliance_score = code_compliance
            overall_quality_score = (bug_score * 0.6 + compliance_score * 0.4)
            
            st.metric(
                "Total Bugs", 
                total_bugs,
                delta=f"{total_bugs - 120}",
                delta_color="inverse" if total_bugs > 150 else "normal"
            )
            
            st.metric(
                "Bugs per LOC", 
                f"{bugs_per_loc:.4f}",
                delta=f"{bugs_per_loc - 0.003:.4f}",
                delta_color="inverse" if bugs_per_loc > 0.005 else "normal"
            )
            
            st.metric(
                "Code Compliance", 
                f"{code_compliance:.1f}%",
                delta=f"{code_compliance - 90:.1f}%",
                delta_color="normal" if code_compliance >= 85 else "inverse"
            )
            
            st.metric(
                "Total Lines of Code", 
                f"{total_lines:,.0f}",
                delta="+5,000",
                delta_color="normal"
            )
            
            # Quality Performance Score
            st.subheader(" Quality Score")
            if overall_quality_score >= 90:
                quality_status = " Excellent"
                quality_color = "normal"
                quality_msg = "World-class code quality standards"
            elif overall_quality_score >= 75:
                quality_status = " Good"
                quality_color = "normal"
                quality_msg = "Good code quality with optimization opportunities"
            elif overall_quality_score >= 60:
                quality_status = " Moderate"
                quality_color = "inverse"
                quality_msg = "Moderate quality - improvement needed"
            else:
                quality_status = " Poor"
                quality_color = "inverse"
                quality_msg = "Critical quality issues - immediate action required"
            
            st.metric("Overall Score", f"{overall_quality_score:.0f}/100", delta=f"{overall_quality_score - 80:.0f}", delta_color=quality_color)
            st.info(f"**Status:** {quality_status}")
            st.info(f"**Message:** {quality_msg}")
        
        st.info(f" {quality_msg}")
        display_dataframe_with_index_1(quality_df)
    
    st.markdown("---")
    
    # Enhanced Release Management Metrics
    st.subheader(" Advanced Release Management & Deployment Analytics")
    release_df, release_msg = calculate_release_management_metrics(st.session_state.projects_data)
    
    if not release_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Release Metrics Visualization
            release_metrics = {
                'Deployment Success Rate': release_df['deployment_success_rate_percent'].iloc[0],
                'Rollback Rate': release_df['rollback_rate_percent'].iloc[0],
                'Total Deployments': release_df['total_deployments'].iloc[0]
            }
            
            # Enhanced Bar Chart with Release Thresholds
            fig = go.Figure()
            
            # Add bars with release-based colors
            colors = ['#00ff88', '#ff6b6b', '#4285f4']
            
            for i, (metric, value) in enumerate(release_metrics.items()):
                fig.add_trace(go.Bar(
                    name=metric,
                    x=[metric],
                    y=[value],
                    marker_color=colors[i],
                    hovertemplate="<b>%{x}</b><br>Value: %{y}<br><extra></extra>"
                ))
            
            # Add release threshold lines
            fig.add_hline(y=95, line_dash="dash", line_color="green", annotation_text="Excellent Success Rate (95%)")
            fig.add_hline(y=90, line_dash="dash", line_color="orange", annotation_text="Good Success Rate (90%)")
            fig.add_hline(y=5, line_dash="dash", line_color="red", annotation_text="Warning Rollback Rate (5%)")
            
            fig.update_layout(
                title="Release Management Metrics with Performance Thresholds",
                xaxis_title="Release Metric",
                yaxis_title="Value",
                barmode='group',
                height=500,
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced Release Metrics with Performance Scoring
            deployment_success = release_df['deployment_success_rate_percent'].iloc[0]
            rollback_rate = release_df['rollback_rate_percent'].iloc[0]
            total_deployments = release_df['total_deployments'].iloc[0]
            
            # Release Performance Scoring
            success_score = deployment_success
            rollback_score = max(0, 100 - (rollback_rate * 10))  # Optimal at 0%
            overall_release_score = (success_score * 0.7 + rollback_score * 0.3)
            
            st.metric(
                "Deployment Success Rate", 
                f"{deployment_success:.1f}%",
                delta=f"{deployment_success - 90:.1f}%",
                delta_color="normal" if deployment_success >= 90 else "inverse"
            )
            
            st.metric(
                "Rollback Rate", 
                f"{rollback_rate:.1f}%",
                delta=f"{rollback_rate - 3:.1f}%",
                delta_color="inverse" if rollback_rate > 5 else "normal"
            )
            
            st.metric(
                "Total Deployments", 
                total_deployments,
                delta="+5",
                delta_color="normal"
            )
            
            # Release Performance Score
            st.subheader(" Release Score")
            if overall_release_score >= 90:
                release_status = " Excellent"
                release_color = "normal"
                release_msg = "World-class release management"
            elif overall_release_score >= 75:
                release_status = " Good"
                release_color = "normal"
                release_msg = "Good release management with optimization opportunities"
            elif overall_release_score >= 60:
                release_status = " Moderate"
                release_color = "inverse"
                release_msg = "Moderate performance - improvement needed"
            else:
                release_status = " Poor"
                release_color = "inverse"
                release_msg = "Critical performance issues - immediate action required"
            
            st.metric("Overall Score", f"{overall_release_score:.0f}/100", delta=f"{overall_release_score - 80:.0f}", delta_color=release_color)
            st.info(f"**Status:** {release_status}")
            st.info(f"**Message:** {release_msg}")
        
        st.info(f" {release_msg}")
        display_dataframe_with_index_1(release_df)
    
    st.markdown("---")
    
    # Enhanced Agile Metrics
    st.subheader(" Advanced Agile & Sprint Performance Analytics")
    agile_df, agile_msg = calculate_agile_metrics(st.session_state.projects_data)
    
    if not agile_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Agile Metrics Visualization
            agile_metrics = {
                'Sprint Velocity': agile_df['sprint_velocity'].iloc[0],
                'Burndown Rate': agile_df['burndown_rate_percent'].iloc[0],
                'Total Story Points': agile_df['total_story_points'].iloc[0]
            }
            
            # Enhanced Bar Chart with Agile Thresholds
            fig = go.Figure()
            
            # Add bars with agile-based colors
            colors = ['#9c27b0', '#00ff88', '#ffa726']
            
            for i, (metric, value) in enumerate(agile_metrics.items()):
                fig.add_trace(go.Bar(
                    name=metric,
                    x=[metric],
                    y=[value],
                    marker_color=colors[i],
                    hovertemplate="<b>%{x}</b><br>Value: %{y}<br><extra></extra>"
                ))
            
            # Add agile threshold lines
            fig.add_hline(y=20, line_dash="dash", line_color="green", annotation_text="Excellent Velocity (20)")
            fig.add_hline(y=15, line_dash="dash", line_color="orange", annotation_text="Good Velocity (15)")
            fig.add_hline(y=90, line_dash="dash", line_color="green", annotation_text="Excellent Burndown (90%)")
            fig.add_hline(y=80, line_dash="dash", line_color="orange", annotation_text="Good Burndown (80%)")
            
            fig.update_layout(
                title="Agile Metrics Overview with Performance Thresholds",
                xaxis_title="Agile Metric",
                yaxis_title="Value",
                barmode='group',
                height=500,
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced Agile Metrics with Performance Scoring
            sprint_velocity = agile_df['sprint_velocity'].iloc[0]
            burndown_rate = agile_df['burndown_rate_percent'].iloc[0]
            total_story_points = agile_df['total_story_points'].iloc[0]
            
            # Agile Performance Scoring
            velocity_score = min(100, sprint_velocity * 5)  # Optimal at 20
            burndown_score = burndown_rate
            overall_agile_score = (velocity_score * 0.4 + burndown_score * 0.4 + min(100, total_story_points / 2) * 0.2)
            
            st.metric(
                "Sprint Velocity", 
                f"{sprint_velocity:.1f}",
                delta=f"{sprint_velocity - 12:.1f}",
                delta_color="normal" if sprint_velocity >= 15 else "inverse"
            )
            
            st.metric(
                "Burndown Rate", 
                f"{burndown_rate:.1f}%",
                delta=f"{burndown_rate - 80:.1f}%",
                delta_color="normal" if burndown_rate >= 80 else "inverse"
            )
            
            st.metric(
                "Total Story Points", 
                total_story_points,
                delta="+25",
                delta_color="normal"
            )
            
            # Agile Performance Score
            st.subheader(" Agile Score")
            if overall_agile_score >= 85:
                agile_status = " Excellent"
                agile_color = "normal"
                agile_msg = "World-class agile performance"
            elif overall_agile_score >= 70:
                agile_status = " Good"
                agile_color = "normal"
                agile_msg = "Good agile performance with optimization opportunities"
            elif overall_agile_score >= 55:
                agile_status = " Moderate"
                agile_color = "inverse"
                agile_msg = "Moderate performance - improvement needed"
            else:
                agile_status = " Poor"
                agile_color = "inverse"
                agile_msg = "Critical performance issues - immediate action required"
            
            st.metric("Overall Score", f"{overall_agile_score:.0f}/100", delta=f"{overall_agile_score - 75:.0f}", delta_color=agile_color)
            st.info(f"**Status:** {agile_status}")
            st.info(f"**Message:** {agile_msg}")
        
        st.info(f" {agile_msg}")
        display_dataframe_with_index_1(agile_df)
    
    # New Section: Project Portfolio Management Dashboard
    st.markdown("---")
    st.subheader(" Project Portfolio Management & Strategic Analytics Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # Portfolio Health Score
        if not delivery_df.empty and not quality_df.empty:
            portfolio_score = (delivery_df['on_time_delivery_percent'].iloc[0] * 0.3 + 
                             delivery_df['budget_adherence_percent'].iloc[0] * 0.3 +
                             quality_df['coding_standard_compliance_percent'].iloc[0] * 0.2 +
                             (100 - release_df['rollback_rate_percent'].iloc[0] * 10) * 0.2)
            
            if portfolio_score >= 90:
                portfolio_status = " Excellent"
                portfolio_color = "normal"
            elif portfolio_score >= 75:
                portfolio_status = " Good"
                portfolio_color = "normal"
            elif portfolio_score >= 60:
                portfolio_status = " Moderate"
                portfolio_color = "inverse"
            else:
                portfolio_status = " Poor"
                portfolio_color = "inverse"
            
            st.metric("Portfolio Health Score", f"{portfolio_score:.0f}/100", delta=f"{portfolio_score - 80:.0f}", delta_color=portfolio_color)
            st.info(f"**Status:** {portfolio_status}")
    
    with col2:
        # Resource Utilization Score
        if not cycle_df.empty and not agile_df.empty:
            resource_score = (cycle_df['avg_development_cycle_time_days'].iloc[0] * -2 + 100) * 0.5 + agile_df['sprint_velocity'].iloc[0] * 2.5
            
            st.metric("Resource Utilization Score", f"{resource_score:.0f}/100", delta=f"{resource_score - 75:.0f}", delta_color="normal" if resource_score >= 70 else "inverse")
            st.info(f"**Utilization:** {'High' if resource_score >= 80 else 'Medium' if resource_score >= 60 else 'Low'}")
    
    with col3:
        # Risk Management Score
        if not quality_df.empty and not release_df.empty:
            risk_score = (100 - quality_df['bugs_per_loc'].iloc[0] * 10000) * 0.6 + (100 - release_df['rollback_rate_percent'].iloc[0] * 10) * 0.4
            
            st.metric("Risk Management Score", f"{risk_score:.0f}/100", delta=f"{risk_score - 85:.0f}", delta_color="normal" if risk_score >= 80 else "inverse")
            st.info(f"**Risk Level:** {'Low' if risk_score >= 80 else 'Medium' if risk_score >= 60 else 'High'}")
    
    with col4:
        # Innovation Score
        if not agile_df.empty and not cycle_df.empty:
            innovation_score = agile_df['sprint_velocity'].iloc[0] * 3 + (100 - cycle_df['avg_development_cycle_time_days'].iloc[0]) * 0.5
            
            st.metric("Innovation Score", f"{innovation_score:.0f}/100", delta=f"{innovation_score - 70:.0f}", delta_color="normal" if innovation_score >= 65 else "inverse")
            st.info(f"**Innovation Level:** {'High' if innovation_score >= 80 else 'Medium' if innovation_score >= 60 else 'Low'}")
    
    # Project Analytics Trends & Strategic Insights
    st.markdown("---")
    st.subheader(" Project Analytics Trends & Strategic Insights")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader(" Current Project Trends")
        
        # Trend indicators
        if not delivery_df.empty:
            delivery_trend = " Improving" if delivery_df['on_time_delivery_percent'].iloc[0] > 80 else " Declining" if delivery_df['on_time_delivery_percent'].iloc[0] < 70 else " Stable"
            st.info(f"**Delivery Performance Trend:** {delivery_trend}")
        
        if not cycle_df.empty:
            cycle_trend = " Optimizing" if cycle_df['avg_development_cycle_time_days'].iloc[0] < 45 else " Slowing" if cycle_df['avg_development_cycle_time_days'].iloc[0] > 60 else " Stable"
            st.info(f"**Cycle Time Trend:** {cycle_trend}")
        
        if not quality_df.empty:
            quality_trend = " Improving" if quality_df['coding_standard_compliance_percent'].iloc[0] > 90 else " Declining" if quality_df['coding_standard_compliance_percent'].iloc[0] < 85 else " Stable"
            st.info(f"**Code Quality Trend:** {quality_trend}")
    
    with col2:
        st.subheader(" Strategic Recommendations")
        
        # Generate strategic recommendations
        recommendations = []
        
        if not delivery_df.empty and delivery_df['on_time_delivery_percent'].iloc[0] < 80:
            recommendations.append(" **Delivery Optimization:** Implement agile methodologies and improve project planning processes")
        
        if not cycle_df.empty and cycle_df['avg_development_cycle_time_days'].iloc[0] > 60:
            recommendations.append(" **Cycle Time Reduction:** Streamline development processes and implement CI/CD pipelines")
        
        if not quality_df.empty and quality_df['coding_standard_compliance_percent'].iloc[0] < 90:
            recommendations.append(" **Quality Enhancement:** Implement automated testing and code review processes")
        
        if not release_df.empty and release_df['rollback_rate_percent'].iloc[0] > 5:
            recommendations.append(" **Release Optimization:** Improve deployment processes and implement rollback strategies")
        
        if not agile_df.empty and agile_df['sprint_velocity'].iloc[0] < 15:
            recommendations.append(" **Agile Optimization:** Enhance sprint planning and team collaboration")
        
        if not recommendations:
            recommendations.append(" **Maintain Excellence:** Current project management practices are optimal")
        
        for rec in recommendations:
            st.info(rec)
    
    st.markdown("---")
    st.success(" **Enterprise Project Management Dashboard Complete** - Providing world-class project analytics, portfolio management, and strategic insights!")

def show_user_experience():
    """Display comprehensive user experience analytics with world-class insights"""
    st.title(" Enterprise User Experience & Accessibility Excellence Dashboard")
    
    if st.session_state.applications_data.empty or st.session_state.tickets_data.empty:
        st.warning(" No application or ticket data available. Please upload data first.")
        return
    
    # Enterprise UX Overview Dashboard
    st.subheader(" Enterprise User Experience Overview Dashboard")
    
    # Summary Metrics with Enhanced Styling
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Total Applications", 
            f"{len(st.session_state.applications_data):,}",
            delta="+2",
            delta_color="normal"
        )
    
    with col2:
        st.metric(
            "Total Users", 
            f"{len(st.session_state.tickets_data):,}",
            delta="+15",
            delta_color="normal"
        )
    
    with col3:
        st.metric(
            "Active Sessions", 
            f"{len(st.session_state.tickets_data) * 0.7:.0f}",
            delta="+8",
            delta_color="normal"
        )
    
    with col4:
        st.metric(
            "Support Tickets", 
            f"{len(st.session_state.tickets_data):,}",
            delta="-5",
            delta_color="inverse"
        )
    
    st.markdown("---")
    
    # Enhanced Application Usability Metrics
    st.subheader(" Advanced Application Usability & User Satisfaction Analytics")
    usability_df, usability_msg = calculate_application_usability_metrics(st.session_state.applications_data, st.session_state.tickets_data)
    
    if not usability_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Usability Metrics Visualization with Performance Thresholds
            usability_metrics = {
                'Usability Score': usability_df['usability_score_percent'].iloc[0],
                'Avg Satisfaction': usability_df['avg_satisfaction_score'].iloc[0] * 20,  # Convert to percentage
                'Satisfied Users': (usability_df['satisfied_users'].iloc[0] / usability_df['total_users_surveyed'].iloc[0]) * 100
            }
            
            # Enhanced Bar Chart with Usability Thresholds
            fig = go.Figure()
            
            # Add bars with usability-based colors
            colors = ['#00ff88', '#ffa726', '#ff6b6b']
            
            for i, (metric, value) in enumerate(usability_metrics.items()):
                fig.add_trace(go.Bar(
                    name=metric,
                    x=[metric],
                    y=[value],
                    marker_color=colors[i],
                    hovertemplate="<b>%{x}</b><br>Value: %{y:.1f}<br><extra></extra>"
                ))
            
            # Add usability threshold lines
            fig.add_hline(y=90, line_dash="dash", line_color="green", annotation_text="Excellent Usability (90%)")
            fig.add_hline(y=80, line_dash="dash", line_color="orange", annotation_text="Good Usability (80%)")
            fig.add_hline(y=70, line_dash="dash", line_color="red", annotation_text="Warning Level (70%)")
            
            fig.update_layout(
                title="Application Usability Metrics with Performance Thresholds",
                xaxis_title="Usability Metric",
                yaxis_title="Value (%)",
                barmode='group',
                height=500,
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced Usability Metrics with Performance Scoring
            usability_score = usability_df['usability_score_percent'].iloc[0]
            avg_satisfaction = usability_df['avg_satisfaction_score'].iloc[0]
            satisfied_users = usability_df['satisfied_users'].iloc[0]
            total_users = usability_df['total_users_surveyed'].iloc[0]
            
            # Usability Performance Scoring
            satisfaction_rate = (satisfied_users / max(total_users, 1)) * 100
            overall_ux_score = (usability_score * 0.4 + satisfaction_rate * 0.4 + (avg_satisfaction / 5 * 100) * 0.2)
            
            if overall_ux_score >= 90:
                ux_status = " Excellent"
                ux_color = "normal"
                ux_msg = "World-class user experience standards"
            elif overall_ux_score >= 80:
                ux_status = " Good"
                ux_color = "normal"
                ux_msg = "Good user experience with optimization opportunities"
            elif overall_ux_score >= 70:
                ux_status = " Moderate"
                ux_color = "inverse"
                ux_msg = "Moderate experience - improvement needed"
            else:
                ux_status = " Poor"
                ux_color = "inverse"
                ux_msg = "Critical experience issues - immediate action required"
            
            st.metric(
                "Usability Score", 
                f"{usability_score:.1f}%",
                delta=f"{usability_score - 85:.1f}%",
                delta_color="normal" if usability_score >= 80 else "inverse"
            )
            
            st.metric(
                "Avg Satisfaction", 
                f"{avg_satisfaction:.1f}/5",
                delta=f"{avg_satisfaction - 4.0:.1f}",
                delta_color="normal" if avg_satisfaction >= 4.0 else "inverse"
            )
            
            st.metric(
                "Satisfied Users", 
                f"{satisfied_users}/{total_users}",
                delta=f"{satisfaction_rate - 80:.1f}%",
                delta_color="normal" if satisfaction_rate >= 80 else "inverse"
            )
            
            # UX Performance Score
            st.subheader(" UX Performance Score")
            st.metric("Overall Score", f"{overall_ux_score:.0f}/100", delta=f"{overall_ux_score - 80:.0f}", delta_color=ux_color)
            st.info(f"**Status:** {ux_status}")
            st.info(f"**Message:** {ux_msg}")
            
            # User Experience Health
            st.subheader(" UX Health")
            usability_health = " Healthy" if usability_score >= 80 else " Moderate" if usability_score >= 70 else " Critical"
            st.info(f"**Usability Health:** {usability_health}")
            
            satisfaction_health = " Healthy" if satisfaction_rate >= 80 else " Moderate" if satisfaction_rate >= 70 else " Critical"
            st.info(f"**Satisfaction Health:** {satisfaction_health}")
        
        st.info(f" {usability_msg}")
        display_dataframe_with_index_1(usability_df)
    
    st.markdown("---")
    
    # Enhanced Website Performance Analysis
    st.subheader(" Advanced Website Performance & Digital Experience Analytics")
    if not st.session_state.applications_data.empty:
        web_df, web_msg = calculate_website_performance_analysis(st.session_state.applications_data, st.session_state.incidents_data)
        
        if not web_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Website Performance Visualization
                performance_metrics = {
                    'Avg Page Load Time': web_df['avg_page_load_time_seconds'].iloc[0],
                    'Mobile Traffic %': web_df['mobile_traffic_percentage'].iloc[0],
                    'Desktop Traffic %': web_df['desktop_traffic_percentage'].iloc[0]
                }
                
                # Enhanced Bar Chart with Performance Thresholds
                fig = go.Figure()
                
                # Add bars with performance-based colors
                colors = ['#ff6b6b', '#4285f4', '#34a853']
                
                for i, (metric, value) in enumerate(performance_metrics.items()):
                    fig.add_trace(go.Bar(
                        name=metric,
                        x=[metric],
                        y=[value],
                        marker_color=colors[i],
                        hovertemplate="<b>%{x}</b><br>Value: %{y:.1f}<br><extra></extra>"
                    ))
                
                # Add performance threshold lines
                fig.add_hline(y=3, line_dash="dash", line_color="green", annotation_text="Excellent Load Time (3s)")
                fig.add_hline(y=5, line_dash="dash", line_color="orange", annotation_text="Good Load Time (5s)")
                fig.add_hline(y=60, line_dash="dash", line_color="green", annotation_text="Excellent Mobile Traffic (60%)")
                fig.add_hline(y=50, line_dash="dash", line_color="orange", annotation_text="Good Mobile Traffic (50%)")
                
                fig.update_layout(
                    title="Website Performance Metrics with Performance Thresholds",
                    xaxis_title="Performance Metric",
                    yaxis_title="Value",
                    barmode='group',
                    height=500,
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Website Performance Metrics with Performance Scoring
                avg_load_time = web_df['avg_page_load_time_seconds'].iloc[0]
                mobile_traffic = web_df['mobile_traffic_percentage'].iloc[0]
                desktop_traffic = web_df['desktop_traffic_percentage'].iloc[0]
                web_incidents = web_df['web_incidents'].iloc[0]
                
                # Performance Scoring
                load_time_score = max(0, 100 - (avg_load_time - 2) * 20)  # Optimal at 2s
                mobile_score = min(100, mobile_traffic * 1.2)  # Optimal at 80%+
                overall_web_score = (load_time_score * 0.5 + mobile_score * 0.3 + (100 - web_incidents * 2) * 0.2)
                
                st.metric(
                    "Avg Page Load Time", 
                    f"{avg_load_time:.1f}s",
                    delta=f"{avg_load_time - 2.5:.1f}s",
                    delta_color="inverse" if avg_load_time > 5 else "normal"
                )
                
                st.metric(
                    "Mobile Traffic", 
                    f"{mobile_traffic:.1f}%",
                    delta=f"{mobile_traffic - 60:.1f}%",
                    delta_color="normal" if mobile_traffic >= 60 else "inverse"
                )
                
                st.metric(
                    "Web Incidents", 
                    web_incidents,
                    delta=f"{web_incidents - 8}",
                    delta_color="inverse" if web_incidents > 10 else "normal"
                )
                
                # Web Performance Score
                st.subheader(" Web Performance Score")
                if overall_web_score >= 85:
                    web_status = " Excellent"
                    web_color = "normal"
                    web_msg = "World-class website performance"
                elif overall_web_score >= 70:
                    web_status = " Good"
                    web_color = "normal"
                    web_msg = "Good performance with optimization opportunities"
                elif overall_web_score >= 55:
                    web_status = " Moderate"
                    web_color = "inverse"
                    web_msg = "Moderate performance - improvement needed"
                else:
                    web_status = " Poor"
                    web_color = "inverse"
                    web_msg = "Critical performance issues - immediate action required"
                
                st.metric("Overall Score", f"{overall_web_score:.0f}/100", delta=f"{overall_web_score - 75:.0f}", delta_color=web_color)
                st.info(f"**Status:** {web_status}")
                st.info(f"**Message:** {web_msg}")
            
            st.info(f" {web_msg}")
            display_dataframe_with_index_1(web_df)
    
    st.markdown("---")
    
    # Enhanced Accessibility Compliance Analysis
    st.subheader(" Advanced Accessibility Compliance & Inclusive Design Analytics")
    if not st.session_state.applications_data.empty:
        accessibility_df, accessibility_msg = calculate_accessibility_compliance_analysis(st.session_state.applications_data)
        
        if not accessibility_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Accessibility Compliance Visualization
                fig = go.Figure(data=[go.Pie(
                    labels=['Compliant Features', 'Non-Compliant Features', 'Partially Compliant'],
                    values=[accessibility_df['compliant_features'].iloc[0], 
                           accessibility_df['non_compliant_features'].iloc[0],
                           int(accessibility_df['total_features'].iloc[0] * 0.05)],  # 5% partially compliant
                    hole=0.4,
                    marker_colors=['#00ff88', '#ff6b6b', '#ffa726'],
                    hovertemplate="<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percent:.1%}<br><extra></extra>"
                )])
                
                fig.update_layout(
                    title="Accessibility Compliance Distribution & Inclusive Design Status",
                    showlegend=True,
                    legend=dict(orientation="v", yanchor="middle", y=0.5, xanchor="left", x=1.02),
                    height=500
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Accessibility Metrics with Performance Scoring
                compliance_rate = accessibility_df['accessibility_compliance_rate_percent'].iloc[0]
                compliant_features = accessibility_df['compliant_features'].iloc[0]
                non_compliant_features = accessibility_df['non_compliant_features'].iloc[0]
                total_features = accessibility_df['total_features'].iloc[0]
                
                # Accessibility Performance Scoring
                if compliance_rate >= 95:
                    accessibility_status = " Excellent"
                    accessibility_color = "normal"
                    accessibility_msg = "World-class accessibility compliance"
                elif compliance_rate >= 85:
                    accessibility_status = " Good"
                    accessibility_color = "normal"
                    accessibility_msg = "Good accessibility with minor compliance issues"
                elif compliance_rate >= 70:
                    accessibility_status = " Moderate"
                    accessibility_color = "inverse"
                    accessibility_msg = "Moderate compliance - attention needed"
                else:
                    accessibility_status = " Poor"
                    accessibility_color = "inverse"
                    accessibility_msg = "Critical compliance issues - immediate action required"
                
                st.metric(
                    "Compliance Rate", 
                    f"{compliance_rate:.1f}%",
                    delta=f"{compliance_rate - 85:.1f}%",
                    delta_color=accessibility_color
                )
                
                st.metric("Compliant Features", compliant_features)
                st.metric("Non-Compliant Features", non_compliant_features)
                
                # Accessibility Status
                st.subheader(" Accessibility Status")
                st.info(f"**Status:** {accessibility_status}")
                st.info(f"**Message:** {accessibility_msg}")
                
                # Compliance Health
                st.subheader(" Compliance Health")
                compliance_health = " Healthy" if compliance_rate >= 85 else " Moderate" if compliance_rate >= 70 else " Critical"
                st.info(f"**Compliance Health:** {compliance_health}")
                
                # WCAG Compliance Level
                wcag_level = "AAA" if compliance_rate >= 95 else "AA" if compliance_rate >= 85 else "A" if compliance_rate >= 70 else "Non-Compliant"
                st.info(f"**WCAG Level:** {wcag_level}")
            
            st.info(f" {accessibility_msg}")
            display_dataframe_with_index_1(accessibility_df)
    
    st.markdown("---")
    
    # Enhanced User Feedback Analysis
    st.subheader(" Advanced User Feedback & Sentiment Analytics")
    if not st.session_state.tickets_data.empty:
        feedback_df, feedback_msg = calculate_user_feedback_analysis(st.session_state.tickets_data)
        
        if not feedback_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced User Feedback Visualization
                feedback_metrics = {
                    'User Feedback Score': feedback_df['user_feedback_score_percent'].iloc[0],
                    'Avg Feedback Score': feedback_df['avg_feedback_score'].iloc[0] * 20,  # Convert to percentage
                    'Positive Feedback': (feedback_df['positive_feedback'].iloc[0] / feedback_df['total_feedback'].iloc[0]) * 100
                }
                
                # Enhanced Bar Chart with Feedback Thresholds
                fig = go.Figure()
                
                # Add bars with feedback-based colors
                colors = ['#00ff88', '#ffa726', '#4285f4']
                
                for i, (metric, value) in enumerate(feedback_metrics.items()):
                    fig.add_trace(go.Bar(
                        name=metric,
                        x=[metric],
                        y=[value],
                        marker_color=colors[i],
                        hovertemplate="<b>%{x}</b><br>Value: %{y:.1f}%<br><extra></extra>"
                    ))
                
                # Add feedback threshold lines
                fig.add_hline(y=90, line_dash="dash", line_color="green", annotation_text="Excellent Feedback (90%)")
                fig.add_hline(y=80, line_dash="dash", line_color="orange", annotation_text="Good Feedback (80%)")
                fig.add_hline(y=70, line_dash="dash", line_color="red", annotation_text="Warning Level (70%)")
                
                fig.update_layout(
                    title="User Feedback Metrics with Performance Thresholds",
                    xaxis_title="Feedback Metric",
                    yaxis_title="Value (%)",
                    barmode='group',
                    height=500,
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced User Feedback Metrics with Performance Scoring
                feedback_score = feedback_df['user_feedback_score_percent'].iloc[0]
                avg_feedback = feedback_df['avg_feedback_score'].iloc[0]
                positive_feedback = feedback_df['positive_feedback'].iloc[0]
                total_feedback = feedback_df['total_feedback'].iloc[0]
                
                # Feedback Performance Scoring
                positive_rate = (positive_feedback / max(total_feedback, 1)) * 100
                overall_feedback_score = (feedback_score * 0.4 + positive_rate * 0.4 + (avg_feedback / 5 * 100) * 0.2)
                
                st.metric(
                    "User Feedback Score", 
                    f"{feedback_score:.1f}%",
                    delta=f"{feedback_score - 80:.1f}%",
                    delta_color="normal" if feedback_score >= 80 else "inverse"
                )
                
                st.metric(
                    "Avg Feedback Score", 
                    f"{avg_feedback:.1f}/5",
                    delta=f"{avg_feedback - 4.0:.1f}",
                    delta_color="normal" if avg_feedback >= 4.0 else "inverse"
                )
                
                st.metric(
                    "Positive Feedback", 
                    f"{positive_feedback}/{total_feedback}",
                    delta=f"{positive_rate - 80:.1f}%",
                    delta_color="normal" if positive_rate >= 80 else "inverse"
                )
                
                # Feedback Performance Score
                st.subheader(" Feedback Score")
                if overall_feedback_score >= 85:
                    feedback_status = " Excellent"
                    feedback_color = "normal"
                    feedback_msg = "World-class user feedback"
                elif overall_feedback_score >= 70:
                    feedback_status = " Good"
                    feedback_color = "normal"
                    feedback_msg = "Good feedback with optimization opportunities"
                elif overall_feedback_score >= 55:
                    feedback_status = " Moderate"
                    feedback_color = "inverse"
                    feedback_msg = "Moderate feedback - improvement needed"
                else:
                    feedback_status = " Poor"
                    feedback_color = "inverse"
                    feedback_msg = "Critical feedback issues - immediate action required"
                
                st.metric("Overall Score", f"{overall_feedback_score:.0f}/100", delta=f"{overall_feedback_score - 75:.0f}", delta_color=feedback_color)
                st.info(f"**Status:** {feedback_status}")
                st.info(f"**Message:** {feedback_msg}")
            
            st.info(f" {feedback_msg}")
            display_dataframe_with_index_1(feedback_df)
    
    # New Section: User Experience Portfolio Management Dashboard
    st.markdown("---")
    st.subheader(" User Experience Portfolio Management & Strategic Analytics Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # Overall UX Score
        if not usability_df.empty and not web_df.empty:
            overall_ux_portfolio_score = (usability_df['usability_score_percent'].iloc[0] * 0.3 + 
                                        (100 - web_df['avg_page_load_time_seconds'].iloc[0] * 20) * 0.3 +
                                        accessibility_df['accessibility_compliance_rate_percent'].iloc[0] * 0.2 +
                                        feedback_df['user_feedback_score_percent'].iloc[0] * 0.2)
            
            if overall_ux_portfolio_score >= 90:
                portfolio_status = " Excellent"
                portfolio_color = "normal"
            elif overall_ux_portfolio_score >= 75:
                portfolio_status = " Good"
                portfolio_color = "normal"
            elif overall_ux_portfolio_score >= 60:
                portfolio_status = " Moderate"
                portfolio_color = "inverse"
            else:
                portfolio_status = " Poor"
                portfolio_color = "inverse"
            
            st.metric("Overall UX Score", f"{overall_ux_portfolio_score:.0f}/100", delta=f"{overall_ux_portfolio_score - 80:.0f}", delta_color=portfolio_color)
            st.info(f"**Status:** {portfolio_status}")
    
    with col2:
        # Digital Experience Score
        if not web_df.empty and not accessibility_df.empty:
            digital_score = (100 - web_df['avg_page_load_time_seconds'].iloc[0] * 20) * 0.6 + accessibility_df['accessibility_compliance_rate_percent'].iloc[0] * 0.4
            
            st.metric("Digital Experience Score", f"{digital_score:.0f}/100", delta=f"{digital_score - 85:.0f}", delta_color="normal" if digital_score >= 80 else "inverse")
            st.info(f"**Experience Level:** {'High' if digital_score >= 80 else 'Medium' if digital_score >= 60 else 'Low'}")
    
    with col3:
        # User Satisfaction Score
        if not usability_df.empty and not feedback_df.empty:
            satisfaction_score = (usability_df['avg_satisfaction_score'].iloc[0] / 5 * 100) * 0.7 + feedback_df['user_feedback_score_percent'].iloc[0] * 0.3
            
            st.metric("User Satisfaction Score", f"{satisfaction_score:.0f}/100", delta=f"{satisfaction_score - 85:.0f}", delta_color="normal" if satisfaction_score >= 80 else "inverse")
            st.info(f"**Satisfaction Level:** {'High' if satisfaction_score >= 80 else 'Medium' if satisfaction_score >= 60 else 'Low'}")
    
    with col4:
        # Accessibility Excellence Score
        if not accessibility_df.empty:
            accessibility_excellence = accessibility_df['accessibility_compliance_rate_percent'].iloc[0] * 1.1
            
            st.metric("Accessibility Excellence", f"{accessibility_excellence:.0f}/100", delta=f"{accessibility_excellence - 90:.0f}", delta_color="normal" if accessibility_excellence >= 85 else "inverse")
            st.info(f"**Accessibility Level:** {'World-Class' if accessibility_excellence >= 95 else 'Excellent' if accessibility_excellence >= 85 else 'Good'}")
    
    # User Experience Analytics Trends & Strategic Insights
    st.markdown("---")
    st.subheader(" User Experience Analytics Trends & Strategic Insights")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader(" Current UX Trends")
        
        # Trend indicators
        if not usability_df.empty:
            usability_trend = " Improving" if usability_df['usability_score_percent'].iloc[0] > 85 else " Declining" if usability_df['usability_score_percent'].iloc[0] < 75 else " Stable"
            st.info(f"**Usability Trend:** {usability_trend}")
        
        if not web_df.empty:
            performance_trend = " Optimizing" if web_df['avg_page_load_time_seconds'].iloc[0] < 3 else " Slowing" if web_df['avg_page_load_time_seconds'].iloc[0] > 5 else " Stable"
            st.info(f"**Performance Trend:** {performance_trend}")
        
        if not accessibility_df.empty:
            accessibility_trend = " Improving" if accessibility_df['accessibility_compliance_rate_percent'].iloc[0] > 85 else " Declining" if accessibility_df['accessibility_compliance_rate_percent'].iloc[0] < 75 else " Stable"
            st.info(f"**Accessibility Trend:** {accessibility_trend}")
    
    with col2:
        st.subheader(" Strategic Recommendations")
        
        # Generate strategic recommendations
        recommendations = []
        
        if not usability_df.empty and usability_df['usability_score_percent'].iloc[0] < 85:
            recommendations.append(" **Usability Enhancement:** Implement user-centered design principles and conduct usability testing")
        
        if not web_df.empty and web_df['avg_page_load_time_seconds'].iloc[0] > 3:
            recommendations.append(" **Performance Optimization:** Implement CDN, image optimization, and caching strategies")
        
        if not accessibility_df.empty and accessibility_df['accessibility_compliance_rate_percent'].iloc[0] < 85:
            recommendations.append(" **Accessibility Improvement:** Implement WCAG 2.1 guidelines and conduct accessibility audits")
        
        if not feedback_df.empty and feedback_df['user_feedback_score_percent'].iloc[0] < 80:
            recommendations.append(" **Feedback Optimization:** Implement real-time feedback collection and user satisfaction surveys")
        
        if not recommendations:
            recommendations.append(" **Maintain Excellence:** Current user experience practices are optimal")
        
        for rec in recommendations:
            st.info(rec)
    
    st.markdown("---")
    st.success(" **Enterprise User Experience Dashboard Complete** - Providing world-class UX analytics, accessibility insights, and strategic recommendations!")

def show_cost_optimization():
    """Display comprehensive cost optimization analytics with world-class insights"""
    st.title(" Enterprise Cost Optimization & Financial Excellence Dashboard")
    
    if st.session_state.projects_data.empty and st.session_state.assets_data.empty:
        st.warning(" No project or asset data available. Please upload data first.")
        return
    
    # Enterprise Cost Overview Dashboard
    st.subheader(" Enterprise Cost Overview Dashboard")
    
    # Summary Metrics with Enhanced Styling
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Total IT Budget", 
            f"${(st.session_state.projects_data.get('budget', pd.Series([0])).sum() + st.session_state.assets_data.get('purchase_cost', pd.Series([0])).sum()):,.0f}",
            delta="+$125K",
            delta_color="normal"
        )
    
    with col2:
        st.metric(
            "Active Projects", 
            f"{len(st.session_state.projects_data[st.session_state.projects_data.get('status', '') == 'In Progress'])}",
            delta="+3",
            delta_color="normal"
        )
    
    with col3:
        st.metric(
            "Total Assets", 
            f"{len(st.session_state.assets_data)}",
            delta="+12",
            delta_color="normal"
        )
    
    with col4:
        st.metric(
            "Cost Efficiency", 
            "87.3%",
            delta="+2.1%",
            delta_color="normal"
        )
    
    st.markdown("---")
    
    # Enhanced IT Budget Utilization
    st.subheader(" Advanced IT Budget Utilization & Financial Analytics")
    budget_df, budget_msg = calculate_it_budget_utilization(st.session_state.projects_data, st.session_state.assets_data)
    
    if not budget_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Budget Utilization Visualization
            budget_metrics = {
                'Budget Utilization': budget_df['budget_utilization_percent'].iloc[0],
                'Project Costs': budget_df['project_costs'].iloc[0],
                'Asset Costs': budget_df['asset_costs'].iloc[0]
            }
            
            # Enhanced Bar Chart with Budget Thresholds
            fig = go.Figure()
            
            # Add bars with budget-based colors
            colors = ['#00ff88', '#ffa726', '#ff6b6b']
            
            for i, (metric, value) in enumerate(budget_metrics.items()):
                fig.add_trace(go.Bar(
                    name=metric,
                    x=[metric],
                    y=[value],
                    marker_color=colors[i],
                    hovertemplate="<b>%{x}</b><br>Amount: $%{y:,.0f}<br><extra></extra>"
                ))
            
            # Add budget threshold lines
            fig.add_hline(y=budget_df['budgeted_amount'].iloc[0], line_dash="dash", line_color="green", annotation_text="Budgeted Amount")
            fig.add_hline(y=budget_df['budgeted_amount'].iloc[0] * 0.9, line_dash="dash", line_color="orange", annotation_text="90% Budget Warning")
            fig.add_hline(y=budget_df['budgeted_amount'].iloc[0] * 0.8, line_dash="dash", line_color="red", annotation_text="80% Budget Alert")
            
            fig.update_layout(
                title="IT Budget Breakdown with Financial Thresholds",
                xaxis_title="Budget Category",
                yaxis_title="Amount ($)",
                barmode='group',
                height=500,
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced Budget Metrics with Performance Scoring
            budget_utilization = budget_df['budget_utilization_percent'].iloc[0]
            actual_spending = budget_df['actual_spending'].iloc[0]
            budgeted_amount = budget_df['budgeted_amount'].iloc[0]
            project_costs = budget_df['project_costs'].iloc[0]
            asset_costs = budget_df['asset_costs'].iloc[0]
            
            # Budget Performance Scoring
            if budget_utilization <= 90:
                budget_status = " Excellent"
                budget_color = "normal"
                budget_msg = "World-class budget management"
            elif budget_utilization <= 100:
                budget_status = " Good"
                budget_color = "normal"
                budget_msg = "Good budget management with optimization opportunities"
            elif budget_utilization <= 110:
                budget_status = " Moderate"
                budget_color = "inverse"
                budget_msg = "Moderate budget management - attention needed"
            else:
                budget_status = " Poor"
                budget_color = "inverse"
                budget_msg = "Critical budget overrun - immediate action required"
            
            st.metric(
                "Budget Utilization", 
                f"{budget_utilization:.1f}%",
                delta=f"{budget_utilization - 100:.1f}%",
                delta_color=budget_color
            )
            
            st.metric(
                "Actual Spending", 
                f"${actual_spending:,.0f}",
                delta=f"${actual_spending - budgeted_amount:,.0f}",
                delta_color="inverse" if actual_spending > budgeted_amount else "normal"
            )
            
            st.metric(
                "Budgeted Amount", 
                f"${budgeted_amount:,.0f}"
            )
            
            # Budget Performance Score
            st.subheader(" Budget Performance Score")
            budget_score = max(0, 100 - (budget_utilization - 90) * 10)  # Optimal at 90%
            st.metric("Overall Score", f"{budget_score:.0f}/100", delta=f"{budget_score - 85:.0f}", delta_color=budget_color)
            st.info(f"**Status:** {budget_status}")
            st.info(f"**Message:** {budget_msg}")
            
            # Budget Health Indicators
            st.subheader(" Budget Health")
            utilization_health = " Healthy" if budget_utilization <= 90 else " Moderate" if budget_utilization <= 100 else " Critical"
            st.info(f"**Utilization Health:** {utilization_health}")
            
            spending_health = " Healthy" if actual_spending <= budgeted_amount else " Over Budget"
            st.info(f"**Spending Health:** {spending_health}")
        
        st.info(f" {budget_msg}")
        display_dataframe_with_index_1(budget_df)
    
    st.markdown("---")
    
    # Enhanced Cost per User/Device Analysis
    st.subheader(" Advanced Cost per User/Device & Resource Efficiency Analytics")
    if not st.session_state.assets_data.empty or not st.session_state.users_data.empty:
        cost_df, cost_msg = calculate_cost_per_user_device(st.session_state.assets_data, st.session_state.users_data)
        
        if not cost_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Cost per User/Device Visualization
                cost_metrics = {
                    'Cost per User': cost_df['cost_per_user'].iloc[0],
                    'Cost per Device': cost_df['cost_per_device'].iloc[0],
                    'Total Asset Costs': cost_df['total_asset_cost'].iloc[0]
                }
                
                # Enhanced Bar Chart with Cost Thresholds
                fig = go.Figure()
                
                # Add bars with cost-based colors
                colors = ['#ff6b6b', '#ffa726', '#4285f4']
                
                for i, (metric, value) in enumerate(cost_metrics.items()):
                    fig.add_trace(go.Bar(
                        name=metric,
                        x=[metric],
                        y=[value],
                        marker_color=colors[i],
                        hovertemplate="<b>%{x}</b><br>Amount: $%{y:,.0f}<br><extra></extra>"
                    ))
                
                # Add cost threshold lines
                optimal_cost_per_user = 5000  # Optimal cost per user
                optimal_cost_per_device = 3000  # Optimal cost per device
                
                fig.add_hline(y=optimal_cost_per_user, line_dash="dash", line_color="green", annotation_text="Optimal Cost per User ($5K)")
                fig.add_hline(y=optimal_cost_per_user * 1.2, line_dash="dash", line_color="orange", annotation_text="Warning Cost per User ($6K)")
                fig.add_hline(y=optimal_cost_per_device, line_dash="dash", line_color="green", annotation_text="Optimal Cost per Device ($3K)")
                fig.add_hline(y=optimal_cost_per_device * 1.2, line_dash="dash", line_color="orange", annotation_text="Warning Cost per Device ($3.6K)")
                
                fig.update_layout(
                    title="Cost Distribution Analysis with Efficiency Thresholds",
                    xaxis_title="Cost Metric",
                    yaxis_title="Amount ($)",
                    barmode='group',
                    height=500,
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Cost Metrics with Performance Scoring
                cost_per_user = cost_df['cost_per_user'].iloc[0]
                cost_per_device = cost_df['cost_per_device'].iloc[0]
                total_asset_cost = cost_df['total_asset_cost'].iloc[0]
                total_users = cost_df['total_users'].iloc[0]
                
                # Cost Efficiency Scoring
                user_cost_score = max(0, 100 - (cost_per_user - optimal_cost_per_user) / 100)  # Optimal at $5K
                device_cost_score = max(0, 100 - (cost_per_device - optimal_cost_per_device) / 100)  # Optimal at $3K
                overall_cost_score = (user_cost_score * 0.6 + device_cost_score * 0.4)
                
                st.metric(
                    "Cost per User", 
                    f"${cost_per_user:,.0f}",
                    delta=f"${cost_per_user - optimal_cost_per_user:,.0f}",
                    delta_color="inverse" if cost_per_user > optimal_cost_per_user else "normal"
                )
                
                st.metric(
                    "Cost per Device", 
                    f"${cost_per_device:,.0f}",
                    delta=f"${cost_per_device - optimal_cost_per_device:,.0f}",
                    delta_color="inverse" if cost_per_device > optimal_cost_per_device else "normal"
                )
                
                st.metric(
                    "Total Users", 
                    total_users,
                    delta="+5",
                    delta_color="normal"
                )
                
                # Cost Efficiency Score
                st.subheader(" Cost Efficiency Score")
                if overall_cost_score >= 85:
                    cost_status = " Excellent"
                    cost_color = "normal"
                    cost_msg = "World-class cost efficiency"
                elif overall_cost_score >= 70:
                    cost_status = " Good"
                    cost_color = "normal"
                    cost_msg = "Good cost efficiency with optimization opportunities"
                elif overall_cost_score >= 55:
                    cost_status = " Moderate"
                    cost_color = "inverse"
                    cost_msg = "Moderate efficiency - improvement needed"
                else:
                    cost_status = " Poor"
                    cost_color = "inverse"
                    cost_msg = "Critical efficiency issues - immediate action required"
                
                st.metric("Overall Score", f"{overall_cost_score:.0f}/100", delta=f"{overall_cost_score - 75:.0f}", delta_color=cost_color)
                st.info(f"**Status:** {cost_status}")
                st.info(f"**Message:** {cost_msg}")
            
            st.info(f" {cost_msg}")
            display_dataframe_with_index_1(cost_df)
    
    st.markdown("---")
    
    # Enhanced Cloud Cost Analysis
    st.subheader(" Advanced Cloud Cost & Infrastructure Optimization Analytics")
    if not st.session_state.assets_data.empty or not st.session_state.servers_data.empty:
        cloud_df, cloud_msg = calculate_cloud_cost_analysis(st.session_state.assets_data, st.session_state.servers_data)
        
        if not cloud_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Cloud Cost Visualization
                cloud_costs = cloud_df['cloud_costs'].iloc[0]
                on_premise_costs = cloud_df['on_premise_costs'].iloc[0]
                total_costs = cloud_df['total_costs'].iloc[0]
                cloud_percentage = cloud_df['cloud_percentage'].iloc[0]
                
                # Enhanced Pie Chart with Cloud Optimization
                fig = go.Figure(data=[go.Pie(
                    labels=['Cloud Costs', 'On-Premise Costs', 'Potential Savings'],
                    values=[cloud_costs, on_premise_costs, on_premise_costs * 0.2],  # 20% potential savings
                    hole=0.4,
                    marker_colors=['#4285f4', '#34a853', '#ffa726'],
                    hovertemplate="<b>%{label}</b><br>Amount: $%{value:,.0f}<br>Percentage: %{percent:.1%}<br><extra></extra>"
                )])
                
                fig.update_layout(
                    title="Cloud vs On-Premise Cost Distribution with Optimization Potential",
                    showlegend=True,
                    legend=dict(orientation="v", yanchor="middle", y=0.5, xanchor="left", x=1.02),
                    height=500
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Cloud Metrics with Performance Scoring
                # Cloud Performance Scoring
                if cloud_percentage >= 70:
                    cloud_status = " Excellent"
                    cloud_color = "normal"
                    cloud_msg = "World-class cloud adoption"
                elif cloud_percentage >= 50:
                    cloud_status = " Good"
                    cloud_color = "normal"
                    cloud_msg = "Good cloud adoption with optimization opportunities"
                elif cloud_percentage >= 30:
                    cloud_status = " Moderate"
                    cloud_color = "inverse"
                    cloud_msg = "Moderate cloud adoption - improvement needed"
                else:
                    cloud_status = " Poor"
                    cloud_color = "inverse"
                    cloud_msg = "Low cloud adoption - immediate action required"
                
                st.metric(
                    "Cloud Percentage", 
                    f"{cloud_percentage:.1f}%",
                    delta=f"{cloud_percentage - 50:.1f}%",
                    delta_color=cloud_color
                )
                
                st.metric(
                    "Cloud Costs", 
                    f"${cloud_costs:,.0f}",
                    delta="-$5K",
                    delta_color="normal"
                )
                
                st.metric(
                    "On-Premise Costs", 
                    f"${on_premise_costs:,.0f}",
                    delta="-$8K",
                    delta_color="normal"
                )
                
                # Cloud Optimization Score
                st.subheader(" Cloud Optimization Score")
                cloud_optimization_score = cloud_percentage * 1.2 + (100 - (on_premise_costs / total_costs * 100)) * 0.8
                st.metric("Overall Score", f"{cloud_optimization_score:.0f}/100", delta=f"{cloud_optimization_score - 75:.0f}", delta_color=cloud_color)
                st.info(f"**Status:** {cloud_status}")
                st.info(f"**Message:** {cloud_msg}")
                
                # Cloud Migration Potential
                st.subheader(" Migration Potential")
                migration_potential = " High" if cloud_percentage < 50 else " Medium" if cloud_percentage < 70 else " Low"
                st.info(f"**Migration Potential:** {migration_potential}")
                
                potential_savings = on_premise_costs * 0.2
                st.info(f"**Potential Annual Savings:** ${potential_savings:,.0f}")
            
            st.info(f" {cloud_msg}")
            display_dataframe_with_index_1(cloud_df)
    
    st.markdown("---")
    
    # Enhanced Energy Efficiency Analysis
    st.subheader(" Advanced Energy Efficiency & Sustainability Analytics")
    if not st.session_state.servers_data.empty or not st.session_state.assets_data.empty:
        energy_df, energy_msg = calculate_energy_efficiency_analysis(st.session_state.servers_data, st.session_state.assets_data)
        
        if not energy_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Energy Efficiency Visualization
                energy_metrics = {
                    'Energy Efficiency Score': energy_df['energy_efficiency_score_percent'].iloc[0],
                    'Total Power Consumption': energy_df['total_power_consumption_watts'].iloc[0] / 1000,  # Convert to kW
                    'Annual Cost Savings': energy_df['annual_cost_savings'].iloc[0] / 1000  # Convert to K
                }
                
                # Enhanced Bar Chart with Energy Thresholds
                fig = go.Figure()
                
                # Add bars with energy-based colors
                colors = ['#00ff88', '#ff6b6b', '#ffa726']
                
                for i, (metric, value) in enumerate(energy_metrics.items()):
                    fig.add_trace(go.Bar(
                        name=metric,
                        x=[metric],
                        y=[value],
                        marker_color=colors[i],
                        hovertemplate="<b>%{x}</b><br>Value: %{y:.1f}<br><extra></extra>"
                    ))
                
                # Add energy threshold lines
                fig.add_hline(y=90, line_dash="dash", line_color="green", annotation_text="Excellent Efficiency (90%)")
                fig.add_hline(y=80, line_dash="dash", line_color="orange", annotation_text="Good Efficiency (80%)")
                fig.add_hline(y=15, line_dash="dash", line_color="green", annotation_text="Optimal Power (15kW)")
                fig.add_hline(y=25, line_dash="dash", line_color="orange", annotation_text="Warning Power (25kW)")
                
                fig.update_layout(
                    title="Energy Efficiency Metrics with Sustainability Thresholds",
                    xaxis_title="Energy Metric",
                    yaxis_title="Value",
                    barmode='group',
                    height=500,
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Energy Metrics with Performance Scoring
                energy_score = energy_df['energy_efficiency_score_percent'].iloc[0]
                power_consumption = energy_df['total_power_consumption_watts'].iloc[0]
                cost_savings = energy_df['annual_cost_savings'].iloc[0]
                
                # Energy Performance Scoring
                power_efficiency = max(0, 100 - (power_consumption - 15000) / 100)  # Optimal at 15kW
                overall_energy_score = (energy_score * 0.6 + power_efficiency * 0.4)
                
                st.metric(
                    "Energy Efficiency Score", 
                    f"{energy_score:.1f}%",
                    delta=f"{energy_score - 80:.1f}%",
                    delta_color="normal" if energy_score >= 80 else "inverse"
                )
                
                st.metric(
                    "Total Power Consumption", 
                    f"{power_consumption:.0f}W",
                    delta=f"{power_consumption - 15000:.0f}W",
                    delta_color="inverse" if power_consumption > 20000 else "normal"
                )
                
                st.metric(
                    "Annual Cost Savings", 
                    f"${cost_savings:,.0f}",
                    delta="+$5K",
                    delta_color="normal"
                )
                
                # Energy Performance Score
                st.subheader(" Energy Performance Score")
                if overall_energy_score >= 85:
                    energy_status = " Excellent"
                    energy_color = "normal"
                    energy_msg = "World-class energy efficiency"
                elif overall_energy_score >= 70:
                    energy_status = " Good"
                    energy_color = "normal"
                    energy_msg = "Good energy efficiency with optimization opportunities"
                elif overall_energy_score >= 55:
                    energy_status = " Moderate"
                    energy_color = "inverse"
                    energy_msg = "Moderate efficiency - improvement needed"
                else:
                    energy_status = " Poor"
                    energy_color = "inverse"
                    energy_msg = "Critical efficiency issues - immediate action required"
                
                st.metric("Overall Score", f"{overall_energy_score:.0f}/100", delta=f"{overall_energy_score - 75:.0f}", delta_color=energy_color)
                st.info(f"**Status:** {energy_status}")
                st.info(f"**Message:** {energy_msg}")
                
                # Sustainability Impact
                st.subheader(" Sustainability Impact")
                carbon_reduction = power_consumption * 0.0005  # kg CO2 reduction
                st.info(f"**Carbon Reduction:** {carbon_reduction:.1f} kg CO2/year")
                
                sustainability_level = " High" if energy_score >= 85 else " Medium" if energy_score >= 70 else " Low"
                st.info(f"**Sustainability Level:** {sustainability_level}")
            
            st.info(f" {energy_msg}")
            display_dataframe_with_index_1(energy_df)
    
    # New Section: Cost Optimization Portfolio Management Dashboard
    st.markdown("---")
    st.subheader(" Cost Optimization Portfolio Management & Strategic Analytics Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # Overall Cost Optimization Score
        if not budget_df.empty and not cost_df.empty:
            overall_cost_score = (budget_score * 0.3 + overall_cost_score * 0.3 + cloud_optimization_score * 0.2 + overall_energy_score * 0.2)
            
            if overall_cost_score >= 90:
                portfolio_status = " Excellent"
                portfolio_color = "normal"
            elif overall_cost_score >= 75:
                portfolio_status = " Good"
                portfolio_color = "normal"
            elif overall_cost_score >= 60:
                portfolio_status = " Moderate"
                portfolio_color = "inverse"
            else:
                portfolio_status = " Poor"
                portfolio_color = "inverse"
            
            st.metric("Overall Cost Score", f"{overall_cost_score:.0f}/100", delta=f"{overall_cost_score - 80:.0f}", delta_color=portfolio_color)
            st.info(f"**Status:** {portfolio_status}")
    
    with col2:
        # Financial Efficiency Score
        if not budget_df.empty and not cost_df.empty:
            financial_efficiency = (budget_score * 0.6 + overall_cost_score * 0.4)
            
            st.metric("Financial Efficiency", f"{financial_efficiency:.0f}/100", delta=f"{financial_efficiency - 85:.0f}", delta_color="normal" if financial_efficiency >= 80 else "inverse")
            st.info(f"**Efficiency Level:** {'High' if financial_efficiency >= 80 else 'Medium' if financial_efficiency >= 60 else 'Low'}")
    
    with col3:
        # Resource Optimization Score
        if not cost_df.empty and not energy_df.empty:
            resource_optimization = (overall_cost_score * 0.7 + overall_energy_score * 0.3)
            
            st.metric("Resource Optimization", f"{resource_optimization:.0f}/100", delta=f"{resource_optimization - 80:.0f}", delta_color="normal" if resource_optimization >= 75 else "inverse")
            st.info(f"**Optimization Level:** {'High' if resource_optimization >= 75 else 'Medium' if resource_optimization >= 60 else 'Low'}")
    
    with col4:
        # Cloud Strategy Score
        if not cloud_df.empty:
            cloud_strategy_score = cloud_optimization_score * 1.1
            
            st.metric("Cloud Strategy", f"{cloud_strategy_score:.0f}/100", delta=f"{cloud_strategy_score - 85:.0f}", delta_color="normal" if cloud_strategy_score >= 80 else "inverse")
            st.info(f"**Strategy Level:** {'World-Class' if cloud_strategy_score >= 90 else 'Excellent' if cloud_strategy_score >= 80 else 'Good'}")
    
    # Cost Optimization Analytics Trends & Strategic Insights
    st.markdown("---")
    st.subheader(" Cost Optimization Analytics Trends & Strategic Insights")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader(" Current Cost Trends")
        
        # Trend indicators
        if not budget_df.empty:
            budget_trend = " Improving" if budget_utilization <= 90 else " Declining" if budget_utilization > 110 else " Stable"
            st.info(f"**Budget Management Trend:** {budget_trend}")
        
        if not cost_df.empty:
            cost_trend = " Optimizing" if overall_cost_score >= 80 else " Increasing" if overall_cost_score < 60 else " Stable"
            st.info(f"**Cost Efficiency Trend:** {cost_trend}")
        
        if not cloud_df.empty:
            cloud_trend = " Migrating" if cloud_percentage < 50 else " Stabilizing" if cloud_percentage >= 70 else " Balanced"
            st.info(f"**Cloud Adoption Trend:** {cloud_trend}")
    
    with col2:
        st.subheader(" Strategic Recommendations")
        
        # Generate strategic recommendations
        recommendations = []
        
        if not budget_df.empty and budget_utilization > 100:
            recommendations.append(" **Budget Optimization:** Implement cost controls and review project priorities")
        
        if not cost_df.empty and overall_cost_score < 70:
            recommendations.append(" **Resource Optimization:** Review asset allocation and user cost distribution")
        
        if not cloud_df.empty and cloud_percentage < 50:
            recommendations.append(" **Cloud Migration:** Develop cloud-first strategy and migration roadmap")
        
        if not energy_df.empty and overall_energy_score < 75:
            recommendations.append(" **Energy Optimization:** Implement green IT initiatives and power management")
        
        if not recommendations:
            recommendations.append(" **Maintain Excellence:** Current cost optimization practices are optimal")
        
        for rec in recommendations:
            st.info(rec)
    
    st.markdown("---")
    st.success(" **Enterprise Cost Optimization Dashboard Complete** - Providing world-class financial analytics, resource optimization, and strategic cost insights!")

def show_strategy_innovation():
    """Display comprehensive IT strategy and innovation analytics with world-class insights"""
    st.title(" Enterprise IT Strategy & Innovation Excellence Dashboard")
    
    if st.session_state.applications_data.empty and st.session_state.users_data.empty:
        st.warning(" No application or user data available. Please upload data first.")
        return
    
    # Enterprise Strategy Overview Dashboard
    st.subheader(" Enterprise IT Strategy Overview Dashboard")
    
    # Summary Metrics with Enhanced Styling
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Total Applications", 
            f"{len(st.session_state.applications_data):,}",
            delta="+5",
            delta_color="normal"
        )
    
    with col2:
        st.metric(
            "Total Users", 
            f"{len(st.session_state.users_data):,}",
            delta="+18",
            delta_color="normal"
        )
    
    with col3:
        st.metric(
            "Active Projects", 
            f"{len(st.session_state.projects_data[st.session_state.projects_data.get('status', '') == 'In Progress'])}",
            delta="+3",
            delta_color="normal"
        )
    
    with col4:
        st.metric(
            "Innovation Index", 
            "89.2",
            delta="+4.1",
            delta_color="normal"
        )
    
    st.markdown("---")
    
    # Enhanced Technology Adoption Metrics
    st.subheader(" Advanced Technology Adoption & Digital Transformation Analytics")
    adoption_df, adoption_msg = calculate_technology_adoption_metrics(st.session_state.applications_data, st.session_state.users_data)
    
    if not adoption_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Enhanced Technology Adoption Visualization
            adoption_metrics = {
                'Adoption Rate': adoption_df['adoption_rate_percent'].iloc[0],
                'Active Users': adoption_df['active_users'].iloc[0],
                'New Technologies': adoption_df['new_technologies_introduced'].iloc[0]
            }
            
            # Enhanced Bar Chart with Adoption Thresholds
            fig = go.Figure()
            
            # Add bars with adoption-based colors
            colors = ['#00ff88', '#ffa726', '#4285f4']
            
            for i, (metric, value) in enumerate(adoption_metrics.items()):
                fig.add_trace(go.Bar(
                    name=metric,
                    x=[metric],
                    y=[value],
                    marker_color=colors[i],
                    hovertemplate="<b>%{x}</b><br>Value: %{y}<br><extra></extra>"
                ))
            
            # Add adoption threshold lines
            fig.add_hline(y=80, line_dash="dash", line_color="green", annotation_text="Excellent Adoption (80%)")
            fig.add_hline(y=70, line_dash="dash", line_color="orange", annotation_text="Good Adoption (70%)")
            fig.add_hline(y=60, line_dash="dash", line_color="red", annotation_text="Warning Level (60%)")
            
            fig.update_layout(
                title="Technology Adoption Metrics with Digital Transformation Thresholds",
                xaxis_title="Adoption Metric",
                yaxis_title="Value",
                barmode='group',
                height=500,
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Enhanced Technology Adoption Metrics with Performance Scoring
            adoption_rate = adoption_df['adoption_rate_percent'].iloc[0]
            active_users = adoption_df['active_users'].iloc[0]
            total_users = adoption_df['total_users'].iloc[0]
            new_technologies = adoption_df['new_technologies_introduced'].iloc[0]
            
            # Technology Adoption Performance Scoring
            user_engagement = (active_users / max(total_users, 1)) * 100
            innovation_velocity = min(100, new_technologies * 5)  # Optimal at 20+ technologies
            overall_adoption_score = (adoption_rate * 0.4 + user_engagement * 0.4 + innovation_velocity * 0.2)
            
            if overall_adoption_score >= 85:
                adoption_status = " Excellent"
                adoption_color = "normal"
                adoption_msg = "World-class technology adoption"
            elif overall_adoption_score >= 70:
                adoption_status = " Good"
                adoption_color = "normal"
                adoption_msg = "Good adoption with optimization opportunities"
            elif overall_adoption_score >= 55:
                adoption_status = " Moderate"
                adoption_color = "inverse"
                adoption_msg = "Moderate adoption - improvement needed"
            else:
                adoption_status = " Poor"
                adoption_color = "inverse"
                adoption_msg = "Critical adoption issues - immediate action required"
            
            st.metric(
                "Adoption Rate", 
                f"{adoption_rate:.1f}%",
                delta=f"{adoption_rate - 75:.1f}%",
                delta_color="normal" if adoption_rate >= 70 else "inverse"
            )
            
            st.metric(
                "Active Users", 
                f"{active_users}/{total_users}",
                delta=f"{user_engagement - 75:.1f}%",
                delta_color="normal" if user_engagement >= 75 else "inverse"
            )
            
            st.metric(
                "New Technologies", 
                new_technologies,
                delta="+3",
                delta_color="normal"
            )
            
            # Technology Adoption Score
            st.subheader(" Technology Adoption Score")
            st.metric("Overall Score", f"{overall_adoption_score:.0f}/100", delta=f"{overall_adoption_score - 75:.0f}", delta_color=adoption_color)
            st.info(f"**Status:** {adoption_status}")
            st.info(f"**Message:** {adoption_msg}")
            
            # Digital Transformation Health
            st.subheader(" Digital Transformation Health")
            adoption_health = " Healthy" if adoption_rate >= 75 else " Moderate" if adoption_rate >= 60 else " Critical"
            st.info(f"**Adoption Health:** {adoption_health}")
            
            innovation_health = " High" if new_technologies >= 15 else " Medium" if new_technologies >= 10 else " Low"
            st.info(f"**Innovation Health:** {innovation_health}")
        
        st.info(f" {adoption_msg}")
        display_dataframe_with_index_1(adoption_df)
    
    st.markdown("---")
    
    # Enhanced ROI on IT Investments
    st.subheader(" Advanced ROI on IT Investments & Financial Impact Analytics")
    if not st.session_state.projects_data.empty or not st.session_state.assets_data.empty:
        roi_df, roi_msg = calculate_roi_on_it_investments(st.session_state.projects_data, st.session_state.assets_data)
        
        if not roi_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced ROI Visualization
                roi_metrics = {
                    'ROI Percentage': roi_df['roi_percentage'].iloc[0],
                    'Total Investment': roi_df['cost_of_it_investment'].iloc[0] / 1000,  # Convert to K
                    'Total Returns': roi_df['benefits_from_it_investment'].iloc[0] / 1000  # Convert to K
                }
                
                # Enhanced Bar Chart with ROI Thresholds
                fig = go.Figure()
                
                # Add bars with ROI-based colors
                colors = ['#00ff88', '#ff6b6b', '#ffa726']
                
                for i, (metric, value) in enumerate(roi_metrics.items()):
                    fig.add_trace(go.Bar(
                        name=metric,
                        x=[metric],
                        y=[value],
                        marker_color=colors[i],
                        hovertemplate="<b>%{x}</b><br>Value: %{y:.1f}K<br><extra></extra>"
                    ))
                
                # Add ROI threshold lines
                fig.add_hline(y=50, line_dash="dash", line_color="green", annotation_text="Excellent ROI (50%)")
                fig.add_hline(y=25, line_dash="dash", line_color="orange", annotation_text="Good ROI (25%)")
                fig.add_hline(y=15, line_dash="dash", line_color="red", annotation_text="Warning ROI (15%)")
                
                fig.update_layout(
                    title="IT Investment ROI Analysis with Financial Performance Thresholds",
                    xaxis_title="ROI Metric",
                    yaxis_title="Value (K)",
                    barmode='group',
                    height=500,
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced ROI Metrics with Performance Scoring
                roi_percentage = roi_df['roi_percentage'].iloc[0]
                total_investment = roi_df['cost_of_it_investment'].iloc[0]
                total_returns = roi_df['benefits_from_it_investment'].iloc[0]
                
                # ROI Performance Scoring
                if roi_percentage >= 50:
                    roi_status = " Excellent"
                    roi_color = "normal"
                    roi_msg = "World-class investment returns"
                elif roi_percentage >= 25:
                    roi_status = " Good"
                    roi_color = "normal"
                    roi_msg = "Good returns with optimization opportunities"
                elif roi_percentage >= 15:
                    roi_status = " Moderate"
                    roi_color = "inverse"
                    roi_msg = "Moderate returns - improvement needed"
                else:
                    roi_status = " Poor"
                    roi_color = "inverse"
                    roi_msg = "Critical return issues - immediate action required"
                
                st.metric(
                    "ROI Percentage", 
                    f"{roi_percentage:.1f}%",
                    delta=f"{roi_percentage - 25:.1f}%",
                    delta_color=roi_color
                )
                
                st.metric(
                    "Total Investment", 
                    f"${total_investment:,.0f}",
                    delta="+$50K",
                    delta_color="normal"
                )
                
                st.metric(
                    "Total Returns", 
                    f"${total_returns:,.0f}",
                    delta="+$75K",
                    delta_color="normal"
                )
                
                # ROI Performance Score
                st.subheader(" ROI Performance Score")
                roi_score = min(100, roi_percentage * 2)  # Convert to 100-point scale
                st.metric("Overall Score", f"{roi_score:.0f}/100", delta=f"{roi_score - 50:.0f}", delta_color=roi_color)
                st.info(f"**Status:** {roi_status}")
                st.info(f"**Message:** {roi_msg}")
                
                # Investment Health
                st.subheader(" Investment Health")
                roi_health = " Healthy" if roi_percentage >= 25 else " Moderate" if roi_percentage >= 15 else " Critical"
                st.info(f"**ROI Health:** {roi_health}")
                
                investment_efficiency = " High" if roi_percentage >= 40 else " Medium" if roi_percentage >= 20 else " Low"
                st.info(f"**Investment Efficiency:** {investment_efficiency}")
            
            st.info(f" {roi_msg}")
            display_dataframe_with_index_1(roi_df)
    
    st.markdown("---")
    
    # Enhanced Emerging Technology Feasibility
    st.subheader(" Advanced Emerging Technology Feasibility & Innovation Analytics")
    if not st.session_state.projects_data.empty:
        feasibility_df, feasibility_msg = calculate_emerging_technology_feasibility(st.session_state.projects_data)
        
        if not feasibility_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Enhanced Feasibility Visualization
                feasibility_metrics = {
                    'Feasibility Score': feasibility_df['feasibility_score_percent'].iloc[0],
                    'Technologies Assessed': feasibility_df['total_technologies_assessed'].iloc[0],
                    'Positive Impact': feasibility_df['technologies_with_positive_impact'].iloc[0]
                }
                
                # Enhanced Bar Chart with Feasibility Thresholds
                fig = go.Figure()
                
                # Add bars with feasibility-based colors
                colors = ['#00ff88', '#ffa726', '#4285f4']
                
                for i, (metric, value) in enumerate(feasibility_metrics.items()):
                    fig.add_trace(go.Bar(
                        name=metric,
                        x=[metric],
                        y=[value],
                        marker_color=colors[i],
                        hovertemplate="<b>%{x}</b><br>Value: %{y}<br><extra></extra>"
                    ))
                
                # Add feasibility threshold lines
                fig.add_hline(y=80, line_dash="dash", line_color="green", annotation_text="Excellent Feasibility (80%)")
                fig.add_hline(y=70, line_dash="dash", line_color="orange", annotation_text="Good Feasibility (70%)")
                fig.add_hline(y=60, line_dash="dash", line_color="red", annotation_text="Warning Level (60%)")
                
                fig.update_layout(
                    title="Emerging Technology Assessment with Innovation Thresholds",
                    xaxis_title="Feasibility Metric",
                    yaxis_title="Value",
                    barmode='group',
                    height=500,
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Enhanced Feasibility Metrics with Performance Scoring
                feasibility_score = feasibility_df['feasibility_score_percent'].iloc[0]
                technologies_assessed = feasibility_df['total_technologies_assessed'].iloc[0]
                positive_impact = feasibility_df['technologies_with_positive_impact'].iloc[0]
                
                # Feasibility Performance Scoring
                assessment_coverage = min(100, technologies_assessed * 2)  # Optimal at 50+ technologies
                overall_feasibility_score = (feasibility_score * 0.6 + assessment_coverage * 0.4)
                
                if overall_feasibility_score >= 85:
                    feasibility_status = " Excellent"
                    feasibility_color = "normal"
                    feasibility_msg = "World-class technology assessment"
                elif overall_feasibility_score >= 70:
                    feasibility_status = " Good"
                    feasibility_color = "normal"
                    feasibility_msg = "Good assessment with optimization opportunities"
                elif overall_feasibility_score >= 55:
                    feasibility_status = " Moderate"
                    feasibility_color = "inverse"
                    feasibility_msg = "Moderate assessment - improvement needed"
                else:
                    feasibility_status = " Poor"
                    feasibility_color = "inverse"
                    feasibility_msg = "Critical assessment issues - immediate action required"
                
                st.metric(
                    "Feasibility Score", 
                    f"{feasibility_score:.1f}%",
                    delta=f"{feasibility_score - 70:.1f}%",
                    delta_color="normal" if feasibility_score >= 70 else "inverse"
                )
                
                st.metric(
                    "Technologies Assessed", 
                    technologies_assessed,
                    delta="+5",
                    delta_color="normal"
                )
                
                st.metric(
                    "Positive Impact", 
                    positive_impact,
                    delta=f"{positive_impact - 10}",
                    delta_color="normal"
                )
                
                # Feasibility Performance Score
                st.subheader(" Feasibility Performance Score")
                st.metric("Overall Score", f"{overall_feasibility_score:.0f}/100", delta=f"{overall_feasibility_score - 70:.0f}", delta_color=feasibility_color)
                st.info(f"**Status:** {feasibility_status}")
                st.info(f"**Message:** {feasibility_msg}")
                
                # Innovation Readiness
                st.subheader(" Innovation Readiness")
                readiness_level = " High" if feasibility_score >= 80 else " Medium" if feasibility_score >= 70 else " Low"
                st.info(f"**Readiness Level:** {readiness_level}")
                
                assessment_quality = " Comprehensive" if technologies_assessed >= 40 else " Moderate" if technologies_assessed >= 25 else " Limited"
                st.info(f"**Assessment Quality:** {assessment_quality}")
            
            st.info(f" {feasibility_msg}")
            display_dataframe_with_index_1(feasibility_df)
    
    st.markdown("---")
    
    # IT Alignment with Business Goals
    st.subheader(" IT Alignment with Business Goals")
    if not st.session_state.projects_data.empty:
        alignment_df, alignment_msg = calculate_it_alignment_with_business_goals(st.session_state.projects_data)
        
        if not alignment_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Create alignment visualization
                alignment_metrics = {
                    'Alignment Score': alignment_df['alignment_score_percent'].iloc[0],
                    'Aligned Initiatives': alignment_df['it_initiatives_aligned_with_business_goals'].iloc[0],
                    'Total Initiatives': alignment_df['total_it_initiatives'].iloc[0]
                }
                
                alignment_data = pd.DataFrame({
                    'Metric': list(alignment_metrics.keys()),
                    'Value': list(alignment_metrics.values())
                })
                
                fig = create_chart("bar", alignment_data, x='Metric', y='Value',
                            title='IT-Business Alignment Metrics',
                            color='Value',
                            color_continuous_scale='viridis')
                fig.update_layout(xaxis_title="Metric", yaxis_title="Value")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.metric("Alignment Score", f"{alignment_df['alignment_score_percent'].iloc[0]:.1f}%")
                st.metric("Aligned Initiatives", alignment_df['it_initiatives_aligned_with_business_goals'].iloc[0])
                st.metric("Total Initiatives", alignment_df['total_it_initiatives'].iloc[0])
            
            st.info(f" {alignment_msg}")
            display_dataframe_with_index_1(alignment_df)
    
    # New Section: IT Strategy Portfolio Management Dashboard
    st.markdown("---")
    st.subheader(" IT Strategy Portfolio Management & Strategic Analytics Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # Overall Strategy Score
        if not adoption_df.empty and not roi_df.empty and 'overall_adoption_score' in locals() and 'roi_score' in locals() and 'overall_feasibility_score' in locals() and 'overall_alignment_score' in locals():
            overall_strategy_score = (overall_adoption_score * 0.3 + roi_score * 0.3 + overall_feasibility_score * 0.2 + overall_alignment_score * 0.2)
            
            if overall_strategy_score >= 90:
                strategy_status = " Excellent"
                strategy_color = "normal"
            elif overall_strategy_score >= 75:
                strategy_status = " Good"
                strategy_color = "normal"
            elif overall_strategy_score >= 60:
                strategy_status = " Moderate"
                strategy_color = "inverse"
            else:
                strategy_status = " Poor"
                strategy_color = "inverse"
            
            st.metric("Overall Strategy Score", f"{overall_strategy_score:.0f}/100", delta=f"{overall_strategy_score - 80:.0f}", delta_color=strategy_color)
            st.info(f"**Status:** {strategy_status}")
    
    with col2:
        # Innovation Excellence Score
        if not adoption_df.empty and not feasibility_df.empty and 'overall_adoption_score' in locals() and 'overall_feasibility_score' in locals():
            innovation_excellence = (overall_adoption_score * 0.6 + overall_feasibility_score * 0.4)
            
            st.metric("Innovation Excellence", f"{innovation_excellence:.0f}/100", delta=f"{innovation_excellence - 85:.0f}", delta_color="normal" if innovation_excellence >= 80 else "inverse")
            st.info(f"**Excellence Level:** {'World-Class' if innovation_excellence >= 90 else 'High' if innovation_excellence >= 80 else 'Medium' if innovation_excellence >= 60 else 'Low'}")
    
    with col3:
        # Business Value Score
        if not roi_df.empty and not alignment_df.empty and 'roi_score' in locals() and 'overall_alignment_score' in locals():
            business_value = (roi_score * 0.7 + overall_alignment_score * 0.3)
            
            st.metric("Business Value", f"{business_value:.0f}/100", delta=f"{business_value - 80:.0f}", delta_color="normal" if business_value >= 75 else "inverse")
            st.info(f"**Value Level:** {'High' if business_value >= 80 else 'Medium' if business_value >= 60 else 'Low'}")
    
    with col4:
        # Digital Transformation Score
        if not adoption_df.empty and not alignment_df.empty and 'overall_adoption_score' in locals() and 'overall_alignment_score' in locals():
            digital_transformation = (overall_adoption_score * 0.5 + overall_alignment_score * 0.5)
            
            st.metric("Digital Transformation", f"{digital_transformation:.0f}/100", delta=f"{digital_transformation - 85:.0f}", delta_color="normal" if digital_transformation >= 80 else "inverse")
            st.info(f"**Transformation Level:** {'Advanced' if digital_transformation >= 90 else 'Mature' if digital_transformation >= 80 else 'Developing' if digital_transformation >= 60 else 'Early'}")
    
    # IT Strategy Analytics Trends & Strategic Insights
    st.markdown("---")
    st.subheader(" IT Strategy Analytics Trends & Strategic Insights")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader(" Current Strategy Trends")
        
        # Trend indicators
        if not adoption_df.empty and 'adoption_rate' in locals():
            adoption_trend = " Accelerating" if adoption_rate >= 80 else " Slowing" if adoption_rate < 60 else " Stable"
            st.info(f"**Technology Adoption Trend:** {adoption_trend}")
        
        if not roi_df.empty and 'roi_percentage' in locals():
            roi_trend = " Improving" if roi_percentage >= 30 else " Declining" if roi_percentage < 15 else " Stable"
            st.info(f"**ROI Performance Trend:** {roi_trend}")
        
        if not feasibility_df.empty and 'feasibility_score' in locals():
            feasibility_trend = " Advancing" if feasibility_score >= 75 else " Stagnating" if feasibility_score < 60 else " Stable"
            st.info(f"**Technology Feasibility Trend:** {feasibility_trend}")
    
    with col2:
        st.subheader(" Strategic Recommendations")
        
        # Generate strategic recommendations
        recommendations = []
        
        if not adoption_df.empty and 'adoption_rate' in locals() and adoption_rate < 75:
            recommendations.append(" **Technology Adoption:** Implement change management and user training programs")
        
        if not roi_df.empty and 'roi_percentage' in locals() and roi_percentage < 25:
            recommendations.append(" **ROI Optimization:** Review investment priorities and implement value measurement")
        
        if not feasibility_df.empty and 'feasibility_score' in locals() and feasibility_score < 70:
            recommendations.append(" **Technology Assessment:** Establish innovation labs and feasibility frameworks")
        
        if not alignment_df.empty and 'alignment_score' in locals() and alignment_score < 80:
            recommendations.append(" **Business Alignment:** Strengthen stakeholder engagement and strategic planning")
        
        if not recommendations:
            recommendations.append(" **Maintain Excellence:** Current IT strategy practices are optimal")
        
        for rec in recommendations:
            st.info(rec)
    
    st.markdown("---")
    st.success(" **Enterprise IT Strategy & Innovation Dashboard Complete** - Providing world-class strategic analytics, innovation insights, and business alignment excellence!")

def show_training_development():
    """Display training and development analysis"""
    st.title(" Training & Development")
    
    # Overview metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total IT Staff", "45", delta="+3")
    with col2:
        st.metric("Training Completion Rate", "87%", delta="+5%")
    with col3:
        st.metric("Certifications Earned", "23", delta="+8")
    with col4:
        st.metric("Training Hours", "1,240", delta="+120")
    
    st.markdown("---")
    
    # Training Program Analysis
    st.subheader(" Training Program Analysis")
    
    # Mock training data
    training_data = pd.DataFrame({
        'Program': ['Cybersecurity', 'Cloud Computing', 'DevOps', 'Data Analytics', 'Network Security'],
        'Participants': [28, 32, 25, 19, 22],
        'Completion_Rate': [92, 88, 85, 78, 90],
        'Satisfaction_Score': [4.2, 4.5, 4.1, 4.3, 4.4],
        'Cost_per_Participant': [1200, 1500, 1800, 2000, 1400]
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Training completion rates chart
        fig = px.bar(training_data, x='Program', y='Completion_Rate',
                     title='Training Program Completion Rates',
                     color='Completion_Rate',
                     color_continuous_scale='viridis')
        fig.update_layout(xaxis_title="Training Program", yaxis_title="Completion Rate (%)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Avg Completion Rate", f"{training_data['Completion_Rate'].mean():.1f}%")
        st.metric("Total Participants", training_data['Participants'].sum())
        st.metric("Total Programs", len(training_data))
    
    st.markdown("---")
    
    # Skill Development Tracking
    st.subheader(" Skill Development Tracking")
    
    # Mock skill assessment data
    skill_data = pd.DataFrame({
        'Skill_Category': ['Programming', 'Networking', 'Security', 'Cloud', 'DevOps', 'Data Analysis'],
        'Beginner_Level': [8, 12, 15, 10, 14, 9],
        'Intermediate_Level': [18, 16, 12, 20, 15, 18],
        'Advanced_Level': [12, 10, 8, 12, 8, 15],
        'Expert_Level': [7, 7, 10, 3, 8, 3]
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Skill level distribution chart
        skill_melted = skill_data.melt(id_vars=['Skill_Category'], 
                                      value_vars=['Beginner_Level', 'Intermediate_Level', 'Advanced_Level', 'Expert_Level'],
                                      var_name='Level', value_name='Count')
        
        fig = px.bar(skill_melted, x='Skill_Category', y='Count', color='Level',
                     title='IT Staff Skill Levels by Category',
                     barmode='stack')
        fig.update_layout(xaxis_title="Skill Category", yaxis_title="Number of Staff")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Total Staff Assessed", skill_data[['Beginner_Level', 'Intermediate_Level', 'Advanced_Level', 'Expert_Level']].sum().sum())
        st.metric("Most Skilled Area", "Programming")
        st.metric("Growth Opportunity", "Cloud Computing")
    
    st.markdown("---")
    
    # Certification Tracking
    st.subheader(" Certification Tracking")
    
    # Mock certification data
    cert_data = pd.DataFrame({
        'Certification': ['AWS Solutions Architect', 'Cisco CCNA', 'CompTIA Security+', 'Microsoft Azure', 'PMP'],
        'Holders': [12, 18, 25, 15, 8],
        'Expiry_Date': ['2025-12', '2025-08', '2025-06', '2025-10', '2025-04'],
        'Renewal_Cost': [150, 200, 150, 165, 300]
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Certification holders chart
        fig = px.pie(cert_data, values='Holders', names='Certification',
                     title='IT Staff Certifications Distribution')
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Total Certifications", cert_data['Holders'].sum())
        st.metric("Most Popular", "CompTIA Security+")
        st.metric("Total Renewal Cost", f"${cert_data['Renewal_Cost'].sum():,}")
    
    st.markdown("---")
    
    # Training ROI Analysis
    st.subheader(" Training ROI Analysis")
    
    # Mock ROI data
    roi_data = pd.DataFrame({
        'Metric': ['Training Investment', 'Productivity Gain', 'Error Reduction', 'Innovation Projects', 'Total ROI'],
        'Value': [125000, 180000, 45000, 75000, 175000],
        'Percentage': [100, 144, 36, 60, 140]
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # ROI analysis chart
        fig = px.bar(roi_data, x='Metric', y='Percentage',
                     title='Training ROI Analysis (% of Investment)',
                     color='Percentage',
                     color_continuous_scale='RdYlGn')
        fig.update_layout(xaxis_title="Metric", yaxis_title="Percentage of Investment")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Total Investment", f"${roi_data.loc[roi_data['Metric'] == 'Training Investment', 'Value'].iloc[0]:,}")
        st.metric("Total Returns", f"${roi_data.loc[roi_data['Metric'] == 'Total ROI', 'Value'].iloc[0]:,}")
        st.metric("ROI Percentage", f"{roi_data.loc[roi_data['Metric'] == 'Total ROI', 'Percentage'].iloc[0]:.0f}%")
    
    st.markdown("---")
    
    # Future Training Recommendations
    st.subheader(" Future Training Recommendations")
    
    recommendations = [
        " **Cybersecurity Advanced Training**: Focus on emerging threats and zero-trust architecture",
        " **Multi-Cloud Strategy**: Expand expertise beyond AWS to Azure and Google Cloud",
        " **AI/ML Integration**: Develop skills in AI-powered IT operations and automation",
        " **Data Governance**: Enhance data management and compliance training",
        " **Agile & DevOps**: Strengthen collaboration and continuous delivery practices"
    ]
    
    for rec in recommendations:
        st.markdown(rec)
    
    # Training calendar preview
    st.markdown("---")
    st.subheader(" Upcoming Training Sessions")
    
    upcoming_training = pd.DataFrame({
        'Date': ['2025-01-15', '2025-01-22', '2025-02-05', '2025-02-12'],
        'Program': ['Advanced Cybersecurity', 'Cloud Architecture', 'DevOps Pipeline', 'Data Analytics'],
        'Instructor': ['Dr. Smith', 'Jane Doe', 'Mike Johnson', 'Sarah Wilson'],
        'Duration': ['2 days', '3 days', '2 days', '4 days'],
        'Seats_Available': [15, 12, 18, 10]
    })
    
    st.dataframe(upcoming_training, use_container_width=True)
    
    st.info(" **Tip**: Regular training and development programs lead to improved employee satisfaction, reduced turnover, and enhanced organizational capabilities.")

def show_disaster_recovery():
    """Display disaster recovery and business continuity analysis"""
    st.title(" Disaster Recovery & Business Continuity")
    
    # Overview metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("RTO Target", "4 hours", delta="-1 hour")
    with col2:
        st.metric("RPO Target", "1 hour", delta="-30 min")
    with col3:
        st.metric("Recovery Success Rate", "99.2%", delta="+0.3%")
    with col4:
        st.metric("Last Test Date", "2024-12-15", delta="2 days ago")
    
    st.markdown("---")
    
    # Recovery Time Objectives (RTO) Analysis
    st.subheader(" Recovery Time Objectives (RTO) Analysis")
    
    # Mock RTO data
    rto_data = pd.DataFrame({
        'System_Category': ['Critical Systems', 'Business Systems', 'Support Systems', 'Development Systems'],
        'Current_RTO': [2.5, 6.0, 12.0, 24.0],
        'Target_RTO': [4.0, 8.0, 16.0, 48.0],
        'Compliance_Status': ['Compliant', 'Compliant', 'Compliant', 'Compliant']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # RTO comparison chart
        fig = go.Figure()
        fig.add_trace(go.Bar(name='Current RTO', x=rto_data['System_Category'], y=rto_data['Current_RTO'],
                             marker_color='lightblue'))
        fig.add_trace(go.Bar(name='Target RTO', x=rto_data['System_Category'], y=rto_data['Target_RTO'],
                             marker_color='lightcoral'))
        
        fig.update_layout(title='RTO Performance vs Targets (Hours)',
                         barmode='group',
                         xaxis_title="System Category",
                         yaxis_title="Recovery Time (Hours)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Avg Current RTO", f"{rto_data['Current_RTO'].mean():.1f} hours")
        st.metric("Avg Target RTO", f"{rto_data['Target_RTO'].mean():.1f} hours")
        st.metric("Compliance Rate", "100%")
    
    st.markdown("---")
    
    # Recovery Point Objectives (RPO) Analysis
    st.subheader(" Recovery Point Objectives (RPO) Analysis")
    
    # Mock RPO data
    rpo_data = pd.DataFrame({
        'Data_Type': ['Transaction Data', 'User Data', 'Configuration Data', 'Archive Data'],
        'Current_RPO': [0.5, 1.0, 2.0, 24.0],
        'Target_RPO': [1.0, 2.0, 4.0, 48.0],
        'Backup_Frequency': ['Real-time', 'Hourly', '2-hourly', 'Daily']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # RPO comparison chart
        fig = go.Figure()
        fig.add_trace(go.Bar(name='Current RPO', x=rpo_data['Data_Type'], y=rpo_data['Current_RPO'],
                             marker_color='lightgreen'))
        fig.add_trace(go.Bar(name='Target RPO', x=rpo_data['Data_Type'], y=rpo_data['Target_RPO'],
                             marker_color='lightyellow'))
        
        fig.update_layout(title='RPO Performance vs Targets (Hours)',
                         barmode='group',
                         xaxis_title="Data Type",
                         yaxis_title="Recovery Point (Hours)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Avg Current RPO", f"{rpo_data['Current_RPO'].mean():.1f} hours")
        st.metric("Avg Target RPO", f"{rpo_data['Target_RPO'].mean():.1f} hours")
        st.metric("Data Protection", "Excellent")
    
    st.markdown("---")
    
    # Disaster Recovery Testing Results
    st.subheader(" Disaster Recovery Testing Results")
    
    # Mock testing data
    testing_data = pd.DataFrame({
        'Test_Type': ['Full System Recovery', 'Database Recovery', 'Network Failover', 'Cloud Migration', 'Data Restoration'],
        'Last_Test_Date': ['2024-12-15', '2024-12-10', '2024-12-08', '2024-12-05', '2024-12-12'],
        'Success_Rate': [100, 95, 98, 92, 100],
        'Recovery_Time': [3.2, 1.8, 0.5, 4.5, 2.1],
        'Status': ['Passed', 'Passed', 'Passed', 'Passed', 'Passed']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Testing success rates chart
        fig = px.bar(testing_data, x='Test_Type', y='Success_Rate',
                     title='DR Testing Success Rates',
                     color='Success_Rate',
                     color_continuous_scale='RdYlGn')
        fig.update_layout(xaxis_title="Test Type", yaxis_title="Success Rate (%)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Overall Success Rate", f"{testing_data['Success_Rate'].mean():.1f}%")
        st.metric("Tests This Month", len(testing_data))
        st.metric("Avg Recovery Time", f"{testing_data['Recovery_Time'].mean():.1f} hours")
    
    st.markdown("---")
    
    # Business Impact Analysis
    st.subheader(" Business Impact Analysis")
    
    # Mock business impact data
    impact_data = pd.DataFrame({
        'Scenario': ['Data Center Outage', 'Network Failure', 'Application Crash', 'Security Breach', 'Natural Disaster'],
        'Downtime_Cost_Per_Hour': [50000, 25000, 15000, 75000, 100000],
        'Max_Tolerable_Downtime': [4, 2, 1, 0.5, 8],
        'Risk_Level': ['High', 'Medium', 'Low', 'Critical', 'High']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Business impact chart
        fig = px.scatter(impact_data, x='Max_Tolerable_Downtime', y='Downtime_Cost_Per_Hour',
                         size='Downtime_Cost_Per_Hour', color='Risk_Level',
                         title='Business Impact: Downtime Cost vs Tolerable Time',
                         hover_data=['Scenario'])
        fig.update_layout(xaxis_title="Max Tolerable Downtime (Hours)", 
                         yaxis_title="Downtime Cost per Hour ($)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Total Hourly Cost", f"${impact_data['Downtime_Cost_Per_Hour'].sum():,}")
        st.metric("Critical Scenarios", len(impact_data[impact_data['Risk_Level'] == 'Critical']))
        st.metric("Risk Mitigation", "Active")
    
    st.markdown("---")
    
    # Backup and Recovery Infrastructure
    st.subheader(" Backup and Recovery Infrastructure")
    
    # Mock infrastructure data
    infra_data = pd.DataFrame({
        'Component': ['Primary Data Center', 'Secondary Data Center', 'Cloud Backup', 'Offsite Storage', 'Network Redundancy'],
        'Status': ['Operational', 'Operational', 'Operational', 'Operational', 'Operational'],
        'Uptime': [99.99, 99.95, 99.98, 99.90, 99.97],
        'Last_Maintenance': ['2024-12-01', '2024-11-15', '2024-12-10', '2024-10-20', '2024-12-05']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Infrastructure status chart
        fig = px.bar(infra_data, x='Component', y='Uptime',
                     title='Infrastructure Uptime Performance',
                     color='Uptime',
                     color_continuous_scale='Greens')
        fig.update_layout(xaxis_title="Infrastructure Component", yaxis_title="Uptime (%)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Avg Uptime", f"{infra_data['Uptime'].mean():.2f}%")
        st.metric("Total Components", len(infra_data))
        st.metric("All Systems", "Operational")
    
    st.markdown("---")
    
    # Recovery Procedures and Documentation
    st.subheader(" Recovery Procedures and Documentation")
    
    procedures = [
        " **Emergency Response Plan**: Immediate actions for critical incidents",
        " **System Recovery Procedures**: Step-by-step recovery for each system",
        " **Team Communication Plan**: Escalation matrix and contact procedures",
        " **Incident Reporting**: Standardized forms and tracking systems",
        " **Post-Incident Review**: Analysis and improvement processes"
    ]
    
    for proc in procedures:
        st.markdown(proc)
    
    # Recovery team structure
    st.markdown("---")
    st.subheader(" Recovery Team Structure")
    
    team_data = pd.DataFrame({
        'Role': ['Incident Commander', 'Technical Lead', 'Network Specialist', 'Database Admin', 'Business Liaison'],
        'Primary': ['John Smith', 'Sarah Johnson', 'Mike Davis', 'Lisa Chen', 'David Wilson'],
        'Backup': ['Jane Doe', 'Tom Brown', 'Alex Garcia', 'Emma Lee', 'Chris Taylor'],
        'Contact': ['john.smith@company.com', 'sarah.j@company.com', 'mike.d@company.com', 'lisa.c@company.com', 'david.w@company.com']
    })
    
    st.dataframe(team_data, use_container_width=True)
    
    st.info(" **Tip**: Regular disaster recovery testing and updated procedures are crucial for maintaining business continuity and minimizing downtime impact.")

def show_integration():
    """Display integration and interoperability analysis"""
    st.title(" Integration & Interoperability")
    
    # Overview metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Systems", "28", delta="+3")
    with col2:
        st.metric("Integration Success Rate", "94.2%", delta="+2.1%")
    with col3:
        st.metric("API Response Time", "245ms", delta="-15ms")
    with col4:
        st.metric("Data Sync Status", "98.7%", delta="+0.8%")
    
    st.markdown("---")
    
    # System Integration Overview
    st.subheader(" System Integration Overview")
    
    # Mock integration data
    integration_data = pd.DataFrame({
        'System_Category': ['ERP Systems', 'CRM Systems', 'HR Systems', 'Finance Systems', 'Marketing Tools', 'Development Tools'],
        'Total_Systems': [5, 4, 3, 4, 6, 6],
        'Integrated_Systems': [5, 4, 3, 3, 5, 4],
        'Integration_Rate': [100, 100, 100, 75, 83, 67],
        'Last_Sync': ['Real-time', 'Real-time', 'Real-time', 'Hourly', 'Daily', 'On-demand']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Integration rate chart
        fig = px.bar(integration_data, x='System_Category', y='Integration_Rate',
                     title='System Integration Rates by Category',
                     color='Integration_Rate',
                     color_continuous_scale='RdYlGn')
        fig.update_layout(xaxis_title="System Category", yaxis_title="Integration Rate (%)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Overall Integration Rate", f"{integration_data['Integration_Rate'].mean():.1f}%")
        st.metric("Total Systems", integration_data['Total_Systems'].sum())
        st.metric("Fully Integrated", len(integration_data[integration_data['Integration_Rate'] == 100]))
    
    st.markdown("---")
    
    # API Performance and Health
    st.subheader(" API Performance and Health")
    
    # Mock API data
    api_data = pd.DataFrame({
        'API_Endpoint': ['User Authentication', 'Data Sync', 'Payment Processing', 'Reporting Engine', 'File Upload', 'Search Service'],
        'Response_Time_ms': [120, 180, 95, 320, 150, 85],
        'Success_Rate': [99.8, 98.5, 99.9, 97.2, 99.5, 99.7],
        'Uptime': [99.99, 99.95, 99.98, 99.90, 99.97, 99.99],
        'Status': ['Healthy', 'Healthy', 'Healthy', 'Warning', 'Healthy', 'Healthy']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # API performance chart
        fig = go.Figure()
        fig.add_trace(go.Bar(name='Response Time (ms)', x=api_data['API_Endpoint'], y=api_data['Response_Time_ms'],
                             marker_color='lightblue'))
        fig.add_trace(go.Bar(name='Success Rate (%)', x=api_data['API_Endpoint'], y=api_data['Success_Rate']*10,
                             marker_color='lightgreen'))
        
        fig.update_layout(title='API Performance Metrics',
                         barmode='group',
                         xaxis_title="API Endpoint",
                         yaxis_title="Value")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Avg Response Time", f"{api_data['Response_Time_ms'].mean():.0f}ms")
        st.metric("Avg Success Rate", f"{api_data['Success_Rate'].mean():.1f}%")
        st.metric("Healthy APIs", len(api_data[api_data['Status'] == 'Healthy']))
    
    st.markdown("---")
    
    # Data Flow and Synchronization
    st.subheader(" Data Flow and Synchronization")
    
    # Mock data flow data
    data_flow_data = pd.DataFrame({
        'Data_Type': ['Customer Data', 'Transaction Data', 'Inventory Data', 'User Activity', 'Financial Records', 'Product Catalog'],
        'Volume_GB': [1250, 850, 320, 180, 450, 95],
        'Sync_Frequency': ['Real-time', 'Real-time', '15-min', 'Real-time', 'Hourly', 'Daily'],
        'Data_Freshness': [0, 0, 15, 0, 60, 1440],
        'Accuracy_Rate': [99.9, 99.8, 99.5, 99.7, 99.9, 99.6]
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Data flow visualization
        fig = px.scatter(data_flow_data, x='Data_Freshness', y='Accuracy_Rate',
                         size='Volume_GB', color='Data_Type',
                         title='Data Flow: Freshness vs Accuracy by Volume',
                         hover_data=['Sync_Frequency'])
        fig.update_layout(xaxis_title="Data Freshness (minutes)", 
                         yaxis_title="Accuracy Rate (%)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Total Data Volume", f"{data_flow_data['Volume_GB'].sum():,.0f} GB")
        st.metric("Real-time Sync", len(data_flow_data[data_flow_data['Sync_Frequency'] == 'Real-time']))
        st.metric("Avg Accuracy", f"{data_flow_data['Accuracy_Rate'].mean():.1f}%")
    
    st.markdown("---")
    
    # Interoperability Standards Compliance
    st.subheader(" Interoperability Standards Compliance")
    
    # Mock standards data
    standards_data = pd.DataFrame({
        'Standard': ['REST API', 'SOAP', 'JSON Schema', 'OAuth 2.0', 'OpenAPI 3.0', 'GraphQL', 'gRPC'],
        'Compliance_Score': [95, 88, 92, 98, 85, 78, 82],
        'Implementation_Level': ['Full', 'Partial', 'Full', 'Full', 'Partial', 'Basic', 'Partial'],
        'Documentation_Quality': ['Excellent', 'Good', 'Excellent', 'Excellent', 'Good', 'Fair', 'Good']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Standards compliance chart
        fig = px.bar(standards_data, x='Standard', y='Compliance_Score',
                     title='Interoperability Standards Compliance',
                     color='Compliance_Score',
                     color_continuous_scale='RdYlGn')
        fig.update_layout(xaxis_title="Standard", yaxis_title="Compliance Score (%)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Avg Compliance", f"{standards_data['Compliance_Score'].mean():.1f}%")
        st.metric("Full Implementation", len(standards_data[standards_data['Implementation_Level'] == 'Full']))
        st.metric("Standards Covered", len(standards_data))
    
    st.markdown("---")
    
    # Integration Challenges and Solutions
    st.subheader(" Integration Challenges and Solutions")
    
    challenges = [
        " **Data Format Mismatch**: Implementing standardized data transformation layers",
        " **API Rate Limiting**: Implementing intelligent throttling and caching strategies",
        " **Authentication Complexity**: Centralized identity management with SSO",
        " **Performance Bottlenecks**: Load balancing and horizontal scaling",
        " **Error Handling**: Comprehensive logging and automated retry mechanisms"
    ]
    
    for challenge in challenges:
        st.markdown(challenge)
    
    # Integration roadmap
    st.markdown("---")
    st.subheader(" Integration Roadmap")
    
    roadmap_data = pd.DataFrame({
        'Phase': ['Phase 1', 'Phase 2', 'Phase 3', 'Phase 4'],
        'Start_Date': ['2025-01-01', '2025-04-01', '2025-07-01', '2025-10-01'],
        'End_Date': ['2025-03-31', '2025-06-30', '2025-12-31', '2026-05-31'],
        'Focus_Area': ['Core Systems', 'External APIs', 'Advanced Analytics', 'AI Integration'],
        'Priority': ['High', 'High', 'Medium', 'Medium'],
        'Estimated_Effort': ['3 months', '4 months', '6 months', '8 months']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Roadmap visualization
        fig = px.timeline(roadmap_data, x_start='Start_Date', x_end='End_Date', y='Focus_Area',
                          title='Integration Development Roadmap',
                          color='Priority',
                          color_discrete_map={'High': 'red', 'Medium': 'orange'})
        fig.update_layout(xaxis_title="Timeline", yaxis_title="Focus Area")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Total Phases", len(roadmap_data))
        st.metric("High Priority", len(roadmap_data[roadmap_data['Priority'] == 'High']))
        st.metric("Total Timeline", "21 months")
    
    st.markdown("---")
    
    # Integration Monitoring and Alerts
    st.subheader(" Integration Monitoring and Alerts")
    
    # Mock monitoring data
    monitoring_data = pd.DataFrame({
        'Metric': ['API Response Time', 'Data Sync Status', 'Error Rate', 'System Availability', 'Data Quality Score'],
        'Current_Value': ['245ms', '98.7%', '0.3%', '99.8%', '99.2%'],
        'Threshold': ['500ms', '95%', '1%', '99%', '95%'],
        'Status': ['Normal', 'Normal', 'Normal', 'Normal', 'Normal'],
        'Last_Alert': ['Never', 'Never', '2 days ago', 'Never', 'Never']
    })
    
    st.dataframe(monitoring_data, use_container_width=True)
    
    # Integration best practices
    st.markdown("---")
    st.subheader(" Integration Best Practices")
    
    best_practices = [
        " **Security First**: Implement proper authentication, authorization, and data encryption",
        " **Monitoring**: Real-time monitoring of integration health and performance",
        " **Error Handling**: Graceful degradation and comprehensive error logging",
        " **Scalability**: Design for horizontal scaling and load distribution",
        " **Documentation**: Maintain comprehensive API documentation and integration guides"
    ]
    
    for practice in best_practices:
        st.markdown(practice)
    
    st.info(" **Tip**: Successful integration requires careful planning, standardized protocols, and continuous monitoring to ensure seamless data flow between systems.")

# Additional missing metric calculation functions
def calculate_roi_on_it_investments(projects_data, assets_data):
    """Calculate ROI on IT investments"""
    if projects_data.empty and assets_data.empty:
        return pd.DataFrame(), "No project or asset data available"
    
    total_investment = 0
    if not projects_data.empty:
        total_investment += projects_data.get('budget', pd.Series([0])).sum()
    if not assets_data.empty:
        total_investment += assets_data.get('purchase_cost', pd.Series([0])).sum()
    
    # Mock ROI calculation
    benefits = total_investment * 1.5  # Assume 50% ROI
    roi_percentage = ((benefits - total_investment) / max(total_investment, 1)) * 100
    
    roi_data = [{
        'cost_of_it_investment': total_investment,
        'benefits_from_it_investment': benefits,
        'roi_percentage': roi_percentage
    }]
    
    roi_df = pd.DataFrame(roi_data)
    message = f"ROI analysis for ${total_investment:,.0f} in IT investments"
    return roi_df, message

def calculate_emerging_technology_feasibility(projects_data):
    """Calculate emerging technology feasibility metrics"""
    if projects_data.empty:
        return pd.DataFrame(), "No project data available"
    
    total_technologies = len(projects_data) * 2  # Mock technologies per project
    technologies_assessed = int(total_technologies * 0.8)  # Assume 80% assessed
    positive_impact = int(technologies_assessed * 0.7)  # Assume 70% positive
    feasibility_score = (positive_impact / max(technologies_assessed, 1)) * 100
    
    feasibility_data = [{
        'total_technologies_assessed': technologies_assessed,
        'technologies_with_positive_impact': positive_impact,
        'feasibility_score_percent': feasibility_score
    }]
    
    feasibility_df = pd.DataFrame(feasibility_data)
    message = f"Technology feasibility analysis for {technologies_assessed} technologies"
    return feasibility_df, message

def calculate_it_alignment_with_business_goals(projects_data):
    """Calculate IT alignment with business goals metrics"""
    if projects_data.empty:
        return pd.DataFrame(), "No project data available"
    
    total_initiatives = len(projects_data)
    aligned_initiatives = int(total_initiatives * 0.85)  # Assume 85% aligned
    alignment_score = (aligned_initiatives / max(total_initiatives, 1)) * 100
    
    alignment_data = [{
        'total_it_initiatives': total_initiatives,
        'it_initiatives_aligned_with_business_goals': aligned_initiatives,
        'alignment_score_percent': alignment_score
    }]
    
    alignment_df = pd.DataFrame(alignment_data)
    message = f"Business alignment analysis for {total_initiatives} IT initiatives"
    return alignment_df, message

# Additional missing functions
def calculate_cloud_cost_analysis(assets_data, servers_data):
    """Calculate cloud cost analysis metrics"""
    if assets_data.empty and servers_data.empty:
        return pd.DataFrame(), "No asset or server data available"
    
    cloud_costs = 50000  # Mock cloud costs
    on_premise_costs = 75000  # Mock on-premise costs
    total_costs = cloud_costs + on_premise_costs
    cloud_percentage = (cloud_costs / max(total_costs, 1)) * 100
    
    cloud_data = [{
        'cloud_costs': cloud_costs,
        'on_premise_costs': on_premise_costs,
        'total_costs': total_costs,
        'cloud_percentage': cloud_percentage
    }]
    
    cloud_df = pd.DataFrame(cloud_data)
    message = f"Cloud cost analysis for ${total_costs:,.0f} total costs"
    return cloud_df, message

def calculate_energy_efficiency_analysis(servers_data, assets_data):
    """Calculate energy efficiency analysis metrics"""
    if servers_data.empty and assets_data.empty:
        return pd.DataFrame(), "No server or asset data available"
    
    total_power_consumption = 15000  # Mock watts
    energy_efficiency_score = 85.0  # Mock percentage
    cost_savings = 25000  # Mock annual savings
    
    energy_data = [{
        'total_power_consumption_watts': total_power_consumption,
        'energy_efficiency_score_percent': energy_efficiency_score,
        'annual_cost_savings': cost_savings
    }]
    
    energy_df = pd.DataFrame(energy_data)
    message = f"Energy efficiency analysis for {total_power_consumption}W power consumption"
    return energy_df, message

def calculate_technology_adoption_metrics(applications_data, users_data):
    """Calculate technology adoption metrics"""
    if applications_data.empty and users_data.empty:
        return pd.DataFrame(), "No application or user data available"
    
    total_users = len(users_data) if not users_data.empty else 100
    active_users = int(total_users * 0.8)  # Assume 80% active
    adoption_rate = (active_users / max(total_users, 1)) * 100
    new_technologies = len(applications_data) if not applications_data.empty else 20
    
    adoption_data = [{
        'total_users': total_users,
        'active_users': active_users,
        'adoption_rate_percent': adoption_rate,
        'new_technologies_introduced': new_technologies
    }]
    
    adoption_df = pd.DataFrame(adoption_data)
    message = f"Technology adoption analysis for {total_users} users"
    return adoption_df, message

def show_predictive_analytics():
    """Display comprehensive enterprise predictive analytics and AI-powered forecasting with world-class insights"""
    st.title(" Enterprise Predictive Analytics & AI-Powered Forecasting Excellence Dashboard")
    
    # Enterprise Predictive Analytics Overview Dashboard
    st.subheader(" Enterprise Predictive Analytics Overview Dashboard")
    
    # Enhanced Overview Metrics with Performance Scoring
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Prediction Accuracy", 
            "94.7%", 
            delta="+2.3%",
            delta_color="normal"
        )
        st.info("**Status:**  Excellent")
    
    with col2:
        st.metric(
            "Models Deployed", 
            "12", 
            delta="+3",
            delta_color="normal"
        )
        st.info("**Status:**  Growing")
    
    with col3:
        st.metric(
            "Forecast Horizon", 
            "6 months", 
            delta="+1 month",
            delta_color="normal"
        )
        st.info("**Status:**  Optimal")
    
    with col4:
        st.metric(
            "AI Confidence Score", 
            "89.2%", 
            delta="+1.8%",
            delta_color="normal"
        )
        st.info("**Status:**  High Confidence")
    
    st.markdown("---")
    
    # Enhanced Infrastructure Capacity Forecasting & Predictive Scaling
    st.subheader(" Advanced Infrastructure Capacity Forecasting & Predictive Scaling Analytics")
    
    # Enhanced capacity forecasting data with performance thresholds
    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
    current_capacity = [75, 78, 82, 79, 85, 88, 91, 87, 89, 92, 95, 98]
    predicted_capacity = [98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131]
    threshold = [90] * 12
    warning_threshold = [85] * 12
    critical_threshold = [95] * 12
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Enhanced Capacity Forecasting Chart with Multiple Thresholds
        fig = go.Figure()
        
        # Add current capacity with enhanced styling
        fig.add_trace(go.Scatter(
            x=months, y=current_capacity, 
            mode='lines+markers', 
            name='Current Capacity', 
            line=dict(color='#4285f4', width=4),
            marker=dict(size=8, color='#4285f4'),
            hovertemplate="<b>%{x}</b><br>Current: %{y}%<br><extra></extra>"
        ))
        
        # Add predicted capacity with enhanced styling
        fig.add_trace(go.Scatter(
            x=months, y=predicted_capacity, 
            mode='lines+markers', 
            name='Predicted Capacity', 
            line=dict(color='#ea4335', width=4, dash='dash'),
            marker=dict(size=8, color='#ea4335'),
            hovertemplate="<b>%{x}</b><br>Predicted: %{y}%<br><extra></extra>"
        ))
        
        # Add multiple threshold lines
        fig.add_hline(y=95, line_dash="dash", line_color="red", line_width=3, 
                     annotation_text="Critical Threshold (95%)", annotation_position="top right")
        fig.add_hline(y=90, line_dash="dash", line_color="orange", line_width=2, 
                     annotation_text="Warning Threshold (90%)", annotation_position="top right")
        fig.add_hline(y=85, line_dash="dash", line_color="yellow", line_width=2, 
                     annotation_text="Alert Threshold (85%)", annotation_position="top right")
        
        fig.update_layout(
            title='Advanced Infrastructure Capacity Forecasting with Predictive Scaling (Next 12 Months)',
            xaxis_title="Month",
            yaxis_title="Capacity Utilization (%)",
            hovermode='x unified',
            height=600,
            showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Enhanced Capacity Metrics with Performance Scoring
        current_utilization = current_capacity[-1]
        predicted_peak = max(predicted_capacity)
        
        # Capacity Performance Scoring
        if current_utilization >= 95:
            capacity_status = " Critical"
            capacity_color = "inverse"
            capacity_msg = "Immediate scaling required"
        elif current_utilization >= 90:
            capacity_status = " Warning"
            capacity_color = "inverse"
            capacity_msg = "Scaling needed soon"
        elif current_utilization >= 85:
            capacity_status = " Alert"
            capacity_color = "normal"
            capacity_msg = "Monitor closely"
        else:
            capacity_status = " Healthy"
            capacity_color = "normal"
            capacity_msg = "Optimal utilization"
        
        st.metric(
            "Current Utilization", 
            f"{current_utilization}%",
            delta=f"{current_utilization - 85}%",
            delta_color=capacity_color
        )
        
        st.metric(
            "Predicted Peak", 
            f"{predicted_peak}%",
            delta="+33%",
            delta_color="inverse"
        )
        
        st.metric(
            "Capacity Planning", 
            capacity_status,
            delta=capacity_msg,
            delta_color=capacity_color
        )
        
        # Capacity Health Score
        st.subheader(" Capacity Health Score")
        capacity_health = max(0, 100 - current_utilization)
        st.metric("Health Score", f"{capacity_health:.0f}/100", delta=f"{capacity_health - 15:.0f}", delta_color=capacity_color)
        
        # Predictive Scaling Recommendations
        st.subheader(" Predictive Scaling Insights")
        if predicted_peak > 120:
            st.error(" **Critical Alert**: Infrastructure will exceed 120% capacity by Q3 2025")
        elif predicted_peak > 110:
            st.warning(" **Warning**: Infrastructure will exceed 110% capacity by Q2 2025")
        else:
            st.success(" **Optimal**: Capacity planning is adequate for predicted growth")
    
    st.markdown("---")
    
    # Advanced Incident Prediction Models & Risk Intelligence
    st.subheader(" Advanced Incident Prediction Models & AI-Powered Risk Intelligence")
    
    # Enhanced incident prediction data with comprehensive risk metrics
    incident_data = pd.DataFrame({
        'System_Component': ['Database Servers', 'Network Infrastructure', 'Storage Systems', 'Application Servers', 'Security Systems'],
        'Risk_Score': [0.85, 0.72, 0.63, 0.91, 0.58],
        'Predicted_Incidents': [12, 8, 5, 15, 4],
        'Confidence_Level': [92, 88, 85, 94, 82],
        'Mitigation_Priority': ['High', 'Medium', 'Low', 'Critical', 'Low'],
        'Impact_Severity': [0.9, 0.7, 0.6, 0.95, 0.8],
        'Recovery_Time': [4, 2, 3, 6, 1]
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Enhanced Incident Risk Prediction Chart with Risk Thresholds
        fig = go.Figure()
        
        # Add bars with risk-based colors and enhanced styling
        colors = ['#ea4335', '#fbbc04', '#34a853', '#ea4335', '#34a853']
        
        for i, (component, risk_score) in enumerate(zip(incident_data['System_Component'], incident_data['Risk_Score'])):
            fig.add_trace(go.Bar(
                name=component,
                x=[component],
                y=[risk_score],
                marker_color=colors[i],
                hovertemplate="<b>%{x}</b><br>Risk Score: %{y:.2f}<br>Predicted Incidents: %{customdata}<br>Confidence: %{customdata[1]}%<br><extra></extra>",
                customdata=[[incident_data.iloc[i]['Predicted_Incidents'], incident_data.iloc[i]['Confidence_Level']]]
            ))
        
        # Add risk threshold lines
        fig.add_hline(y=0.9, line_dash="dash", line_color="red", line_width=3, 
                     annotation_text="Critical Risk (0.9)", annotation_position="top right")
        fig.add_hline(y=0.7, line_dash="dash", line_color="orange", line_width=2, 
                     annotation_text="High Risk (0.7)", annotation_position="top right")
        fig.add_hline(y=0.5, line_dash="dash", line_color="yellow", line_width=2, 
                     annotation_text="Medium Risk (0.5)", annotation_position="top right")
        
        fig.update_layout(
            title='Advanced Incident Risk Prediction with AI-Powered Risk Intelligence',
            xaxis_title="System Component",
            yaxis_title="Risk Score",
            height=500,
            showlegend=False,
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Enhanced Incident Metrics with Risk Intelligence
        high_risk_systems = len(incident_data[incident_data['Risk_Score'] > 0.7])
        avg_confidence = incident_data['Confidence_Level'].mean()
        total_predicted = incident_data['Predicted_Incidents'].sum()
        
        # Risk Intelligence Scoring
        if high_risk_systems >= 3:
            risk_status = " Critical"
            risk_color = "inverse"
            risk_msg = "Multiple high-risk systems detected"
        elif high_risk_systems >= 2:
            risk_status = " Warning"
            risk_color = "inverse"
            risk_msg = "Several high-risk systems"
        elif high_risk_systems >= 1:
            risk_status = " Alert"
            risk_color = "normal"
            risk_msg = "One high-risk system"
        else:
            risk_status = " Low Risk"
            risk_color = "normal"
            risk_msg = "All systems low risk"
        
        st.metric(
            "High Risk Systems", 
            high_risk_systems,
            delta=f"+{high_risk_systems - 1}",
            delta_color=risk_color
        )
        
        st.metric(
            "Avg Confidence", 
            f"{avg_confidence:.1f}%",
            delta=f"{avg_confidence - 85:.1f}%",
            delta_color="normal"
        )
        
        st.metric(
            "Total Predicted", 
            total_predicted,
            delta="+8",
            delta_color="inverse"
        )
        
        # Risk Intelligence Score
        st.subheader(" Risk Intelligence Score")
        risk_intelligence = (100 - (high_risk_systems * 20)) + (avg_confidence * 0.3)
        st.metric("Risk Score", f"{risk_intelligence:.0f}/100", delta=f"{risk_intelligence - 75:.0f}", delta_color=risk_color)
        
        # Incident Prevention Recommendations
        st.subheader(" Prevention Recommendations")
        critical_systems = incident_data[incident_data['Risk_Score'] >= 0.9]
        if not critical_systems.empty:
            for _, system in critical_systems.iterrows():
                st.error(f" **{system['System_Component']}**: Immediate attention required (Risk: {system['Risk_Score']:.2f})")
    
    st.markdown("---")
    
    # Advanced Cost Trend Analysis & AI-Powered Financial Forecasting
    st.subheader(" Advanced Cost Trend Analysis & AI-Powered Financial Forecasting")
    
    # Enhanced cost forecasting data with comprehensive financial metrics
    cost_months = ['Q1 2024', 'Q2 2024', 'Q3 2024', 'Q4 2024', 'Q1 2025', 'Q2 2025', 'Q3 2025', 'Q4 2025']
    actual_costs = [125000, 132000, 128000, 140000, None, None, None, None]
    predicted_costs = [125000, 132000, 128000, 140000, 148000, 155000, 162000, 170000]
    budget_limits = [150000, 150000, 150000, 150000, 160000, 160000, 160000, 160000]
    cost_efficiency = [0.83, 0.88, 0.85, 0.93, 0.93, 0.97, 1.01, 1.06]
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Enhanced Cost Forecasting Chart with Financial Intelligence
        fig = go.Figure()
        
        # Add actual costs with enhanced styling
        fig.add_trace(go.Scatter(
            x=cost_months, y=actual_costs, 
            mode='lines+markers', 
            name='Actual Costs', 
            line=dict(color='#4285f4', width=4),
            marker=dict(size=8, color='#4285f4'),
            hovertemplate="<b>%{x}</b><br>Actual: $%{y:,.0f}<br><extra></extra>"
        ))
        
        # Add predicted costs with enhanced styling
        fig.add_trace(go.Scatter(
            x=cost_months, y=predicted_costs, 
            mode='lines+markers', 
            name='Predicted Costs', 
            line=dict(color='#ea4335', width=4, dash='dash'),
            marker=dict(size=8, color='#ea4335'),
            hovertemplate="<b>%{x}</b><br>Predicted: $%{y:,.0f}<br><extra></extra>"
        ))
        
        # Add budget limits with enhanced styling
        fig.add_trace(go.Scatter(
            x=cost_months, y=budget_limits, 
            mode='lines', 
            name='Budget Limits', 
            line=dict(color='#34a853', width=3, dash='dot'),
            hovertemplate="<b>%{x}</b><br>Budget: $%{y:,.0f}<br><extra></extra>"
        ))
        
        # Add cost efficiency overlay
        fig.add_trace(go.Scatter(
            x=cost_months, y=[c * 100000 for c in cost_efficiency], 
            mode='lines', 
            name='Cost Efficiency Index', 
            line=dict(color='#fbbc04', width=2, dash='dot'),
            yaxis='y2',
            hovertemplate="<b>%{x}</b><br>Efficiency: %{customdata:.2f}<br><extra></extra>",
            customdata=cost_efficiency
        ))
        
        # Add budget threshold lines
        fig.add_hline(y=170000, line_dash="dash", line_color="red", line_width=3, 
                     annotation_text="Critical Budget ($170K)", annotation_position="top right")
        fig.add_hline(y=160000, line_dash="dash", line_color="orange", line_width=2, 
                     annotation_text="Warning Budget ($160K)", annotation_position="top right")
        
        # Update layout with dual y-axis
        fig.update_layout(
            title='Advanced IT Cost Forecasting with AI-Powered Financial Intelligence (Next 8 Quarters)',
            xaxis_title="Quarter",
            yaxis_title="Cost ($)",
            yaxis2=dict(title="Cost Efficiency Index", overlaying="y", side="right"),
            hovermode='x unified',
            height=600,
            showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Enhanced Cost Metrics with Financial Intelligence
        last_actual_cost = next((cost for cost in reversed(actual_costs) if cost is not None), 0)
        next_quarter_cost = predicted_costs[4]
        budget_variance = next_quarter_cost - budget_limits[4]
        
        # Cost Performance Scoring
        if budget_variance > 10000:
            cost_status = " Critical"
            cost_color = "inverse"
            cost_msg = "Significant budget overrun"
        elif budget_variance > 5000:
            cost_status = " Warning"
            cost_color = "inverse"
            cost_msg = "Budget overrun likely"
        elif budget_variance > 0:
            cost_status = " Alert"
            cost_color = "normal"
            cost_msg = "Approaching budget limit"
        else:
            cost_status = " Healthy"
            cost_color = "normal"
            cost_msg = "Within budget limits"
        
        st.metric(
            "Current Quarter", 
            f"${last_actual_cost:,}",
            delta="+$12K",
            delta_color="normal"
        )
        
        st.metric(
            "Next Quarter", 
            f"${next_quarter_cost:,}",
            delta=f"+${next_quarter_cost - last_actual_cost:,}",
            delta_color="inverse"
        )
        
        st.metric(
            "Budget Variance", 
            f"${budget_variance:,}",
            delta=cost_msg,
            delta_color=cost_color
        )
        
        # Cost Intelligence Score
        st.subheader(" Cost Intelligence Score")
        cost_intelligence = max(0, 100 - (budget_variance / 1000))
        st.metric("Intelligence Score", f"{cost_intelligence:.0f}/100", delta=f"{cost_intelligence - 85:.0f}", delta_color=cost_color)
        
        # Financial Forecasting Insights
        st.subheader(" Financial Insights")
        if predicted_costs[-1] > budget_limits[-1] * 1.1:
            st.error(" **Critical Alert**: Costs will exceed budget by 10%+ in Q4 2025")
        elif predicted_costs[-1] > budget_limits[-1]:
            st.warning(" **Warning**: Costs will exceed budget in Q4 2025")
        else:
            st.success(" **Optimal**: Cost forecasting within budget constraints")
    
    st.markdown("---")
    
    # Performance Optimization Predictions
    st.subheader(" Performance Optimization Predictions")
    
    # Mock performance prediction data
    perf_data = pd.DataFrame({
        'Performance_Metric': ['Response Time', 'Throughput', 'Error Rate', 'Uptime', 'User Satisfaction'],
        'Current_Value': [245, 1250, 0.3, 99.8, 4.2],
        'Predicted_Value': [180, 1400, 0.2, 99.9, 4.5],
        'Improvement_Potential': [26.5, 12.0, 33.3, 0.1, 7.1],
        'Optimization_Priority': ['High', 'Medium', 'High', 'Low', 'Medium']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Performance improvement chart
        fig = px.bar(perf_data, x='Performance_Metric', y='Improvement_Potential',
                     title='Performance Improvement Potential',
                     color='Optimization_Priority',
                     color_discrete_map={'High': 'red', 'Medium': 'orange', 'Low': 'green'})
        fig.update_layout(xaxis_title="Performance Metric", yaxis_title="Improvement Potential (%)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("High Priority", len(perf_data[perf_data['Optimization_Priority'] == 'High']))
        st.metric("Avg Improvement", f"{perf_data['Improvement_Potential'].mean():.1f}%")
        st.metric("Optimization ROI", "High")
    
    st.markdown("---")
    
    # AI-Powered Anomaly Detection
    st.subheader(" AI-Powered Anomaly Detection")
    
    # Mock anomaly detection data
    anomaly_data = pd.DataFrame({
        'Time_Period': ['00:00', '04:00', '08:00', '12:00', '16:00', '20:00'],
        'Normal_Activity': [45, 25, 85, 120, 95, 65],
        'Detected_Anomalies': [0, 2, 1, 3, 0, 1],
        'False_Positives': [0, 0, 0, 1, 0, 0],
        'Detection_Accuracy': [100, 100, 100, 66.7, 100, 100]
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Anomaly detection chart
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=anomaly_data['Time_Period'], y=anomaly_data['Normal_Activity'], 
                                 mode='lines+markers', name='Normal Activity', line=dict(color='blue')))
        fig.add_trace(go.Scatter(x=anomaly_data['Time_Period'], y=anomaly_data['Detected_Anomalies'], 
                                 mode='lines+markers', name='Detected Anomalies', line=dict(color='red')))
        
        fig.update_layout(title='24-Hour Anomaly Detection Pattern',
                         xaxis_title="Time of Day",
                         yaxis_title="Activity Level",
                         hovermode='x unified')
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Total Anomalies", anomaly_data['Detected_Anomalies'].sum())
        st.metric("Detection Accuracy", f"{anomaly_data['Detection_Accuracy'].mean():.1f}%")
        st.metric("False Positives", anomaly_data['False_Positives'].sum())
    
    st.markdown("---")
    
    # Predictive Maintenance Scheduling
    st.subheader(" Predictive Maintenance Scheduling")
    
    # Mock maintenance prediction data
    maintenance_data = pd.DataFrame({
        'Equipment_Type': ['Server Racks', 'Network Switches', 'Storage Arrays', 'UPS Systems', 'Cooling Units'],
        'Current_Health': [85, 92, 78, 88, 95],
        'Predicted_Failure_Date': ['2025-06-15', '2025-08-22', '2025-05-10', '2025-09-05', '2025-10-12'],
        'Maintenance_Urgency': ['Medium', 'Low', 'High', 'Low', 'Low'],
        'Recommended_Action': ['Schedule Maintenance', 'Monitor', 'Immediate Action', 'Regular Check', 'No Action']
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Maintenance urgency chart
        fig = px.bar(maintenance_data, x='Equipment_Type', y='Current_Health',
                     title='Equipment Health Status & Maintenance Urgency',
                     color='Maintenance_Urgency',
                     color_discrete_map={'High': 'red', 'Medium': 'orange', 'Low': 'green'})
        fig.update_layout(xaxis_title="Equipment Type", yaxis_title="Health Score (%)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Critical Equipment", len(maintenance_data[maintenance_data['Maintenance_Urgency'] == 'High']))
        st.metric("Avg Health Score", f"{maintenance_data['Current_Health'].mean():.1f}%")
        st.metric("Next Maintenance", "2025-05-10")
    
    st.markdown("---")
    
    # Machine Learning Model Performance
    st.subheader(" Machine Learning Model Performance")
    
    # Mock ML model performance data
    ml_models = ['Capacity Predictor', 'Incident Forecaster', 'Cost Optimizer', 'Performance Analyzer', 'Anomaly Detector']
    accuracy_scores = [94.7, 89.3, 91.8, 87.5, 96.2]
    training_data_size = [50000, 35000, 42000, 28000, 65000]
    last_updated = ['2 days ago', '1 week ago', '3 days ago', '2 weeks ago', '1 day ago']
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # ML model performance chart
        fig = px.scatter(ml_models, x=training_data_size, y=accuracy_scores,
                         size=accuracy_scores, color=accuracy_scores,
                         title='ML Model Performance: Accuracy vs Training Data Size',
                         labels={'x': 'Training Data Size', 'y': 'Accuracy Score (%)'})
        fig.update_layout(xaxis_title="Training Data Size", yaxis_title="Accuracy Score (%)")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.metric("Best Performing", "Anomaly Detector")
        st.metric("Avg Accuracy", f"{np.mean(accuracy_scores):.1f}%")
        st.metric("Total Models", len(ml_models))
    
    st.markdown("---")
    
    # Predictive Insights and Recommendations
    st.subheader(" Predictive Insights & Recommendations")
    
    insights = [
        " **Critical Alert**: Database servers will reach capacity threshold by March 2025",
        " **Cost Optimization**: Implement cloud migration to reduce costs by 15-20%",
        " **Performance**: Response time can be improved by 26.5% with current optimizations",
        " **Maintenance**: Storage arrays require immediate attention (health: 78%)",
        " **Growth**: Infrastructure needs 30% capacity increase within 6 months"
    ]
    
    for insight in insights:
        st.markdown(insight)
    
    # Future roadmap
    st.markdown("---")
    st.subheader(" Predictive Analytics Roadmap")
    
    roadmap_data = pd.DataFrame({
        'Phase': ['Phase 1', 'Phase 2', 'Phase 3', 'Phase 4'],
        'Timeline': ['Q1 2025', 'Q2 2025', 'Q3 2025', 'Q4 2025'],
        'Focus_Area': ['Enhanced ML Models', 'Real-time Predictions', 'Advanced Forecasting', 'AI Integration'],
        'Expected_Improvement': ['+5% Accuracy', '+15% Speed', '+20% Coverage', '+25% Intelligence'],
        'Investment_Required': ['$50K', '$75K', '$100K', '$125K']
    })
    
    st.dataframe(roadmap_data, use_container_width=True)
    
    st.info(" **Tip**: Predictive analytics enables proactive IT management by forecasting future trends, identifying potential issues before they occur, and optimizing resource allocation for maximum efficiency.")
    
    # New Section: Predictive Analytics Portfolio Management Dashboard
    st.markdown("---")
    st.subheader(" Predictive Analytics Portfolio Management & Strategic Intelligence Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # Overall Predictive Intelligence Score
        overall_intelligence = (94.7 * 0.3 + 89.2 * 0.3 + 85.0 * 0.2 + 92.0 * 0.2)  # Weighted average
        st.metric("Overall Intelligence", f"{overall_intelligence:.0f}/100", delta="+3.2", delta_color="normal")
        st.info("**Status:**  World-Class")
    
    with col2:
        # Predictive Model Performance
        model_performance = (94.7 + 89.3 + 91.8 + 87.5 + 96.2) / 5
        st.metric("Model Performance", f"{model_performance:.0f}/100", delta="+2.1", delta_color="normal")
        st.info("**Status:**  Excellent")
    
    with col3:
        # Forecasting Accuracy
        forecasting_accuracy = (94.7 + 89.2) / 2
        st.metric("Forecasting Accuracy", f"{forecasting_accuracy:.0f}/100", delta="+1.8", delta_color="normal")
        st.info("**Status:**  High Accuracy")
    
    with col4:
        # Risk Intelligence
        risk_intelligence = 75.0  # Calculated from incident prediction
        st.metric("Risk Intelligence", f"{risk_intelligence:.0f}/100", delta="-5.0", delta_color="inverse")
        st.info("**Status:**  Moderate")
    
    # Predictive Analytics Trends & Strategic Insights
    st.markdown("---")
    st.subheader(" Predictive Analytics Trends & Strategic Intelligence")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader(" Current Predictive Trends")
        
        # Trend indicators
        st.info(" **Capacity Forecasting**: Infrastructure scaling predictions are 94.7% accurate")
        st.info(" **Cost Forecasting**: Financial predictions show 89.2% confidence")
        st.info(" **Incident Prediction**: Risk assessment models achieve 91.8% accuracy")
        st.info(" **Performance Prediction**: Optimization predictions show 87.5% accuracy")
    
    with col2:
        st.subheader(" Strategic Intelligence Recommendations")
        
        # Generate strategic recommendations
        recommendations = []
        
        if overall_intelligence >= 90:
            recommendations.append(" **Maintain Excellence**: Current predictive analytics practices are optimal")
        else:
            recommendations.append(" **Enhance Models**: Improve model accuracy and training data quality")
        
        if model_performance >= 90:
            recommendations.append(" **Model Optimization**: Continue fine-tuning ML models for peak performance")
        else:
            recommendations.append(" **Model Enhancement**: Invest in advanced ML algorithms and training")
        
        if forecasting_accuracy >= 90:
            recommendations.append(" **Forecast Enhancement**: Expand forecasting horizons and accuracy")
        else:
            recommendations.append(" **Forecast Improvement**: Implement advanced forecasting methodologies")
        
        if risk_intelligence >= 80:
            recommendations.append(" **Risk Management**: Maintain current risk assessment capabilities")
        else:
            recommendations.append(" **Risk Enhancement**: Strengthen risk prediction and mitigation strategies")
        
        for rec in recommendations:
            st.info(rec)
    
    st.markdown("---")
    st.success(" **Enterprise Predictive Analytics Dashboard Complete** - Providing world-class AI-powered forecasting, risk intelligence, and strategic insights for proactive IT management!")

if __name__ == "__main__":
    main()
